---
title: "Take-home 2 - Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows"
author: "Muhamad Ameer Noor"
date: "15 December 2023"
date-modified: "last-modified"
editor: source
format: 
  html:
    self_contained: false
    code-fold: true
    code-summary: "code chunk"
    fontsize: 17px
    number-sections: true
    number-depth: 2
execute:
  echo: true # all code chunks will appear
  eval: true # all code chunks will run live (be evaluated)
  warning: false # don't display warnings
  message: false
---

![Illustration](../images/the2.png)

# Overview

::: panel-tabset
## The Scene

Understanding why city residents wake up early to travel from home to work and assessing the consequences of discontinuing a public bus service along a specific route are key challenges faced by transport operators and urban managers in the realm of urban mobility. Traditionally, answering such questions relied on expensive, time-consuming commuter surveys. However, these surveys not only demanded considerable resources but also yielded data that took a substantial amount of time to clean and analyze, often rendering it outdated by the time reports were ready.

In today's digital era, urban infrastructures, including public buses and mass rapid transits, are becoming increasingly digital. The proliferation of technologies like GPS on vehicles and SMART cards for public transport users generates vast geospatial data sets, offering insights into movement patterns over time and space. Despite this wealth of data, planners struggle to effectively leverage and transform it into valuable information, impacting the return on investment in data collection and management.

To address this gap, this exercise conducts a case study showcasing the potential of Geographic Information System and Spatial Data Analysis (GDSA). By integrating data from various sources, this approach aims to build spatial interaction models that unveil the factors influencing urban mobility patterns in the context of public bus transit.

## The Objective

This task aims to achieve specific goals, focusing on General Geospatial Data Science and Spatial Interaction Modeling.

::: panel-tabset
### General Geospatial Data Science

For the General Geospatial Data Science, the aim is to do the following: - Create a detailed hexagon map (375m distance from center to edges) representing Traffic Analysis Zones (TAZ). - Various time will have different pattern of traffic flow. For this study, the focus is on the period of ***weekday morning peak from 6am to 9am***. - Develop an Origin-Destination (O-D) matrix illustrating commuter flows during the chosen time interval. - Visualize passenger trip flows using geospatial methods, analyzing the observed spatial patterns. - Gather relevant data, both spatial and aspatial, from publicly available sources. - Generate a distance matrix based on the earlier derived hexagon data.

::: {.callout-note collapse="true" title="is 375m a magical number?"}
Studies have found that people are usually willing to walk approximately 750 meters to get to public transportation. A more specific comfortable walking distance that takes into account the local weather and how cities are laid out.

When planning city maps and deciding where to place public transport stops, we use hexagons to represent areas on the map. Why hexagons? They fit together perfectly without wasting any space, which makes them great for dividing the map into zones. To match the 750-meter walking distance, each hexagon is sized so that the distance from the center to any edge is half that distance, which is 375 meters. This ensures that anyone within the hexagon is no more than a 750-meter walk away from the center, where a public transport stop would be ideally located. This method is a smart way to make sure that everyone has easy access to transport within a reasonable walking distance.

*summarized from: [Daniels & Mulley](https://www.jstor.org/stable/26202654), [Dhuri](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e), [Sekste & Kazakov](https://www.kontur.io/blog/h3-hexagonal-grid/), and in-class explanation from [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486)*
:::

::: {.callout-note collapse="true" title="what is Traffic Analysis Zones (TAZ)?"}
A Traffic Analysis Zone (TAZ) is a way of dividing a city into smaller areas for transportation planning. Each TAZ has some information about the people and places in it, such as how many people live there, how many cars they have, and where they work or go to school. These information help planners understand how people travel and what kind of transportation they need.

Some key points about TAZs are:

-   The size and shape of a TAZ can vary depending on the location and the purpose of the study. For example, a TAZ in a downtown area might be smaller and more regular than a TAZ in a rural area.

-   The choice of a TAZ system is very important because it affects the accuracy and usefulness of the transportation models. A good TAZ system should reflect the reality of the travel patterns and demands in the city.

-   AZs are not fixed and can be changed or updated over time to reflect new data or changes in the city. However, changing TAZs can also cause some problems, such as losing historical data or making comparisons difficult.

*Summarized from: [Miller, 2021](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf)*
:::

::: {.callout-note collapse="true" title="what is Origin-Destination (O-D) matrix?"}
An Origin-Destination (O-D) matrix is a way of showing how many trips are made from one place to another in a given area. For example, an O-D matrix can tell us how many people travel from their home to their work, or from their hotel to a tourist attraction, or from one city to another. An O-D matrix can help us understand the travel patterns and demands of people, and plan for better transportation systems.

An O-D matrix usually looks like a table, where the rows represent the origins (where the trips start) and the columns represent the destinations (where the trips end). Each cell in the table shows the number of trips between a specific origin and destination. Sometimes, the table can also include other information, such as the mode of transportation (car, bus, bike, etc.), the time of the day, or the purpose of the trip.

Here is an example of a simple O-D matrix for a city with four zones (A, B, C, and D):

|       | A   | B   | C   | D   | Total |
|-------|-----|-----|-----|-----|-------|
| A     | 0   | 10  | 5   | 15  | 30    |
| B     | 20  | 0   | 10  | 10  | 40    |
| C     | 10  | 15  | 0   | 5   | 30    |
| D     | 5   | 5   | 5   | 0   | 15    |
| Total | 35  | 30  | 20  | 30  | 115   |

This table tells us that there are 115 trips in total in the city, and that the most common origin-destination pair is A-D, with 15 trips. It also tells us that no one travels within the same zone (the diagonal cells are zero).

An O-D matrix can be created from different sources of data, such as surveys, GPS, mobile phones, or social media. Depending on the data source, the level of detail and accuracy of the O-D matrix can vary. For example, a survey might ask people to report their exact home and work locations, while a GPS device might only record the coordinates of the start and end points of a trip. Therefore, different methods and techniques are needed to process and analyze the data, and to convert them into a meaningful O-D matrix.

*Summarized From: [Co≈ükun, et al., 2020](https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/449/2020/isprs-archives-XLIII-B4-2020-449-2020.pdf)*
:::

::: {.callout-note collapse="true" title="what is trip flows?"}
Trip flows are the movements of people or things from one place to another in a given area. For example, trip flows can show how many people travel from their home to their work, or from one city to another, or from one country to another. Trip flows can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society.

*Summarized from: [Tao Ran, 2021](https://doi.org/10.1007/978-3-030-55462-0_7)*
:::

::: {.callout-note collapse="true" title="spatial vs aspatial data"}
Spatial data is data that has a geographic or spatial component, meaning that it is related to a specific location on the Earth's surface. For example, the coordinates of a city, the shape of a lake, or the population density of a region are all spatial data. Spatial data can be represented using maps, graphs, or statistics, and can be analyzed using Geographic Information Systems (GIS).

Aspatial data is data that does not have a direct connection to a specific location. For example, the name of a person, the color of a car, or the price of a product are all aspatial data. Aspatial data can be represented using tables, charts, or text, and can be analyzed using various methods such as arithmetic, logic, or statistics.

The main difference between spatial and aspatial data is that spatial data can show the spatial relationships and patterns of the data, such as distance, direction, or proximity, while aspatial data cannot. Spatial data can also be combined with aspatial data to provide more information and insights. For example, a map of a city can show both the spatial data (the location and shape of the buildings) and the aspatial data (the name and use of the buildings).

*Summarized from: [Cengel](https://cengel.github.io/R-spatial/intro.html)*
:::

::: {.callout-note collapse="true" title="what is distance matrix?"}
A distance matrix is a way of showing how far away different places are from each other in a given area. For example, a distance matrix can tell us how many kilometers or minutes it takes to travel from one city to another by car, bus, or bike. Applying geospatial analytics on distance matrix can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society. *dive deeper at (ArcGIS)\[https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/distance-analysis.htm\]*
:::

### Spatial Interaction Modeling

For the General Spatial Interaction Model, the aim is to do the following:

-   Adjust spatial interaction models to identify factors influencing urban commuting during the specified time.

-   Present modeling results using suitable geovisualization and graphical methods.

-   Interpret the outcomes to gain insights into the factors impacting commuting flows in the urban landscape during the selected time frame.

::: {.callout-note collapse="true" title="what is spatial interaction model?"}
A spatial interaction model is a way of describing how people or things move from one place to another in a given area. For example, a spatial interaction model can show how many people commute from their home to their work, or how many goods are traded between different cities or countries. A spatial interaction model can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society.

One of the most common spatial interaction models is the gravity model, which is based on an analogy to the physical law of gravity. The gravity model assumes that the movement between two places is proportional to their size (such as population or income) and inversely proportional to their distance. The gravity model can be written as:

$$T_{ij} = k \frac{v_i^{\lambda} w_j^{\alpha}}{d_{ij}^{\beta}}$$

where $T_{ij}$ is the movement from place $i$ to place $j$, $v_i$ is the propulsiveness factor from the origin($i$) place,$w_j$ is the attractiveness factor from the destination ($j$), and $d_{ij}$ is the distance between the two places. $k$ is a constant model parameter, while $\lambda$, $\alpha$, and $\beta$ are parameters that measures the effect of their respective variables.

There are other types of spatial interaction models, such as the potential model and retail model, which have different assumptions and formulations. These models can be used to explain different kinds of movements, such as migration, tourism, or disease spread.

References: [Kam Tin Seong](https://isss624-ay2023-24nov.netlify.app/lesson/lesson03/lesson03-sim#/title-slide) and [Spatial interaction models with R](https://cran.r-project.org/web/packages/simodels/vignettes/simodels.html.)
:::
:::

## The Method

The spatial interaction models being used in this task are the variants of **Gravity Model** including **Unconstrained**, **Origin Constrained**, **Destination Constrained**, and **Doubly Constraint**. The models' performance will be compared to see which one is more appropriate to use in the modelling.

::: {.callout-note collapse="true" title="What is Unconstrained Gravity Model?"}
The Unconstrained Gravity Model is a simple form of the gravity model where the principle of conservation is ignored. This means that the interaction between two locations is not limited by the total number of interactions at the origin or destination. In this model, the interaction between two locations is a constant scaling factor, independent of all origins and destinations.
:::

::: {.callout-note collapse="true" title="What is Origin Constrained Gravity Model?"}
The Origin/Production Constrained Gravity Model, also known as the production constrained model, includes origin-specific balancing factors that act as constraints. These constraints ensure that the estimated rows of the flow data matrix sum to the observed row totals. In other words, the total number of interactions originating from a location is fixed, and the model distributes these interactions across various destinations.
:::

::: {.callout-note collapse="true" title="What is Destination Constrained Gravity Model?"}
The Destination/Attraction Constrained Gravity Model, also known as the attraction constrained model, includes destination-specific balancing factors that act as constraints. These constraints ensure that the estimated columns of the flow data matrix sum to the observed column totals. This means that the total number of interactions attracted to a location is fixed, and the model distributes these interactions across various origins.
:::

::: {.callout-note collapse="true" title="What is Doubly Constrained Gravity Model?"}
The Doubly Constrained Gravity Model includes both origin and destination-specific balancing factors that act as constraints. These constraints ensure that the estimated rows and columns of the flow data matrix sum to the observed row and column totals. In other words, both the total number of interactions originating from a location and attracted to a location are fixed.
:::

To dive deeper on these models, explanation can be found on research by [Haynes & Fotheringham, 1985](https://researchrepository.wvu.edu/rri-web-book/16/) and class materials by [Prof Kam Tin Seong](https://r4gdsa.netlify.app/chap16)

## The Data

the content of the following panel explained what aspatial and geospatial data are used in this project.

::: panel-tabset
### Aspatial

::: panel-tabset
#### Passenger Volume by Origin Destination Bus Stops

-   October 2023 Period

-   downloaded from [LTA DataMall - Dynamic Dataset](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html) via [API](https://datamall.lta.gov.sg/content/dam/datamall/datasets/LTA_DataMall_API_User_Guide.pdf)

-   `csv` format.

-   Columns/Fields in the dataset includes YEAR_MONTH, DAY_TYPE, TIME_PER_HOUR, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE, and TOTAL_TRIPS.

::: {.callout-note collapse="true" title="metadata"}
-   YEAR_MONTH: Represent year and Month in which the data is collected. Since it is a monthly data frame, only one unique value exist in each data frame.

-   DAY_TYPE: Represent type of the day which classified as *weekdays* or *weekends/holidays*.

-   TIME_PER_HOUR: Hour which the passenger trip is based on, in intervals from 0 to 23 hours.

-   PT_TYPE: Type of public transport, Since it is bus data sets, only one unique value exist in each data frame (i.e. *bus*)

-   ORIGIN_PT_CODE: ID of origin bus stop

-   DESTINATION_PT_CODE: ID of destination bus stop

-   TOTAL_TRIPS: Number of trips which represent passenger volumes
:::

::: {.callout-note collapse="true" title="Tutorial on Fetching the Data"}
1.  Click this [link](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html), and click the `Request for API Access` ![](../images/tutorial1.jpg)

2.  Fill in the required form ![](../images/tutorial2.jpg)

3.  Check email for confirmation. The `API Account Key` will be required for later step. ![](../images/tutorial3.jpg)

4.  The user guide from LTA [here](https://datamall.lta.gov.sg/content/dam/datamall/datasets/LTA_DataMall_API_User_Guide.pdf) will explains how to make API calls. The user guide also provide the link required for various kind of dataset, keep the link for future use. ![](../images/tutorial4.jpg)

5.  *The following step assume usage of desktop apps version of `Postman` to make the API call*. Firstly, go to [Postman](https://www.postman.com/) and click on the logo of the OS system that you are using. ![](../images/tutorial5.jpg)

6.  *The following step is for Windows User, adjust accordingly if you use other OS*. Click on the dowload button, install the apps, and launch it. ![](../images/tutorial6.png)

7.  In the apps, copy-paste the url from step 4, and make sure that the option is set to `GET`. In this case where the data is monthly, you need to add a parameter of the month data that you want to download in the format of YYYYMM (202308 shown in the example). The parameter is under `Params` section. ![](../images/tutorial7.png)

8.  Next, go to `Headers` section and add AccountKey which value can be obtained from step 3. Click the blue `Send` button. ![](../images/tutorial8.png)

9.  Click the link that will come out on the bottom of the apps, it will be opened in a new tab. ![](../images/tutorial9.png)

10. In this last step, click `Send and Download` in the new tab. You can choose where to put the data and the download will start. ![](../images/tutorial10.jpg)
:::

#### HDB

The dataset contains comprehensive information about various Housing and Development Board (HDB) blocks situated in Singapore. The dataset not only includes details on the number and types of dwelling units but also crucially provides the geographic coordinates, specifically the longitudes and latitudes, corresponding to each HDB block. This geographical information opens the door to transforming the dataset into a spatial data frame, enabling us to treat the HDB block details as a spatial object. By leveraging these coordinates, we can engage in spatial analysis and visualization, gaining valuable insights into the spatial distribution and relationships among different HDB blocks across Singapore. This spatial perspective enhances the depth of understanding and opens avenues for exploring the geographical patterns inherent in the HDB block dataset. The original dataset can be downloaded from [Singapore's National Open Data Collection](https://beta.data.gov.sg/collections/150/view). However, the original dataset does not contain geoidentifier. The geoidentifier in the HDB dataset that is used here was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486).

#### School

The School Directory and Information dataset, sourced from [Singapore's National Open Data Collection](https://beta.data.gov.sg/collections/457/datasets/d_688b934f82c1059ed0a6993d2a829089/view), provides valuable information pertinent to the morning period, notably involving students commuting to school. The dataset encompasses details about the locations, implicitly indicated by the 'POSTAL_CODE' field, of MOE kindergartens, primary schools, secondary schools, and junior colleges. Notably, it excludes information on the locations of ITEs, polytechnics, and universities. With a total of 346 records, this dataset serves as a comprehensive resource for understanding the geographical distribution of various educational institutions.
:::

### Geospatial

Geospatial data in `shp` format are used in this project, as shown in the following panel:

::: panel-tabset
#### Bus Stop Location

-   provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.

-   downloaded from [LTA DataMall - Static Dataset](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)

-   Columns/Fields in the dataset includes BUS_STOP_N, BUS_ROOF_N, LOC_DESC, and geometry.

::: {.callout-note collapse="true" title="metadata"}
-   BUS_STOP_N: The unique identifier for each bus stop.
-   BUS_ROOF_N: The identifier for the bus route or roof associated with the bus stop.
-   LOC_DESC: Location description providing additional information about the bus stop's surroundings.
-   geometry: The spatial information representing the location of each bus stop as a point in the SVY21 projected coordinate reference system.
:::

#### URA Master Plan 2019 (MPSZ)

-   Provides information about the sub-zone boundary of Urban Redevelopment Authority (URA) Master Plan 2019 (MPSZ-2019).

-   The original dataset can be downloaded from [Singapore's National Open Data Collection](https://beta.data.gov.sg/). However, the original dataset have a different file type from the one that is used in this project. The changed file type of the data was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486).

-   Columns/Fields in the dataset includes SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C, REGION_N, REGION_C, and geometry

-   While the analysis primarily focuses on hexagon cells, incorporating the Master Planning Sub-Zone 2019 file enables the integration of additional point layers like Retail and Leisure onto the Singapore map. This integration facilitates the visualization of their respective locations within various planning sub-zones across Singapore.

::: {.callout-note collapse="true" title="metadata"}
-   SUBZONE_N: The unique name for each subzone boundary.
-   SUBZONE_C: The unique identifier for each subzone boundary.
-   PLN_AREA_N: The unique name for each planning area.
-   PLN_AREA_C: The unique identifier for each planning area
-   REGION_N: The unique name for each region.
-   REGION_C: The unique identifier for each region.
-   geometry: The spatial information representing the location of each subzone boundary in Coordinate Reference System (CRS) from World Geodetic Systems (WGS) 84.
:::

#### Hexagon

A [hexagon](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm) layer of 375m (perpendicular distance between the centre of the hexagon and its edges.) Each spatial unit is regular in shape and finer than the Master Plan 2019 Planning Sub-zone GIS data set of URA.

::: {.callout-note collapse="true" title="why hexagon?"}
-   ***Uniform Distances Everywhere***: Think of hexagons as honeycomb cells. Each cell (hexagon) touches its neighbors at the same distance from its center. It's like standing in the middle of a room and being the same distance from every wall, making it easier to measure and compare things.

-   ***Outlier-Free Shape***: Hexagons are like well-rounded polygons without any pointy tips. Sharp corners can create odd spots in data, but hexagons smoothly cover space without sticking out anywhere. This helps prevent weird data spikes that don't fit the pattern.

-   ***Consistent Spatial Relationships***: Imagine a beehive where every hexagon is surrounded by others in the same pattern. This regular pattern is great for analyzing data because you can expect the same relationships everywhere, making the data predictable and easier to work with.

-   ***Ideal for Non-Perpendicular Features***: Real-world features like rivers and roads twist and turn. Squares can be awkward for mapping these, but hexagons, which are more circular, can follow their flow better. This way, a hexagon-based map can mimic the real world more closely than a checkerboard of squares.

Summarized from: [Dhuri](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e), and [Sekste & Kazakov](https://www.kontur.io/blog/h3-hexagonal-grid/).
:::

#### Other Supporting Data

The following dataset will support as propulsive/attractive factors for the modelling. The data was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486). The data includes:

-   Business: provide information about business locations across Singapore

-   Rapid Transit System Station: encompasses the geographical positions of Mass Rapid Transit (MRT) and Light Rail Transit (LRT) stations in Singapore, represented as polygon shapes.

-   Train Station Exit Layer: includes exit points for all MRT and LRT stations in Singapore, stored as individual points.

-   Entertainment: highlights the locations of entertainment venues in Singapore, such as cinemas and theaters, presented as points.

-   Food & Beverage: captures the locations of Food & Beverage venues in Singapore, such as restaurants and cafes, organized as points.

-   Financial Services: showcases the locations of Financial Services in Singapore, encompassing ATMs, money changers, and banks, stored as individual points.

-   Leisure & Recreation: denotes the locations of Leisure and Recreation venues in Singapore, spanning sports venues, museums, and galleries, organized as points.

-   Retails: documents the locations of Retail venues in Singapore, encompassing all shops that may not fit into other categories, presented as points.
:::
:::
:::

# Preparation

Before starting with the analysis, we have to load the library and import the data. This section also contains minor checking and setup of the data.

## Import Library

The following code chunk utilizing [pacman](https://www.rdocumentation.org/packages/pacman/versions/0.5.1) will import the required library (and install it if it does not exist in the environment yet).

```{r}
#| code-fold: false
pacman::p_load(tmap, sf, tidyverse, sfdep, knitr, Hmisc, mapview, DT, sp, stplanr, reshape2, skimr, performance, plotly, httr, corrplot, gifski, patchwork)
```

::: {.callout-note collapse="true" title="Packages Explanations"}
-   [tmap](https://cran.r-project.org/web/packages/tmap/): Used for creating thematic maps in R, both static and interactive, with extensive mapping capabilities.

-   [sf](https://r-spatial.github.io/sf/): Handles and manipulates geospatial data, enabling operations like reading, writing, transforming, and visualizing spatial data.

-   [tidyverse](https://www.tidyverse.org/): A suite of R packages designed for data science tasks, including data manipulation, exploration, and visualization.

-   [sfdep](https://cran.r-project.org/web/packages/sfdep/index.html): This package provides methods for measuring and diagnosing spatial dependence in linear regression models, particularly when working with spatial econometrics. It is tailored to work with 'sf' objects, which are used to handle spatial data in R.

-   [knitr](https://yihui.org/knitr/): Allows for dynamic report generation with R, making it easy to integrate R code into reports and weave together narrative text and code output.

-   [Hmisc](https://cran.r-project.org/web/packages/Hmisc/index.html): Contains many functions useful for data analysis, high-level graphics, utility operations, and functions for dealing with missing values.

-   [mapview](https://cran.r-project.org/web/packages/mapview/index.html): Facilitates the interactive viewing of spatial data in R, built on top of Leaflet.js.

-   [DT](https://rstudio.github.io/DT/): Provides an R interface to the JavaScript library DataTables, useful for creating interactive tables in R markdown documents and Shiny apps.

-   [sp](https://cran.r-project.org/web/packages/sp/index.html): Provides classes and methods for spatial data, and has been superseded by `sf` but is still widely used for compatibility reasons.

-   [stplanr](https://cran.r-project.org/web/packages/stplanr/index.html): Offers sustainable transport planning tools for spatial lines, networks, and movement data.

-   [reshape2](https://cran.r-project.org/web/packages/reshape2/index.html): An R package that allows you to flexibly reshape data, such as melting and casting data frames.

-   [skimr](https://cran.r-project.org/web/packages/skimr/index.html): Summarizes data in a frictionless way and produces a report with useful summary statistics.

-   [performance](https://cran.r-project.org/web/packages/performance/index.html): Assesses the quality and performance of statistical models, including checks for assumptions.

-   [plotly](https://plotly.com/r/): An R package that creates interactive web graphics using the plotly.js library.

-   [httr](https://cran.r-project.org/web/packages/httr/index.html): Simplifies the process of working with HTTP requests, such as API calls.

-   [gifski](https://cran.r-project.org/web/packages/gifski/index.html): Converts images, plots, or animations into high-quality GIFs using the gifski library.
:::

## Data Import and Minor Wrangling

This section will import the required aspatial and geospatial dataset. The process also **involves minor data change** like **setting the correct reference system**, and **removing duplicates** before going to more complex data wrangling in the next section. 

### Geospatial

the following panel will show how each geospatial dataset is imported, modify the CRS Code to 3414 to standardize, check on the data in general, check for duplicates, and display how the data looks like in Singapore map.

::: {.callout-note collapse="true" title="What is CRS Code?"}
A Coordinate Reference System (CRS) Code is a standardized method for locating and describing positions on the Earth's surface. It helps define how geographic data is represented in maps and digital systems.
:::

::: {.callout-note collapse="true" title="Why 3414?"}
The CRS code 3414 specifically refers to the coordinate reference system used for geospatial data in Singapore. It's a unique identifier assigned to this specific system, allowing geographers and mapping software to accurately interpret and display location-based information for Singapore.
:::

::: panel-tabset
#### MPSZ
the following code will import the masterplan subzone 2019 dataset and assign the correct coordinate reference.
```{r}
# Read spatial data for MPSZ-2019 and transform CRS
mpsz <- st_read(dsn='../data/geospatial',  # Specify data source directory
                layer='MPSZ-2019') %>%    # Specify layer to read
  st_transform(crs=3414)                  # Transform CRS to 3414

# View the structure and contents of the mpsz data frame
glimpse(mpsz)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package imports spatial vector data into R. It specifies the data source (`dsn`) and layer (`layer`). Here, it reads the 'MPSZ-2019' layer from the specified directory.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of spatial data. In this case, it transforms the CRS of `mpsz` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

next, check for possible duplicates using this code.
```{r}
# check for duplicates based on unique id
if_else(n_distinct(mpsz$SUBZONE_N) == nrow(mpsz), "no duplicates detected", "possible duplicates detected")
```
::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. In this code, it's used to count the unique values in `mpsz$SUBZONE_N`.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `mpsz`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this context, it checks if the number of unique `SUBZONE_N` values equals the total number of rows in `mpsz`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.
```{r}
# set tmap mode (plot for lighter rendering, view for analysis)
tmap_mode('plot')

# display the data in a map
tm_shape(mpsz)+
  tm_polygons(alpha = 0.3) +
  tm_layout(main.title = 'Singapore Planning Zone', main.title.position = "center")
```
::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_mode) from **tmap** package sets the mode for creating maps. The mode `'plot'` is chosen here for static map plotting, which is typically lighter and faster for rendering compared to the interactive `'view'` mode.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data (`mpsz` in this case) for plotting.
-   [tm_polygons](https://rdrr.io/cran/tmap/man/tm_polygons.html) from **tmap** package adds a layer of polygons to the map, with an alpha parameter to adjust the transparency of these polygons.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) from **tmap** package customizes the layout of the map, including the title and its position.
-   This code snippet creates a thematic map displaying the Singapore Planning Zone with polygonal areas, each represented with a certain level of transparency.
:::

#### Bus Stop Location
the following code will import the bus stop location dataset and assign the correct coordinate reference.
```{r}
busstop <- st_read(dsn = "../data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)

# check the data
glimpse(busstop)
```


::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "BusStop" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `busstop` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

Next, we will check for duplicates using the following code.
*BUS_STOP_N is used as the basis for duplicate checking because it will be the reference in joining the data*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(busstop$BUS_STOP_N) == nrow(busstop), "no duplicates detected", "possible duplicates detected")
```

Since duplicates is found, we will try to check what are the duplicated value using the following code. 


::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "BusStop" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `busstop` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::


```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- busstop[duplicated(busstop$BUS_STOP_N) | duplicated(busstop$BUS_STOP_N, fromLast = TRUE), ] %>%
  arrange(BUS_STOP_N)

# show the number of duplicates
nrow(duplicates)

# Display the sorted rows with duplicate BUS_STOP_N
kable(head(duplicates, n = 32))
```
::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R identifies duplicate elements in a vector or rows in a data frame. Here, it's used to find duplicate values in the `BUS_STOP_N` column of the `busstop` data frame.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from **dplyr** package sorts a data frame by one or more columns. In this code, `arrange` is used to sort the `duplicates` data frame by `BUS_STOP_N`.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) returns the number of rows in a data frame. It's used to count the number of duplicate rows.
-   [kable](https://www.rdocumentation.org/packages/knitr/versions/1.28/topics/kable) from the **knitr** package creates a simple table from a data frame or matrix. This function is used to display the first 32 rows of the `duplicates` data frame in a markdown table format.
:::

Based on the table, we can see that the duplicates does comes from the same bus stop code. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 5161 to 5145.

```{r}
# Keep one row of the duplicates in the original dataset
busstop <- busstop[!duplicated(busstop$BUS_STOP_N) | duplicated(busstop$BUS_STOP_N, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(busstop)
```
::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `BUS_STOP_N` column of the `busstop` data frame to find duplicates.
-   The subset operation (`busstop[...]`) is used to keep only one row for each duplicate in `busstop`. The logical condition `!duplicated(busstop$BUS_STOP_N)` keeps the first occurrence of each duplicate, and `duplicated(busstop$BUS_STOP_N, fromLast = TRUE)` keeps the last occurrence.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the updated `busstop` data frame after removing duplicates.
:::

next, we try to visualize how does the bus stop distribution looks like in the map using the following code.
```{r}
# set tmap mode (plot for lighter rendering, view for analysis)
tmap_mode('plot')

# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.3)+
  tm_shape(busstop)+
  tm_dots() +
  tm_layout(main.title = 'Bus Stop Distribution Map', main.title.position = "center")
```
::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_mode) from **tmap** package sets the mode for creating maps. The mode `'plot'` is chosen for static map plotting, which is typically faster and lighter for rendering than the interactive `'view'` mode.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used twice in this code: first for the `mpsz` dataset and then for `busstop`.
-   [tm_polygons](https://rdrr.io/cran/tmap/man/tm_polygons.html) from **tmap** package adds a layer of polygons to the map, in this case, for the `mpsz` data.
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) from **tmap** package adds a layer of dots to the map, representing the `busstop` data.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) from **tmap** package customizes the layout of the map, including the main title and its position.
-   This code snippet creates a thematic map showing the distribution of bus stops (`busstop`) over a polygonal map of planning zones (`mpsz`).
:::

#### Train Station
the following code will import the train station dataset and assign the correct coordinate reference.
```{r}
station <- st_read(dsn = '../data/geospatial',
                        layer = 'RapidTransitSystemStation') %>%
  st_transform(crs = 3414)

# the data contain non closed ring, use st_is_valid to fix
station <- station %>%
  filter(st_is_valid(.))
         
# check the data
glimpse(station)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_read) from the **sf** package is used to read spatial data from a file, database, or web service. In this code, it's used to read the 'RapidTransitSystemStation' layer from the geospatial data located at '../data/geospatial'.
-   [st_transform](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_transform) from the **sf** package is used to transform or convert coordinates of simple feature. Here, it's used to transform the coordinates of the `station` data to the coordinate reference system (CRS) with the EPSG code 3414.
-   [st_is_valid](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_is_valid) from the **sf** package checks if the geometry, in this case `station`, is valid. The `filter` function from the **dplyr** package is then used to keep only the valid geometries in `station`.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `station` data frame after the transformations.
:::

next, check for possible duplicates using this code.
*the duplicate consider geometry as the unique value as different MRT station should have different location*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(station$geometry) == nrow(station), "no duplicates detected", "possible duplicates detected")
```
since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.
```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(station) +
  tm_fill(col = 'green',
          id = 'STN_NAM_DE') +
  tm_layout(main.title = 'MRT Station Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_shape) from the **tmap** package is used to specify the spatial object that you want to visualize. In this code, it's used twice to specify two different spatial objects: `mpsz` and `station`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_polygons) from the **tmap** package is used to create a layer of polygons. Here, it's used to create a layer of polygons from `mpsz` with white color and 0.01 alpha (transparency).
-   [tm_fill](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_fill) from the **tmap** package is used to fill the polygons of `station` with green color. The `id` argument is set to 'STN_NAM_DE', which means the polygons are grouped by the 'STN_NAM_DE' attribute of `station`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_layout) from the **tmap** package is used to set the layout of the map. Here, it's used to set the main title of the map to 'MRT Station Distribution Map' and position it in the center.
:::

#### Train Exit
the following code will import the train exit location dataset and assign the correct coordinate reference.
```{r}
trainexit <- st_read(dsn = '../data/geospatial',
                     layer = 'Train_Station_Exit_Layer') %>%
  st_transform(crs = 3414)

glimpse(trainexit)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_read) from the **sf** package is used to read spatial data from a file, database, or web service. In this code, it's used to read the 'Train_Station_Exit_Layer' layer from the geospatial data located at '../data/geospatial'.
-   [st_transform](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_transform) from the **sf** package is used to transform or convert coordinates of simple feature. Here, it's used to transform the coordinates of the `trainexit` data to the coordinate reference system (CRS) with the EPSG code 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `trainexit` data frame after the transformations.
:::

next, check for possible duplicates using this code.
*the duplicate consider geometry as the unique value as different MRT exit should have different location*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(trainexit$geometry) == nrow(trainexit), "no duplicates detected", "possible duplicates detected")
```
::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/n_distinct) from the **dplyr** package is used to count the number of unique values in a vector. In this code, it's used to count the number of unique geometries in the `trainexit` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R is used to get the number of rows in a data frame. Here, it's used to get the number of rows in the `trainexit` data frame.
-   [if_else](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/if_else) from the **dplyr** package is a vectorized conditional function that returns a value based on whether a condition is true or false. In this code, it's used to check if the number of unique geometries is equal to the number of rows in `trainexit`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.
```{r}
# visualize the data
tm_shape(mpsz)+
  tm_polygons(alpha = 0.5)+
  tm_shape(trainexit)+
  tm_dots(col = 'blue',
          id = 'exit_id') +
  tm_layout(main.title = 'Train Station Exit Distribution Map', main.title.position = "center")
```


#### Business
the following code will import the business point of interest dataset and assign the correct coordinate reference.
```{r}
biz <- st_read(dsn = "../data/geospatial",
                   layer = "Business") %>%
  st_transform(crs = 3414)

glimpse(biz)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "Business" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `biz` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

next, check for possible duplicates using this code.
*considering some business might have branches, the duplicate condition here is only if all column value is the same*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(biz) == nrow(biz), "no duplicates detected", "possible duplicates detected")
```
::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/n_distinct) from the **dplyr** package is used to count the number of unique values in a vector. In this code, it's used to count the number of unique geometries in the `trainexit` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R is used to get the number of rows in a data frame. Here, it's used to get the number of rows in the `trainexit` data frame.
-   [if_else](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/if_else) from the **dplyr** package is a vectorized conditional function that returns a value based on whether a condition is true or false. In this code, it's used to check if the number of unique geometries is equal to the number of rows in `trainexit`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since duplicates is found, we will try to check what are the duplicated value using the following code. 
```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- biz[duplicated(biz) | duplicated(biz, fromLast = TRUE), ] %>%
  arrange(POI_NAME)

# Display the sorted rows with duplicate BUS_STOP_N
head(duplicates)
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `biz` data frame to find duplicates. The logical condition `duplicated(biz) | duplicated(biz, fromLast = TRUE)` keeps both the first and last occurrence of each duplicate.
-   The subset operation (`biz[...]`) is used to keep only the rows for each duplicate in `biz`.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from the **dplyr** package is used to arrange rows by column values. Here, it's used to arrange the rows of `duplicates` by 'POI_NAME'.
-   [head](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head) from **base** R is used to return the first parts of a vector, matrix, table, data frame or function. In this code, it's used to display the first few rows of the `duplicates` data frame after removing duplicates and arranging by 'POI_NAME'.
:::

Based on the table, we can see that the duplicates does comes from the same business point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 6550 to 6549

```{r}
# Keep one row of the duplicates in the original dataset
biz <- biz[!duplicated(biz) | duplicated(biz, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(biz)
```
::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `biz` data frame to find duplicates. The logical condition `!duplicated(biz) | duplicated(biz, fromLast = TRUE)` keeps both the first and last occurrence of each duplicate.
-   The subset operation (`biz[...]`) is used to keep only one row for each duplicate in `biz`.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `biz` data frame after removing duplicates.
:::


```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(biz)+
  tm_dots(col = 'red') +
  tm_layout(main.title = 'Business Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_shape) from the **tmap** package is used to specify the spatial object that you want to visualize. In this code, it's used twice to specify two different spatial objects: `mpsz` and `trainexit`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_polygons) from the **tmap** package is used to create a layer of polygons. Here, it's used to create a layer of polygons from `mpsz` with 0.5 alpha (transparency).
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_dots) from the **tmap** package is used to create a layer of dots. Here, it's used to create a layer of dots from `trainexit` with blue color. The `id` argument is set to 'exit_id', which means the dots are grouped by the 'exit_id' attribute of `trainexit`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_layout) from the **tmap** package is used to set the layout of the map. Here, it's used to set the main title of the map to 'Train Station Exit Distribution Map' and position it in the center.
:::

#### Entertainment

```{r}
entertn <- st_read(dsn = '../data/geospatial',
                     layer = 'entertn') %>%
  st_transform(crs = 3414)

glimpse(entertn)
```

*considering some entertainment might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(entertn) == nrow(entertn), "no duplicates detected", "possible duplicates detected")
```

show the duplicates

```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- entertn[duplicated(entertn) | duplicated(entertn, fromLast = TRUE), ] %>%
  arrange(POI_NAME)

# Display the sorted rows with duplicateW
head(duplicates)
```

The output shows that it really is the same entertainment

The following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 114 to 113

```{r}
# Keep one row of the duplicates in the original dataset
entertn <- entertn[!duplicated(entertn) | duplicated(entertn, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(entertn)
```

visualize the distribution on the map

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(entertn)+
  tm_dots(col = 'cyan',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Entertainment Distribution Map', main.title.position = "center")
```

#### Food & Beverage

```{r}
fnb <- st_read(dsn = '../data/geospatial',
                     layer = 'F&B') %>%
  st_transform(crs = 3414)

glimpse(fnb)
```

*considering some F & B might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(fnb) == nrow(fnb), "no duplicates detected", "possible duplicates detected")
```

visualize the distribution on the map

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(fnb)+
  tm_dots(col = 'magenta',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Food & Beverage Distribution Map', main.title.position = "center")
```

#### Financial Services

```{r}
finance <- st_read(dsn = '../data/geospatial',
                   layer = 'FinServ') %>%
  st_transform(crs = 3414)

glimpse(finance)
```

*considering some financial services might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(finance) == nrow(finance), "no duplicates detected", "possible duplicates detected")
```

show the duplicates

```{r}
# Subset rows for duplicates and arrange by POI_NAME and POI_ST_NAM
duplicates <- finance[duplicated(finance) | duplicated(finance, fromLast = TRUE), ] %>%
  arrange(POI_NAME, POI_ST_NAM)

# Display the sorted rows with duplicatew
kable(head(duplicates, n=20))
```

The output shows that it really is the same financial service

The following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 3320 to 3058

```{r}
# Keep one row of the duplicates in the original dataset
finance <- finance[!duplicated(finance) | duplicated(finance, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(finance)
```

visualize the distribution on the map

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(finance)+
  tm_dots(col = 'black',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Financial Service Distribution Map', main.title.position = "center")
```

#### Leisure & Recreation

```{r}
lnr <- st_read(dsn = '../data/geospatial',
                   layer = 'Liesure&Recreation') %>%
  st_transform(crs = 3414)

glimpse(lnr)
```

```{r}
n_distinct(lnr$LINK_ID)
n_distinct(lnr$POI_ID)
n_distinct(lnr$geometry)
```

*considering some leisure & recreation might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(lnr) == nrow(lnr), "no duplicates detected", "possible duplicates detected")
```

visualize the distribution on the map

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(lnr)+
  tm_dots(col = 'coral',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Leisure & Recreation Distribution Map', main.title.position = "center")
```

#### Retails

```{r}
retails <- st_read(dsn = '../data/geospatial',
                   layer = 'Retails') %>%
  st_transform(crs = 3414)

glimpse(retails)
```

*considering some financial services might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(retails) == nrow(retails), "no duplicates detected", "possible duplicates detected")
```

show the duplicates

```{r}
# Subset rows for duplicates and arrange by POI_NAME and POI_ST_NAM
duplicates <- retails[duplicated(retails) | duplicated(retails, fromLast = TRUE), ] %>%
  arrange(POI_NAME, POI_ST_NAM)

# Display the sorted rows with duplicatew
kable(head(duplicates, n=20))
```

The output indicates that it really is the same retail business

The following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 37635 to 37463

```{r}
# Keep one row of the duplicates in the original dataset
retails <- retails[!duplicated(retails) | duplicated(retails, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(retails)
```

visualize the distribution on the map

```{r}
# visualize the output
tm_shape(mpsz) +
  tm_polygons(alpha = 0.01) +
  tm_shape(finance) +
  tm_dots(col = 'lightcoral', id = 'POI_NAME') +
  tm_layout(main.title="Retail Distribution Map", main.title.position = "center")
```

:::

### Aspatial

This subsection will import the aspatial data used in this project and check it. The data includes Passenger Volume by Origin Destination Bus Stops and HDB data which explanations can be found in previous section.

#### Passenger Volume

Firstly, the following code will import the Passenger Volume by Origin Destination Bus Stops dataset. At the same time, it will also set the reference bus stop code data type to `factor` for easing compatibility issue and more efficient processing. As previously mentioned, this project will focus on weekday morning peak, so the code will also filter the data by that criteria, grouping it using the reference column, while summing the total trip for each unique grouped reference value. Finally, the code will generate output of summary statistics of the resulting dataset.

```{r}
# Load csv file
odb10 <- read_csv("../data/aspatial/origin_destination_bus_202310.csv.gz")

# change georeference data type into factors
odb10 <- odb10 %>%
  mutate(
    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)
  )

# filter and group the data
odb10 <- odb10 %>%
  filter(DAY_TYPE == "WEEKDAY", 
         TIME_PER_HOUR >= 6 & TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

# check the summary statistics of resulting dataframe
describe(odb10)
```

::: {.callout-note collapse="true" title="Functions"}
-   [read_csv](https://readr.tidyverse.org/reference/read_delim.html) from **readr** package reads a CSV file into R, converting it to a data frame.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new variables to a data frame or modify existing ones. Here, it converts `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE` to factors.
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to subset rows based on specified conditions. In this code, it filters data for weekdays during morning peak hours (6 to 9 AM).
-   [group_by](https://dplyr.tidyverse.org/reference/group_by.html) from **dplyr** package groups the data by specified columns, here by `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE`.
-   [summarise](https://dplyr.tidyverse.org/reference/summarise.html) from **dplyr** package calculates summary statistics for each group, in this case summing up `TOTAL_TRIPS`.
-   [describe](https://www.rdocumentation.org/packages/Hmisc/versions/4.4-0/topics/describe) from **Hmisc** package provides a detailed summary of an object's contents, typically offering statistics like mean, standard deviation, frequency, and others.
:::

::: {.callout-note collapse="true" title="How to Read the Output?"}
The data provides details about the origin and destination points, along with the corresponding number of total trips, time information, and categorization based on day types. The variable summaries indicate the data distribution, with details such as the unique values, frequency, and descriptive statistics for each column.
:::

#### HDB

For the HDB dataset

```{r}
# Load csv file
hdb <- read_csv("../data/aspatial/hdb.csv")

# check the data
glimpse(hdb)
```

visualize the distribution

```{r}
hdb_sf <- st_as_sf(hdb,
                   coords = c("lng", "lat"),
                   crs = 4326) %>%
  st_transform(crs = 3414)

# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(hdb_sf) +
  tm_dots(col = 'lightblue',
          id = 'building') +
  tm_layout(main.title = 'HDB Distribution Map', main.title.position = "center")
```

#### Schools

For the School dataset

Geocoding is the conversion of an address or postal code into geographic coordinates, typically latitude and longitude. The Singapore Land Authority offers the [OneMap API](https://www.onemap.gov.sg/apidocs/), specifically the [Search](https://www.onemap.gov.sg/apidocs/apidocs) API, which retrieves latitude, longitude, and x,y coordinates for a given address or postal code.

To perform geocoding using the [SLA OneMap API](https://www.onemap.gov.sg/docs/#onemap-rest-apis), the provided code, written in R, reads input data from a CSV file using the *read_csv* function from the **readr** package. The **httr** package's HTTP call functions are then used to send individual records to the OneMap geocoding server.

The geocoding process creates two data frames: `found` for successfully geocoded records and `not_found` for those that failed. The `found` data table is joined with the initial CSV data table using a unique identifier (POSTAL) and saved as a new CSV file named `found`.

```{r}
url <- "https://www.onemap.gov.sg/api/common/elastic/search"

csv <- read_csv("../data/aspatial/Generalinformationofschools.csv")
postcodes <- csv$postal_code

found <- data.frame()
not_found <- data.frame()

for(postcode in postcodes){
  query <-list('searchVal' = postcode, 'returnGeom'='Y', 'getAddrDetails'='Y', 'pageNum' = '1')
  res  <- GET(url, query=query)
  
  if((content(res)$found)!=0)
    found<-rbind(found, data.frame(content(res))[4:13])
  else {
  not_found = data.frame(postcode)
  }
}

glimpse(found)
```

```{r}
schools = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)

# manually add the Zhenghua Secondary School data
schools[schools$school_name == "ZHENGHUA SECONDARY SCHOOL", "results.LATITUDE"] <- 1.3887
schools[schools$school_name == "ZHENGHUA SECONDARY SCHOOL", "results.LONGITUDE"] <- 103.7652

# check the output
glimpse(schools)
```

visualize the distribution

```{r}
schools_sf <- schools %>%
  rename(
    latitude = "results.LATITUDE",
    longitude = "results.LONGITUDE"
  ) %>%
  select(
    postal_code, 
    school_name, 
    latitude, 
    longitude
  ) %>%
  st_as_sf(
    coords = c("longitude", "latitude"),
    crs=4326
  ) %>%
  st_transform(
    crs = 3414
  )

# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(schools_sf) +
  tm_dots(col = 'lightgreen',
          id = 'building') +
  tm_layout(main.title = 'Schools Distribution Map', main.title.position = "center")
```

# Data Wrangling

## Create Hexagon Layer

```{r}
# Create hexagonal grid based on bus stop locations
hexagonal_grid <- st_make_grid(busstop,
                               cellsize = 750,
                               what = "polygons",
                               square = FALSE)

# Convert hexagonal grid to spatial dataframe
hex_grid_sf <- st_sf(hex_grid = hexagonal_grid) %>%
  mutate(hexagon_id = 1:length(lengths(hexagonal_grid)))

# Count the number of bus stops within each hexagon
hex_grid_sf$num_bus_stops = lengths(st_intersects(hex_grid_sf, busstop))

# Filter hexagons with at least one bus stop
hexagons_with_bus_stops <- filter(hex_grid_sf, num_bus_stops > 0)

# check the output
glimpse(hexagons_with_bus_stops)
```

visualize the output

```{r}
# Plot the number of bus stops per hexagon with a color gradient and legend
tm_shape(hexagons_with_bus_stops) +
  tm_borders() +
  tm_fill("num_bus_stops",
          title = "Bus Stop Density",
          style = "jenks",
          palette = "YlOrRd",
          legend.show = TRUE) +
  tm_layout(main.title = 'Bus Stop Density in Hexagonal Grid',
            main.title.position = "center",
            legend.position = c("right", "top"))
```

## Combine Bus Stop and Hexagon

```{r}
# Perform a spatial join
busstop_with_hex_info <- st_join(busstop, hexagons_with_bus_stops, by = NULL, join = st_within)

# Select the relevant columns
busstop_with_hex_info <- busstop_with_hex_info %>%
  select(BUS_STOP_N, BUS_ROOF_N, LOC_DESC, hexagon_id, geometry) %>%
  mutate(
    BUS_STOP_N = as.factor(BUS_STOP_N)
  )

# Display the resulting data
glimpse(busstop_with_hex_info)
```

## Combine Trips and Hexagon

```{r}
# Add origin_hex to odb10
odb_hex <- odb10 %>%
  left_join(busstop_with_hex_info %>% select(BUS_STOP_N, hexagon_id), 
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(origin_hex = hexagon_id)

# Add destination_hex to odb10
odb_hex <- odb_hex %>%
  left_join(busstop_with_hex_info %>% select(BUS_STOP_N, hexagon_id), 
            by = c("DESTINATION_PT_CODE" = "BUS_STOP_N")) %>%
  rename(destination_hex = hexagon_id)

# Remove additional geometry columns
odb_hex <- odb_hex %>%
  select(-contains("geometry."))

# Convert hex id to factor
odb_hex <- odb_hex %>%
  mutate(
    origin_hex = as.factor(origin_hex),
    destination_hex = as.factor(destination_hex)
  )

# check the output
glimpse(odb_hex)
```

## Final Duplicate Checking

```{r}
if_else(n_distinct(odb_hex) == nrow(odb_hex), "no duplicates detected", "possible duplicates detected")
```

## Construct O-D Matrix

```{r}
# Remove rows with missing values
clean_odb <- odb_hex[complete.cases(odb_hex), ]

# Aggregate data based on origin_hex and destination_hex
odb_flow <- clean_odb %>%
  group_by(origin_hex, destination_hex) %>%
  summarise(trips = sum(TRIPS),
            origin_hex = unique(origin_hex),
            destination_hex = unique(destination_hex)) %>%
  ungroup()

# check the output
glimpse(odb_flow)
```

## Create Distance Variable

```{r}
# set busstop to spatial
busstop_sp <- as(busstop_with_hex_info, "Spatial")

# calculate the distance
tp_dist <- sp::spDists(busstop_sp, longlat = FALSE)

# add column names to the variable
hex_id_names <- busstop_sp$hexagon_id
colnames(tp_dist) <- paste0(hex_id_names)
rownames(tp_dist) <- paste0(hex_id_names)

# melt the table
dist <- reshape2::melt(tp_dist) %>%
  rename(origin_hex = Var1,
         destination_hex = Var2,
         distance = value) %>%
  mutate(origin_hex = as.factor(origin_hex),
         destination_hex = as.factor(destination_hex))

# remove duplicates
dist <- distinct(dist, origin_hex, destination_hex, .keep_all = TRUE)

# Perform left join to odb_flow with factor conversion
odb_flow <- odb_flow %>%
  left_join(
    dist %>% select(origin_hex, destination_hex, distance) %>%
      mutate(across(c(origin_hex, destination_hex), as.character)),
    by = c("origin_hex", "destination_hex")
  ) %>%
  mutate(across(c(origin_hex, destination_hex), as.factor))

# check the output
glimpse(odb_flow)
```

## Remove Intra-Zonal Flows

the following code chunk will remove intra-zonal flows

```{r}
odb_flow <- odb_flow %>%
  filter(as.character(origin_hex) != as.character(destination_hex))

# check the output
glimpse(odb_flow)
```

::: {.callout-note collapse="true" title="Why intra-zonal flows is removed?"}
The exclusion of intra-zonal flows from the Spatial Interaction Model calibration process is likely driven by the desire to focus on modeling and understanding interactions between different zones, without interference from flows within the same zone.

Intra-zonal flows, representing movements within a single zone, may not contribute significantly to the overall spatial interaction patterns that the model aims to capture. By removing intra-zonal flows, the analysis can concentrate on the dynamics and factors influencing interactions between distinct zones, providing a clearer and more interpretable representation of spatial relationships.

Including intra-zonal flows in the model might introduce unnecessary complexity or noise, potentially affecting the accuracy and interpretability of the model's results. Therefore, excluding intra-zonal flows is a methodological choice to streamline the analysis and enhance the model's ability to capture meaningful inter-zonal interactions.
:::

# Visualize the Desire Lines

firstly, create the line

There are important arguments in od2line:

flow: A data frame representing the origin-destination data. The first column should be the same as the first column of the data in the zones dataframe.

zones: The origin and destination of the travels should be passed here.

zone_code: Name of the variable in zones containing the ids of the zones.

```{r}
flowline <- od2line(flow = odb_flow, 
        zones = hexagons_with_bus_stops,
        zone_code = 'hexagon_id')

glimpse(flowline)
```

## 3rd Quantile Trips (175)

```{r}
summary(flowline$trips)
```

since trips without filter will be too messy, first trial will use the third quantile

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(trips >= 175) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7),
           alpha = 0.2)+
  tm_layout(main.title = 'Weekday Morning Peak Flow for 3rd Quantile Trips', main.title.position = 'center')
```

busier in central and east

## 99th Quantile Trips (6082)

```{r}
quantile(flowline$trips, probs = seq(0, 1, 0.01), na.rm = TRUE)
```

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(trips >= 6082) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7))+
  tm_layout(main.title = 'Weekday Morning Peak Flow for 99th Percentile Trips', main.title.position = 'center',
            main.title.size = 1.2)
```

north-downtown, west-downton, north-east, and short trip becomes more visible

## 3rd Quantile Distance (8923)

```{r}
summary(flowline$distance)
```

since trips without filter will be too messy, first trial will use the third quantile

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(distance >= 8923) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7),
           alpha = 0.2)+
  tm_layout(main.title = 'Weekday Morning Peak Flowline for 3rd Quantile Distance', main.title.position = 'center',
            main.title.size = 1.2)
```

too messy

## 99th Quantile Distance (18587)

```{r}
quantile(flowline$distance, probs = seq(0, 1, 0.01), na.rm = TRUE)
```

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(distance >= 18587) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7))+
  tm_layout(main.title = 'Weekday Morning Peak Flowline for 99th Percentile Distance', main.title.position = 'center',
            main.title.size = 1.2)
```

more dense from all directions to downtown, and also slightly dense between north and east

## Spread Across Decile of Trips

```{r}
quantile(flowline$trips, probs = seq(0, 1, 0.1), na.rm = TRUE)
```

```{r}
flowline <- flowline %>%
  mutate(
    decile = cut(trips, 
                         breaks = c(0, 2, 5, 11, 21, 38, 67, 125, 254, 661, 5000, Inf),
                         labels = c("<2", "2-5", "5-11", "11-21",
                                    "21-38", "38-67", "67-125", "125-254",
                                    "254-661", "661-5000", ">5000"),
                         ordered_result=TRUE
))

# check the output
glimpse(flowline)
```

```{r}
#| eval: false
trips_spread <- tm_shape(mpsz) +
  tm_fill(col = "white") +
  tm_borders(col = "black", lwd = .5) +
tm_shape(flowline) +
  tm_lines(lwd = "trips", scale = 1.5, col = "coral", alpha = .7) +
  tm_layout(title = "Weekday Morning Peak Traffic Flow", title.position = c("right", "top")) +
  tm_facets(along = "decile", free.coords = FALSE)

# Save animation as gif
tmap_animation(trips_spread, "../images/trips_spread.gif", loop = TRUE, delay = 300,
               outer.margins = NA, restart.delay=500)
```

![](../images/trips_spread.gif) for the first 8 decile, the trips slightly change but still more dense on central. on the extra category of \>5000, north-downtown, west-downtown, north-east, and short trip becomes more visible

# Assembling Variables

## Push Factors

HDB, station, trainexit, proxy_pop, busstop

station and trainexit that can function as transit are special case where they might be a push and pull factor. people might come from train station to take a bus or take a bus to go to a train station.

```{r}
factors_holder <- hexagons_with_bus_stops %>%
  rename(push_num_bus_stops = num_bus_stops)

# define function to add push poi counts columns
add_push_poi_counts <- function(factors_holder, poi_datasets) {
  # Loop through each POI dataset
  for (poi_name in poi_datasets) {
    # Add a new column with the count using st_intersects and lengths
    factors_holder[[paste0("push_", poi_name, "_count")]] <- 
      ifelse(
        lengths(st_intersects(factors_holder, get(poi_name))) == 0,
        0.99,
        lengths(st_intersects(factors_holder, get(poi_name)))
      )
  }
  
  # Return the updated factors_holder dataframe
  return(factors_holder)
}

# List of POI dataset names
push_poi_datasets <- c("station", "trainexit", "hdb_sf")

# Call the function
factors_holder <- add_push_poi_counts(factors_holder, push_poi_datasets)

# check the output
glimpse(factors_holder)
```

add approximate population of the location, based on estimated hdb capacity

```{r}
hdb_capacity <- hdb_sf %>%
  mutate(capacity = `1room_sold` *1 +
           `2room_sold` * 2+
           `3room_sold` * 3+
           `4room_sold` * 4+
           `5room_sold` * 5+
           exec_sold * 4+
           multigen_sold * 6+
           studio_apartment_sold *1+
           `1room_rental` * 1+
           `2room_rental` * 2+
           `3room_rental` * 3+
           other_room_rental * 2) %>%
  select(blk_no, geometry, capacity)

# get hexagon_id for hdb, then sum the capacity
hdb_capacity <- st_join(hdb_capacity, hexagons_with_bus_stops, by = NULL, join = st_within) %>%
  group_by(hexagon_id) %>%
  summarise(push_est_pop = sum(capacity)) %>%
  st_drop_geometry()

# Convert hexagon_id to character in both dataframes
factors_holder <- factors_holder %>% mutate(hexagon_id = as.character(hexagon_id))
hdb_capacity <- hdb_capacity %>% mutate(hexagon_id = as.character(hexagon_id))

# Left join factors_holder and hdb_capacity by hexagon_id
factors_holder <- left_join(factors_holder, hdb_capacity, by = "hexagon_id") %>%
  mutate(hexagon_id = as.factor(hexagon_id)) %>%  # Convert back to factor
  mutate_at(vars(contains("push_est_pop")), list(~coalesce(., 0.99)))  # Fill missing values with 0.99 

glimpse(factors_holder)
```

## Pull Factors

station, trainexit, biz, entertn, fnb, finance, lnr, retails, school

```{r}
# define function to add pull poi counts columns
add_pull_poi_counts <- function(factors_holder, poi_datasets) {
  # Loop through each POI dataset
  for (poi_name in poi_datasets) {
    # Add a new column with the count using st_intersects and lengths
    factors_holder[[paste0("pull_", poi_name, "_count")]] <- 
      ifelse(
        lengths(st_intersects(factors_holder, get(poi_name))) == 0,
        0.99,
        lengths(st_intersects(factors_holder, get(poi_name)))
      )
  }
  
  # Return the updated factors_holder dataframe
  return(factors_holder)
}

# List of POI dataset names
pull_poi_datasets <- c("station", "trainexit", "biz", "entertn", "fnb", "finance", "lnr", "retails", "schools_sf")

# Call the function
factors_holder <- add_pull_poi_counts(factors_holder, pull_poi_datasets)

glimpse(factors_holder)
```

## Final Dataset

```{r}
glimpse(odb_flow)
```

```{r}
final_df <- left_join(odb_flow, factors_holder %>% select(starts_with("push_"), hexagon_id), by = c("origin_hex" = "hexagon_id"))

final_df <- left_join(final_df, factors_holder %>% select(starts_with("pull_"), hexagon_id), by = c("destination_hex" = "hexagon_id"))

final_df <- final_df %>%
  select(-hex_grid.x, -hex_grid.y)

# Assuming "final_df" is your data frame
final_df <- final_df %>%
  mutate(push_est_pop = ifelse(push_est_pop == 0, 0.99, push_est_pop))

# Check the output
glimpse(final_df)
```

```{r}
#| eval: false
write_rds(final_df, "../data/rds/final_df_the2.rds")
```

# Spatial Interaction Model

first, re-import the data

```{r}
final_df <- read_rds("../data/rds/final_df_the2.rds")
glimpse(final_df)
```

GLM model does not have a built in R2 value, therefore we build a function to calculate the R2. 
```{r}
CalcRSquared <- function(observed, estimated){
  r <- cor(observed, estimated)
  R2 <- r^2
  R2
}
```

As explained in previous [section](Spatial Interaction Modeling) and [The Method], the Spatial Interaction Model used here are various type of the Gravity Model.

## Correlation Analysis

Before running the model, its a good practice to check collinearity between the variables to prevent bumping into multicollinearity problem in the model.

```{r}
# Calculate correlation
correlation_matrix <- cor(final_df[,3:18])

# Generate correlation plot
corrplot.mixed(correlation_matrix,
               lower = "color",      # Use ellipse for lower part
               upper = "number",     # Display correlation numbers in the upper part
               tl.pos = "lt",        # Place variable names to the left and top
               tl.col = "black",     # Set variable names color to black
               tl.cex = 0.7,         # Set variable names font size
               number.cex = 0.4      # Set correlation numbers font size
               )
```

Upon examining the correlation matrix, using a conservative threshold of correlation value above 0.7, the highly correlated variables are as follows:

1.  all station count and train exit count for each push and pull respectively have a strong positive correlation, suggesting that areas with a higher number of stations also tend to have a higher count of train exits. This result is quite expected.

2.  push_hdb_sf_count and push_est_pop also show a strong positive correlation, which may imply that regions with a higher number of HDB (Housing Development Board) flats tend to have a larger estimated population, a logical relationship given that the data is derived from the same source. Nevertheless, this result might also be caused by many of the hexagonal grid actually do not have HDB in the vicinity.

3.  pull_finance_count and pull_train_exit_count.

4.  pull_fnb_count and pull_finance_count.

5.  pull_fnb_count and pull_lnr_count.

6.  pull_fnr_count and pull_retails_count

::: {.notebox .lightbulb data-latex="lightbulb"}
this finding will be considered as basis for eliminating some variables in the model. Nevertheless, for experimental and comparison purpose, models without variables elimination will also be run in the next section.
:::

## The 4 Models Without Elimination

::: panel-tabset

### Unconstrained
```{r}
uncSIM <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                log(push_station_count) +
                log(push_trainexit_count) +
                log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                log(pull_finance_count) +
                log(pull_lnr_count) +
                log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(uncSIM)
CalcRSquared(uncSIM$data$trips, uncSIM$fitted.values)
```

### Origin Constrained
```{r}
orcSIM <- glm(formula = trips ~
                log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                log(pull_finance_count) +
                log(pull_lnr_count) +
                log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance) +
                origin_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(orcSIM)
CalcRSquared(orcSIM$data$trips, orcSIM$fitted.values)
```

### Destination Constrained
```{r}
decSIM <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                log(push_station_count) +
                log(push_trainexit_count) +
                log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(distance) +
                destination_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(decSIM)
CalcRSquared(decSIM$data$trips, decSIM$fitted.values)
```

### Doubly Constrained
```{r}
docSIM <- glm(formula = trips ~ 
                origin_hex +
                destination_hex +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(docSIM)
CalcRSquared(docSIM$data$trips, docSIM$fitted.values)
```

:::

## The 3 Models With Elimination

::: panel-tabset

### Unconstrained
```{r}
uncSIM2 <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                #log(push_station_count) +
                log(push_trainexit_count) +
                #log(push_hdb_sf_count) +
                log(push_est_pop) +
                #log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                #log(pull_finance_count) +
                #log(pull_lnr_count) +
                #log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(uncSIM2)
CalcRSquared(uncSIM2$data$trips, uncSIM2$fitted.values)
```

### Origin Constrained
```{r}
orcSIM2 <- glm(formula = trips ~
                #log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                #log(pull_finance_count) +
                #log(pull_lnr_count) +
                #log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance) +
                origin_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(orcSIM2)
CalcRSquared(orcSIM2$data$trips, orcSIM2$fitted.values)
```

### Destination Constrained
```{r}
decSIM2 <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                #log(push_station_count) +
                log(push_trainexit_count) +
                #log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(distance) +
                destination_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(decSIM2)
CalcRSquared(decSIM2$data$trips, decSIM2$fitted.values)
```
:::

## Compare The Result
### Performance Table
```{r}
model_list <- list(
  Unconstrained = uncSIM,
  Unconstrained_with_Elimination = uncSIM2,
  Origin_Constrained = orcSIM,
  Origin_Constrained_with_Elimination = orcSIM2,
  Destination_Constrained = decSIM,
  Destination_Constrained_with_Elimination = decSIM2,
  Doubly_Constrained = docSIM
)

# Compare performance with multiple metrics
compare_performance(model_list, metrics = c("AIC", "BIC", "RMSE"))
```

### Visualized Fitted Value
```{r}
# Function to round fitted values and create a data frame
round_and_rename <- function(sim_data, sim_name) {
  as.data.frame(sim_data$fitted.values) %>%
    round(digits = 0) %>%
    setNames(paste0(sim_name, "_trips"))
}

# Round and rename fitted values for each simulation
uncSIM_fitted <- round_and_rename(orcSIM, "uncSIM")
orcSIM_fitted <- round_and_rename(orcSIM, "orcSIM")
decSIM_fitted <- round_and_rename(decSIM, "decSIM")
docSIM_fitted <- round_and_rename(docSIM, "docSIM")
uncSIM2_fitted <- round_and_rename(orcSIM2, "uncSIM2")
orcSIM2_fitted <- round_and_rename(orcSIM2, "orcSIM2")
decSIM2_fitted <- round_and_rename(decSIM2, "decSIM2")

# Combine the rounded and renamed fitted values
final_df_viz <- final_df %>%
  cbind(uncSIM_fitted, orcSIM_fitted, decSIM_fitted, docSIM_fitted, uncSIM2_fitted, orcSIM2_fitted, decSIM2_fitted)

# Create a function to generate a ggplot
generate_ggplot <- function(data, x_col, color, title) {
  ggplot(data = data, aes(x = !!sym(x_col), y = trips)) +
    geom_point(
      size = data$trips / 10000,
      alpha = .6,
      shape = 21  # Change point shape
    ) +
    xlim(0, 50000) +
    geom_smooth(
      method = lm,
      se = TRUE,
      color = "blue"  # Change smooth line color
    ) +
    labs(title = title) +
    theme(
      plot.title = element_text(size = 10),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.text.y = element_blank(),
      axis.title.y = element_blank()
    )
}

# Generate ggplots for each simulation
p_unc <- generate_ggplot(final_df_viz, "uncSIM_trips", "black", "Unconstrained")
p_orc <- generate_ggplot(final_df_viz, "orcSIM_trips", "black", "Origin-constrained")
p_dec <- generate_ggplot(final_df_viz, "decSIM_trips", "black", "Destination-constrained")
p_doc <- generate_ggplot(final_df_viz, "docSIM_trips", "black", "Doubly-constrained")
p_unc2 <- generate_ggplot(final_df_viz, "uncSIM2_trips", "black", "Unconstrained with Elimination")
p_orc2 <- generate_ggplot(final_df_viz, "orcSIM2_trips", "black", "Origin-constrained with Elimination")
p_dec2 <- generate_ggplot(final_df_viz, "decSIM2_trips", "black", "Destination-constrained with Elimination")

# Combine the plots using patchwork
p_unc + p_unc2 + p_orc + p_orc2 + p_dec + p_dec2+ p_doc 
```


## Conclusion

# References

Cengel. [Introduction to spatial data in R](https://cengel.github.io/R-spatial/intro.html)

Co≈ükun, et al. (2020). [Performance Matters on Identification of Origin-Destination Matrix on Geospatial Big Data](https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/449/2020/isprs-archives-XLIII-B4-2020-449-2020.pdf)

Daniels & Mulley. [Explaining walking distance to public transport: The dominance of public transport supply](https://www.jstor.org/stable/26202654)

Haynes & Fotheringham (1985). [Gravity and Spatial Interaction Models](https://researchrepository.wvu.edu/rri-web-book/16/)

Kam Tin Seong. [16 Calibrating Spatial Interaction Models with R](https://r4gdsa.netlify.app/chap16)

Miller (2021). [TRAFFIC ANALYSIS ZONE DEFINITION: ISSUES & GUIDANCE](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf)

R. [Spatial interaction models with R](https://cran.r-project.org/web/packages/simodels/vignettes/simodels.html.)

Sekste and Kazakov. ["H3 hexagonal grid: Why we use it for data analysis and visualization"](https://www.kontur.io/blog/h3-hexagonal-grid/).

Sid Dhuri (2020). ["Spatial Data Analysis With Hexagonal Grids"](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e)

Tao Ran (2021). [Big Spatial Flow Data Analytics. In: Werner, M., Chiang, YY. (eds) Handbook of Big Geospatial Data](https://doi.org/10.1007/978-3-030-55462-0_7)

Land Transport Authority. [Land Transport Data Mall](https://datamall.lta.gov.sg/content/datamall/en.html)
