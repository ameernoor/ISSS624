---
title: "Take-home 2 - Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows"
author: "Muhamad Ameer Noor"
date: "15 December 2023"
date-modified: "last-modified"
editor: source
format: 
  html:
    self_contained: false
    code-fold: true
    code-summary: "code chunk"
    fontsize: 17px
    number-sections: true
    number-depth: 2
execute:
  echo: true # all code chunks will appear
  eval: true # all code chunks will run live (be evaluated)
  warning: false # don't display warnings
  message: false
---

![Illustration](../images/the2.png)

# Overview

::: panel-tabset
## The Scene

Understanding why city residents wake up early to travel from home to work and assessing the consequences of discontinuing a public bus service along a specific route are key challenges faced by transport operators and urban managers in the realm of urban mobility. Traditionally, answering such questions relied on expensive, time-consuming commuter surveys. However, these surveys not only demanded considerable resources but also yielded data that took a substantial amount of time to clean and analyze, often rendering it outdated by the time reports were ready.

In today's digital era, urban infrastructures, including public buses and mass rapid transits, are becoming increasingly digital. The proliferation of technologies like GPS on vehicles and SMART cards for public transport users generates vast geospatial data sets, offering insights into movement patterns over time and space. Despite this wealth of data, planners struggle to effectively leverage and transform it into valuable information, impacting the return on investment in data collection and management.

To address this gap, this exercise conducts a case study showcasing the potential of Geographic Information System and Spatial Data Analysis (GDSA). By integrating data from various sources, this approach aims to build spatial interaction models that unveil the factors influencing urban mobility patterns in the context of public bus transit.

## The Objective

This task aims to achieve specific goals, focusing on General Geospatial Data Science and Spatial Interaction Modeling.

::: panel-tabset
### General Geospatial Data Science

For the General Geospatial Data Science, the aim is to do the following: - Create a detailed hexagon map (375m distance from center to edges) representing Traffic Analysis Zones (TAZ). - Various time will have different pattern of traffic flow. For this study, the focus is on the period of ***weekday morning peak from 6am to 9am***. - Develop an Origin-Destination (O-D) matrix illustrating commuter flows during the chosen time interval. - Visualize passenger trip flows using geospatial methods, analyzing the observed spatial patterns. - Gather relevant data, both spatial and aspatial, from publicly available sources. - Generate a distance matrix based on the earlier derived hexagon data.

::: {.callout-note collapse="true" title="is 375m a magical number?"}
Studies have found that people are usually willing to walk approximately 750 meters to get to public transportation. A more specific comfortable walking distance that takes into account the local weather and how cities are laid out.

When planning city maps and deciding where to place public transport stops, we use hexagons to represent areas on the map. Why hexagons? They fit together perfectly without wasting any space, which makes them great for dividing the map into zones. To match the 750-meter walking distance, each hexagon is sized so that the distance from the center to any edge is half that distance, which is 375 meters. This ensures that anyone within the hexagon is no more than a 750-meter walk away from the center, where a public transport stop would be ideally located. This method is a smart way to make sure that everyone has easy access to transport within a reasonable walking distance.

*summarized from: [Daniels & Mulley](https://www.jstor.org/stable/26202654), [Dhuri](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e), [Sekste & Kazakov](https://www.kontur.io/blog/h3-hexagonal-grid/), and in-class explanation from [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486)*
:::

::: {.callout-note collapse="true" title="what is Traffic Analysis Zones (TAZ)?"}
A Traffic Analysis Zone (TAZ) is a way of dividing a city into smaller areas for transportation planning. Each TAZ has some information about the people and places in it, such as how many people live there, how many cars they have, and where they work or go to school. These information help planners understand how people travel and what kind of transportation they need.

Some key points about TAZs are:

-   The size and shape of a TAZ can vary depending on the location and the purpose of the study. For example, a TAZ in a downtown area might be smaller and more regular than a TAZ in a rural area.

-   The choice of a TAZ system is very important because it affects the accuracy and usefulness of the transportation models. A good TAZ system should reflect the reality of the travel patterns and demands in the city.

-   AZs are not fixed and can be changed or updated over time to reflect new data or changes in the city. However, changing TAZs can also cause some problems, such as losing historical data or making comparisons difficult.

*Summarized from: [Miller, 2021](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf)*
:::

::: {.callout-note collapse="true" title="what is Origin-Destination (O-D) matrix?"}
An Origin-Destination (O-D) matrix is a way of showing how many trips are made from one place to another in a given area. For example, an O-D matrix can tell us how many people travel from their home to their work, or from their hotel to a tourist attraction, or from one city to another. An O-D matrix can help us understand the travel patterns and demands of people, and plan for better transportation systems.

An O-D matrix usually looks like a table, where the rows represent the origins (where the trips start) and the columns represent the destinations (where the trips end). Each cell in the table shows the number of trips between a specific origin and destination. Sometimes, the table can also include other information, such as the mode of transportation (car, bus, bike, etc.), the time of the day, or the purpose of the trip.

Here is an example of a simple O-D matrix for a city with four zones (A, B, C, and D):

|       | A   | B   | C   | D   | Total |
|-------|-----|-----|-----|-----|-------|
| A     | 0   | 10  | 5   | 15  | 30    |
| B     | 20  | 0   | 10  | 10  | 40    |
| C     | 10  | 15  | 0   | 5   | 30    |
| D     | 5   | 5   | 5   | 0   | 15    |
| Total | 35  | 30  | 20  | 30  | 115   |

This table tells us that there are 115 trips in total in the city, and that the most common origin-destination pair is A-D, with 15 trips. It also tells us that no one travels within the same zone (the diagonal cells are zero).

An O-D matrix can be created from different sources of data, such as surveys, GPS, mobile phones, or social media. Depending on the data source, the level of detail and accuracy of the O-D matrix can vary. For example, a survey might ask people to report their exact home and work locations, while a GPS device might only record the coordinates of the start and end points of a trip. Therefore, different methods and techniques are needed to process and analyze the data, and to convert them into a meaningful O-D matrix.

*Summarized From: [Co≈ükun, et al., 2020](https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/449/2020/isprs-archives-XLIII-B4-2020-449-2020.pdf)*
:::

::: {.callout-note collapse="true" title="what is trip flows?"}
Trip flows are the movements of people or things from one place to another in a given area. For example, trip flows can show how many people travel from their home to their work, or from one city to another, or from one country to another. Trip flows can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society.

*Summarized from: [Tao Ran, 2021](https://doi.org/10.1007/978-3-030-55462-0_7)*
:::

::: {.callout-note collapse="true" title="spatial vs aspatial data"}
Spatial data is data that has a geographic or spatial component, meaning that it is related to a specific location on the Earth's surface. For example, the coordinates of a city, the shape of a lake, or the population density of a region are all spatial data. Spatial data can be represented using maps, graphs, or statistics, and can be analyzed using Geographic Information Systems (GIS).

Aspatial data is data that does not have a direct connection to a specific location. For example, the name of a person, the color of a car, or the price of a product are all aspatial data. Aspatial data can be represented using tables, charts, or text, and can be analyzed using various methods such as arithmetic, logic, or statistics.

The main difference between spatial and aspatial data is that spatial data can show the spatial relationships and patterns of the data, such as distance, direction, or proximity, while aspatial data cannot. Spatial data can also be combined with aspatial data to provide more information and insights. For example, a map of a city can show both the spatial data (the location and shape of the buildings) and the aspatial data (the name and use of the buildings).

*Summarized from: [Cengel](https://cengel.github.io/R-spatial/intro.html)*
:::

::: {.callout-note collapse="true" title="what is distance matrix?"}
A distance matrix is a way of showing how far away different places are from each other in a given area. For example, a distance matrix can tell us how many kilometers or minutes it takes to travel from one city to another by car, bus, or bike. Applying geospatial analytics on distance matrix can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society. *dive deeper at (ArcGIS)\[https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/distance-analysis.htm\]*
:::

### Spatial Interaction Modeling

For the General Spatial Interaction Model, the aim is to do the following:

-   Adjust spatial interaction models to identify factors influencing urban commuting during the specified time.

-   Present modeling results using suitable geovisualization and graphical methods.

-   Interpret the outcomes to gain insights into the factors impacting commuting flows in the urban landscape during the selected time frame.

::: {.callout-note collapse="true" title="what is spatial interaction model?"}
A spatial interaction model is a way of describing how people or things move from one place to another in a given area. For example, a spatial interaction model can show how many people commute from their home to their work, or how many goods are traded between different cities or countries. A spatial interaction model can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society.

One of the most common spatial interaction models is the gravity model, which is based on an analogy to the physical law of gravity. The gravity model assumes that the movement between two places is proportional to their size (such as population or income) and inversely proportional to their distance. The gravity model can be written as:

$$T_{ij} = k \frac{v_i^{\lambda} w_j^{\alpha}}{d_{ij}^{\beta}}$$

where $T_{ij}$ is the movement from place $i$ to place $j$, $v_i$ is the propulsiveness factor from the origin($i$) place,$w_j$ is the attractiveness factor from the destination ($j$), and $d_{ij}$ is the distance between the two places. $k$ is a constant model parameter, while $\lambda$, $\alpha$, and $\beta$ are parameters that measures the effect of their respective variables.

There are other types of spatial interaction models, such as the potential model and retail model, which have different assumptions and formulations. These models can be used to explain different kinds of movements, such as migration, tourism, or disease spread.

References: [Kam Tin Seong](https://isss624-ay2023-24nov.netlify.app/lesson/lesson03/lesson03-sim#/title-slide) and [Spatial interaction models with R](https://cran.r-project.org/web/packages/simodels/vignettes/simodels.html.)
:::
:::

## The Method

The spatial interaction models being used in this task are the variants of **Gravity Model** including **Unconstrained**, **Origin Constrained**, **Destination Constrained**, and **Doubly Constraint**. The models' performance will be compared to see which one is more appropriate to use in the modelling.

::: {.callout-note collapse="true" title="What is Unconstrained Gravity Model?"}
The Unconstrained Gravity Model is a simple form of the gravity model where the principle of conservation is ignored. This means that the interaction between two locations is not limited by the total number of interactions at the origin or destination. In this model, the interaction between two locations is a constant scaling factor, independent of all origins and destinations.
:::

::: {.callout-note collapse="true" title="What is Origin Constrained Gravity Model?"}
The Origin/Production Constrained Gravity Model, also known as the production constrained model, includes origin-specific balancing factors that act as constraints. These constraints ensure that the estimated rows of the flow data matrix sum to the observed row totals. In other words, the total number of interactions originating from a location is fixed, and the model distributes these interactions across various destinations.
:::

::: {.callout-note collapse="true" title="What is Destination Constrained Gravity Model?"}
The Destination/Attraction Constrained Gravity Model, also known as the attraction constrained model, includes destination-specific balancing factors that act as constraints. These constraints ensure that the estimated columns of the flow data matrix sum to the observed column totals. This means that the total number of interactions attracted to a location is fixed, and the model distributes these interactions across various origins.
:::

::: {.callout-note collapse="true" title="What is Doubly Constrained Gravity Model?"}
The Doubly Constrained Gravity Model includes both origin and destination-specific balancing factors that act as constraints. These constraints ensure that the estimated rows and columns of the flow data matrix sum to the observed row and column totals. In other words, both the total number of interactions originating from a location and attracted to a location are fixed.
:::

To dive deeper on these models, explanation can be found on research by [Haynes & Fotheringham, 1985](https://researchrepository.wvu.edu/rri-web-book/16/) and class materials by [Prof Kam Tin Seong](https://r4gdsa.netlify.app/chap16)

## The Data

the content of the following panel explained what aspatial and geospatial data are used in this project.

::: panel-tabset
### Aspatial

::: panel-tabset
#### Passenger Volume by Origin Destination Bus Stops

-   October 2023 Period

-   downloaded from [LTA DataMall - Dynamic Dataset](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html) via [API](https://datamall.lta.gov.sg/content/dam/datamall/datasets/LTA_DataMall_API_User_Guide.pdf)

-   `csv` format.

-   Columns/Fields in the dataset includes YEAR_MONTH, DAY_TYPE, TIME_PER_HOUR, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE, and TOTAL_TRIPS.

::: {.callout-note collapse="true" title="metadata"}
-   YEAR_MONTH: Represent year and Month in which the data is collected. Since it is a monthly data frame, only one unique value exist in each data frame.

-   DAY_TYPE: Represent type of the day which classified as *weekdays* or *weekends/holidays*.

-   TIME_PER_HOUR: Hour which the passenger trip is based on, in intervals from 0 to 23 hours.

-   PT_TYPE: Type of public transport, Since it is bus data sets, only one unique value exist in each data frame (i.e. *bus*)

-   ORIGIN_PT_CODE: ID of origin bus stop

-   DESTINATION_PT_CODE: ID of destination bus stop

-   TOTAL_TRIPS: Number of trips which represent passenger volumes
:::

::: {.callout-note collapse="true" title="Tutorial on Fetching the Data"}
1.  Click this [link](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html), and click the `Request for API Access` ![](../images/tutorial1.jpg)

2.  Fill in the required form ![](../images/tutorial2.jpg)

3.  Check email for confirmation. The `API Account Key` will be required for later step. ![](../images/tutorial3.jpg)

4.  The user guide from LTA [here](https://datamall.lta.gov.sg/content/dam/datamall/datasets/LTA_DataMall_API_User_Guide.pdf) will explains how to make API calls. The user guide also provide the link required for various kind of dataset, keep the link for future use. ![](../images/tutorial4.jpg)

5.  *The following step assume usage of desktop apps version of `Postman` to make the API call*. Firstly, go to [Postman](https://www.postman.com/) and click on the logo of the OS system that you are using. ![](../images/tutorial5.jpg)

6.  *The following step is for Windows User, adjust accordingly if you use other OS*. Click on the dowload button, install the apps, and launch it. ![](../images/tutorial6.png)

7.  In the apps, copy-paste the url from step 4, and make sure that the option is set to `GET`. In this case where the data is monthly, you need to add a parameter of the month data that you want to download in the format of YYYYMM (202308 shown in the example). The parameter is under `Params` section. ![](../images/tutorial7.png)

8.  Next, go to `Headers` section and add AccountKey which value can be obtained from step 3. Click the blue `Send` button. ![](../images/tutorial8.png)

9.  Click the link that will come out on the bottom of the apps, it will be opened in a new tab. ![](../images/tutorial9.png)

10. In this last step, click `Send and Download` in the new tab. You can choose where to put the data and the download will start. ![](../images/tutorial10.jpg)
:::

#### HDB

The dataset contains comprehensive information about various Housing and Development Board (HDB) blocks situated in Singapore. The dataset not only includes details on the number and types of dwelling units but also crucially provides the geographic coordinates, specifically the longitudes and latitudes, corresponding to each HDB block. This geographical information opens the door to transforming the dataset into a spatial data frame, enabling us to treat the HDB block details as a spatial object. By leveraging these coordinates, we can engage in spatial analysis and visualization, gaining valuable insights into the spatial distribution and relationships among different HDB blocks across Singapore. This spatial perspective enhances the depth of understanding and opens avenues for exploring the geographical patterns inherent in the HDB block dataset. The original dataset can be downloaded from [Singapore's National Open Data Collection](https://beta.data.gov.sg/collections/150/view). However, the original dataset does not contain geoidentifier. The geoidentifier in the HDB dataset that is used here was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486).

#### School

The School Directory and Information dataset, sourced from [Singapore's National Open Data Collection](https://beta.data.gov.sg/collections/457/datasets/d_688b934f82c1059ed0a6993d2a829089/view), provides valuable information pertinent to the morning period, notably involving students commuting to school. The dataset encompasses details about the locations, implicitly indicated by the 'POSTAL_CODE' field, of MOE kindergartens, primary schools, secondary schools, and junior colleges. Notably, it excludes information on the locations of ITEs, polytechnics, and universities. With a total of 346 records, this dataset serves as a comprehensive resource for understanding the geographical distribution of various educational institutions.
:::

### Geospatial

Geospatial data in `shp` format are used in this project, as shown in the following panel:

::: panel-tabset
#### Bus Stop Location

-   provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.

-   downloaded from [LTA DataMall - Static Dataset](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)

-   Columns/Fields in the dataset includes BUS_STOP_N, BUS_ROOF_N, LOC_DESC, and geometry.

::: {.callout-note collapse="true" title="metadata"}
-   BUS_STOP_N: The unique identifier for each bus stop.
-   BUS_ROOF_N: The identifier for the bus route or roof associated with the bus stop.
-   LOC_DESC: Location description providing additional information about the bus stop's surroundings.
-   geometry: The spatial information representing the location of each bus stop as a point in the SVY21 projected coordinate reference system.
:::

#### URA Master Plan 2019 (MPSZ)

-   Provides information about the sub-zone boundary of Urban Redevelopment Authority (URA) Master Plan 2019 (MPSZ-2019).

-   The original dataset can be downloaded from [Singapore's National Open Data Collection](https://beta.data.gov.sg/). However, the original dataset have a different file type from the one that is used in this project. The changed file type of the data was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486).

-   Columns/Fields in the dataset includes SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C, REGION_N, REGION_C, and geometry

-   While the analysis primarily focuses on hexagon cells, incorporating the Master Planning Sub-Zone 2019 file enables the integration of additional point layers like Retail and Leisure onto the Singapore map. This integration facilitates the visualization of their respective locations within various planning sub-zones across Singapore.

::: {.callout-note collapse="true" title="metadata"}
-   SUBZONE_N: The unique name for each subzone boundary.
-   SUBZONE_C: The unique identifier for each subzone boundary.
-   PLN_AREA_N: The unique name for each planning area.
-   PLN_AREA_C: The unique identifier for each planning area
-   REGION_N: The unique name for each region.
-   REGION_C: The unique identifier for each region.
-   geometry: The spatial information representing the location of each subzone boundary in Coordinate Reference System (CRS) from World Geodetic Systems (WGS) 84.
:::

#### Hexagon

A [hexagon](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm) layer of 375m (perpendicular distance between the centre of the hexagon and its edges.) Each spatial unit is regular in shape and finer than the Master Plan 2019 Planning Sub-zone GIS data set of URA.

::: {.callout-note collapse="true" title="why hexagon?"}
-   ***Uniform Distances Everywhere***: Think of hexagons as honeycomb cells. Each cell (hexagon) touches its neighbors at the same distance from its center. It's like standing in the middle of a room and being the same distance from every wall, making it easier to measure and compare things.

-   ***Outlier-Free Shape***: Hexagons are like well-rounded polygons without any pointy tips. Sharp corners can create odd spots in data, but hexagons smoothly cover space without sticking out anywhere. This helps prevent weird data spikes that don't fit the pattern.

-   ***Consistent Spatial Relationships***: Imagine a beehive where every hexagon is surrounded by others in the same pattern. This regular pattern is great for analyzing data because you can expect the same relationships everywhere, making the data predictable and easier to work with.

-   ***Ideal for Non-Perpendicular Features***: Real-world features like rivers and roads twist and turn. Squares can be awkward for mapping these, but hexagons, which are more circular, can follow their flow better. This way, a hexagon-based map can mimic the real world more closely than a checkerboard of squares.

Summarized from: [Dhuri](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e), and [Sekste & Kazakov](https://www.kontur.io/blog/h3-hexagonal-grid/).
:::

#### Other Supporting Data

The following dataset will support as propulsive/attractive factors for the modelling. The data was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486). The data includes:

-   Business: provide information about business locations across Singapore

-   Rapid Transit System Station: encompasses the geographical positions of Mass Rapid Transit (MRT) and Light Rail Transit (LRT) stations in Singapore, represented as polygon shapes.

-   Train Station Exit Layer: includes exit points for all MRT and LRT stations in Singapore, stored as individual points.

-   Entertainment: highlights the locations of entertainment venues in Singapore, such as cinemas and theaters, presented as points.

-   Food & Beverage: captures the locations of Food & Beverage venues in Singapore, such as restaurants and cafes, organized as points.

-   Financial Services: showcases the locations of Financial Services in Singapore, encompassing ATMs, money changers, and banks, stored as individual points.

-   Leisure & Recreation: denotes the locations of Leisure and Recreation venues in Singapore, spanning sports venues, museums, and galleries, organized as points.

-   Retails: documents the locations of Retail venues in Singapore, encompassing all shops that may not fit into other categories, presented as points.
:::
:::
:::

# Preparation

Before starting with the analysis, we have to load the library and import the data. This section also contains minor checking and setup of the data.

## Import Library

The following code chunk utilizing [pacman](https://www.rdocumentation.org/packages/pacman/versions/0.5.1) will import the required library (and install it if it does not exist in the environment yet).

```{r}
#| code-fold: false
pacman::p_load(tmap, sf, tidyverse, sfdep, knitr, Hmisc, mapview, DT, sp, stplanr, reshape2, skimr, performance, plotly, httr, corrplot, gifski, patchwork)
```

::: {.callout-note collapse="true" title="Packages Explanations"}
-   [tmap](https://cran.r-project.org/web/packages/tmap/): Used for creating thematic maps in R, both static and interactive, with extensive mapping capabilities.

-   [sf](https://r-spatial.github.io/sf/): Handles and manipulates geospatial data, enabling operations like reading, writing, transforming, and visualizing spatial data.

-   [tidyverse](https://www.tidyverse.org/): A suite of R packages designed for data science tasks, including data manipulation, exploration, and visualization.

-   [sfdep](https://cran.r-project.org/web/packages/sfdep/index.html): This package provides methods for measuring and diagnosing spatial dependence in linear regression models, particularly when working with spatial econometrics. It is tailored to work with 'sf' objects, which are used to handle spatial data in R.

-   [knitr](https://yihui.org/knitr/): Allows for dynamic report generation with R, making it easy to integrate R code into reports and weave together narrative text and code output.

-   [Hmisc](https://cran.r-project.org/web/packages/Hmisc/index.html): Contains many functions useful for data analysis, high-level graphics, utility operations, and functions for dealing with missing values.

-   [mapview](https://cran.r-project.org/web/packages/mapview/index.html): Facilitates the interactive viewing of spatial data in R, built on top of Leaflet.js.

-   [DT](https://rstudio.github.io/DT/): Provides an R interface to the JavaScript library DataTables, useful for creating interactive tables in R markdown documents and Shiny apps.

-   [sp](https://cran.r-project.org/web/packages/sp/index.html): Provides classes and methods for spatial data, and has been superseded by `sf` but is still widely used for compatibility reasons.

-   [stplanr](https://cran.r-project.org/web/packages/stplanr/index.html): Offers sustainable transport planning tools for spatial lines, networks, and movement data.

-   [reshape2](https://cran.r-project.org/web/packages/reshape2/index.html): An R package that allows you to flexibly reshape data, such as melting and casting data frames.

-   [skimr](https://cran.r-project.org/web/packages/skimr/index.html): Summarizes data in a frictionless way and produces a report with useful summary statistics.

-   [performance](https://cran.r-project.org/web/packages/performance/index.html): Assesses the quality and performance of statistical models, including checks for assumptions.

-   [plotly](https://plotly.com/r/): An R package that creates interactive web graphics using the plotly.js library.

-   [httr](https://cran.r-project.org/web/packages/httr/index.html): Simplifies the process of working with HTTP requests, such as API calls.

-   [gifski](https://cran.r-project.org/web/packages/gifski/index.html): Converts images, plots, or animations into high-quality GIFs using the gifski library.
:::

## Data Import and Minor Wrangling

This section will import the required aspatial and geospatial dataset. The process also **involves minor data change** like **setting the correct reference system**, and **removing duplicates** before going to more complex data wrangling in the next section.

### Geospatial

the following panel will show how each geospatial dataset is imported, modify the CRS Code to 3414 to standardize, check on the data in general, check for duplicates, and display how the data looks like in Singapore map.

::: {.callout-note collapse="true" title="What is CRS Code?"}
A Coordinate Reference System (CRS) Code is a standardized method for locating and describing positions on the Earth's surface. It helps define how geographic data is represented in maps and digital systems.
:::

::: {.callout-note collapse="true" title="Why 3414?"}
The CRS code 3414 specifically refers to the coordinate reference system used for geospatial data in Singapore. It's a unique identifier assigned to this specific system, allowing geographers and mapping software to accurately interpret and display location-based information for Singapore.
:::

::: panel-tabset

#### MPSZ

::: panel-tabset
##### import

the following code will import the masterplan subzone 2019 dataset and assign the correct coordinate reference.

```{r}
# Read spatial data for MPSZ-2019 and transform CRS
mpsz <- st_read(dsn='../data/geospatial',  # Specify data source directory
                layer='MPSZ-2019') %>%    # Specify layer to read
  st_transform(crs=3414)                  # Transform CRS to 3414
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package imports spatial vector data into R. It specifies the data source (`dsn`) and layer (`layer`). Here, it reads the 'MPSZ-2019' layer from the specified directory.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of spatial data. In this case, it transforms the CRS of `mpsz` to 3414.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
# View the structure and contents of the mpsz data frame
glimpse(mpsz)
```

::: {.callout-note collapse="true" title="Functions"}
[glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

##### duplicate checking

next, check for possible duplicates using this code.

```{r}
# check for duplicates based on unique id
if_else(n_distinct(mpsz$SUBZONE_N) == nrow(mpsz), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. In this code, it's used to count the unique values in `mpsz$SUBZONE_N`.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `mpsz`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this context, it checks if the number of unique `SUBZONE_N` values equals the total number of rows in `mpsz`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

##### distribution map

since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.

```{r}
# set tmap mode (plot for lighter rendering, view for analysis)
tmap_mode('plot')

# display the data in a map
tm_shape(mpsz)+
  tm_polygons(alpha = 0.3) +
  tm_layout(main.title = 'Singapore Planning Zone', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_mode) from **tmap** package sets the mode for creating maps. The mode `'plot'` is chosen here for static map plotting, which is typically lighter and faster for rendering compared to the interactive `'view'` mode.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data (`mpsz` in this case) for plotting.
-   [tm_polygons](https://rdrr.io/cran/tmap/man/tm_polygons.html) from **tmap** package adds a layer of polygons to the map, with an alpha parameter to adjust the transparency of these polygons.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) from **tmap** package customizes the layout of the map, including the title and its position.
-   This code snippet creates a thematic map displaying the Singapore Planning Zone with polygonal areas, each represented with a certain level of transparency.
:::
:::

#### Bus Stop Location

::: panel-tabset
##### import

the following code will import the bus stop location dataset and assign the correct coordinate reference.

```{r}
busstop <- st_read(dsn = "../data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package imports spatial vector data into R. It specifies the data source (`dsn`) and layer (`layer`). Here, it reads the 'BusStop' layer from the specified directory.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of spatial data. In this case, it transforms the CRS of `busstop` to 3414.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
# check the data
glimpse(busstop)
```

::: {.callout-note collapse="true" title="Functions"}
[glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

##### duplicate checking

Next, we will check for duplicates using the following code. *BUS_STOP_N is used as the basis for duplicate checking because it will be the reference in joining the data*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(busstop$BUS_STOP_N) == nrow(busstop), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "BusStop" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `busstop` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

Since duplicates is found, we will try to check what are the duplicated value using the following code.

```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- busstop[duplicated(busstop$BUS_STOP_N) | duplicated(busstop$BUS_STOP_N, fromLast = TRUE), ] %>%
  arrange(BUS_STOP_N)

# show the number of duplicates
nrow(duplicates)

# Display the sorted rows with duplicate BUS_STOP_N
kable(head(duplicates, n = 32))
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R identifies duplicate elements in a vector or rows in a data frame. Here, it's used to find duplicate values in the `BUS_STOP_N` column of the `busstop` data frame.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from **dplyr** package sorts a data frame by one or more columns. In this code, `arrange` is used to sort the `duplicates` data frame by `BUS_STOP_N`.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) returns the number of rows in a data frame. It's used to count the number of duplicate rows.
-   [kable](https://www.rdocumentation.org/packages/knitr/versions/1.28/topics/kable) from the **knitr** package creates a simple table from a data frame or matrix. This function is used to display the first 32 rows of the `duplicates` data frame in a markdown table format.
:::

Based on the table, we can see that the duplicates does comes from the same bus stop code. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 5161 to 5145.

```{r}
# Keep one row of the duplicates in the original dataset
busstop <- busstop[!duplicated(busstop$BUS_STOP_N) | duplicated(busstop$BUS_STOP_N, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(busstop)
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `BUS_STOP_N` column of the `busstop` data frame to find duplicates.
-   The subset operation (`busstop[...]`) is used to keep only one row for each duplicate in `busstop`. The logical condition `!duplicated(busstop$BUS_STOP_N)` keeps the first occurrence of each duplicate, and `duplicated(busstop$BUS_STOP_N, fromLast = TRUE)` keeps the last occurrence.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the updated `busstop` data frame after removing duplicates.
:::

##### distribution map

next, we try to visualize how does the bus stop distribution looks like in the map using the following code.

```{r}
# set tmap mode (plot for lighter rendering, view for analysis)
tmap_mode('plot')

# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.3)+
  tm_shape(busstop)+
  tm_dots() +
  tm_layout(main.title = 'Bus Stop Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_mode) from **tmap** package sets the mode for creating maps. The mode `'plot'` is chosen for static map plotting, which is typically faster and lighter for rendering than the interactive `'view'` mode.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used twice in this code: first for the `mpsz` dataset and then for `busstop`.
-   [tm_polygons](https://rdrr.io/cran/tmap/man/tm_polygons.html) from **tmap** package adds a layer of polygons to the map, in this case, for the `mpsz` data.
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) from **tmap** package adds a layer of dots to the map, representing the `busstop` data.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) from **tmap** package customizes the layout of the map, including the main title and its position.
-   This code snippet creates a thematic map showing the distribution of bus stops (`busstop`) over a polygonal map of planning zones (`mpsz`).
:::
:::

#### Train Station

::: panel-tabset
##### import

the following code will import the train station dataset and assign the correct coordinate reference.

```{r}
station <- st_read(dsn = '../data/geospatial',
                        layer = 'RapidTransitSystemStation') %>%
  st_transform(crs = 3414)

# the data contain non closed ring, use st_is_valid to fix
station <- station %>%
  filter(st_is_valid(.))
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_read) from the **sf** package is used to read spatial data from a file, database, or web service. In this code, it's used to read the 'RapidTransitSystemStation' layer from the geospatial data located at '../data/geospatial'.
-   [st_transform](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_transform) from the **sf** package is used to transform or convert coordinates of simple feature. Here, it's used to transform the coordinates of the `station` data to the coordinate reference system (CRS) with the EPSG code 3414.
-   [st_is_valid](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_is_valid) from the **sf** package checks if the geometry, in this case `station`, is valid. The `filter` function from the **dplyr** package is then used to keep only the valid geometries in `station`.
:::

::: {.callout-note collapse="true" title="what is non-closed rings?"}
In geospatial data, a "non-closed ring" refers to a geometric shape that is intended to form a closed loop (like a polygon) but, due to some issues in the data, is not properly closed. It might have a small gap or overlap at the supposed endpoint, making it an invalid geometric object.

The existence of non-closed rings in a dataset could result from various reasons, such as errors in data collection, digitization issues, or inaccuracies in the original source. These issues can lead to geometric shapes that do not fulfill the criteria of a closed ring, making the data inconsistent or problematic for further spatial analysis.

To address this, the `st_is_valid` function is used in the given R code snippet. This function is designed to check the validity of geometries in a spatial dataset. When applied to the `station` dataset, the `st_is_valid` function identifies and filters out any geometries (in this case, stations) that are not valid, essentially removing those with non-closed rings.

By using `filter(st_is_valid(.))`, we are keeping only the valid geometries in the `station` dataset, effectively fixing or cleaning the data by excluding any stations with non-closed rings. This ensures that the remaining geometries in the dataset are properly formed, closed rings, making them suitable for accurate spatial analysis and visualization.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
# check the data
glimpse(station)
```

::: {.callout-note collapse="true" title="Functions"}
[glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `station` data frame after the transformations.
:::

##### check duplicates

next, check for possible duplicates using this code. *the duplicate consider geometry as the unique value as different MRT station should have different location*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(station$geometry) == nrow(station), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. In this code, it's used to count the unique `geometry` values in the `station` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. It's used to get the total number of rows in `station`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this context, it checks if the number of unique `geometry` values equals the total number of rows in `station`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

##### distribution map

since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(station) +
  tm_fill(col = 'green',
          id = 'STN_NAM_DE') +
  tm_layout(main.title = 'MRT Station Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_shape) from the **tmap** package is used to specify the spatial object that you want to visualize. In this code, it's used twice to specify two different spatial objects: `mpsz` and `station`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_polygons) from the **tmap** package is used to create a layer of polygons. Here, it's used to create a layer of polygons from `mpsz` with white color and 0.01 alpha (transparency).
-   [tm_fill](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_fill) from the **tmap** package is used to fill the polygons of `station` with green color. The `id` argument is set to 'STN_NAM_DE', which means the polygons are grouped by the 'STN_NAM_DE' attribute of `station`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_layout) from the **tmap** package is used to set the layout of the map. Here, it's used to set the main title of the map to 'MRT Station Distribution Map' and position it in the center.
:::
:::

#### Train Exit

::: panel-tabset
##### import

the following code will import the train exit location dataset and assign the correct coordinate reference.

```{r}
trainexit <- st_read(dsn = '../data/geospatial',
                     layer = 'Train_Station_Exit_Layer') %>%
  st_transform(crs = 3414)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_read) from the **sf** package is used to read spatial data from a file, database, or web service. In this code, it's used to read the 'Train_Station_Exit_Layer' layer from the geospatial data located at '../data/geospatial'.
-   [st_transform](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_transform) from the **sf** package is used to transform or convert coordinates of simple feature. Here, it's used to transform the coordinates of the `trainexit` data to the coordinate reference system (CRS) with the EPSG code 3414.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
glimpse(trainexit)
```

::: {.callout-note collapse="true" title="Functions"}
[glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `trainexit` data frame after the transformations.
:::

##### check duplicates

next, check for possible duplicates using this code. *the duplicate consider geometry as the unique value as different MRT exit should have different location*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(trainexit$geometry) == nrow(trainexit), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/n_distinct) from the **dplyr** package is used to count the number of unique values in a vector. In this code, it's used to count the number of unique geometries in the `trainexit` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R is used to get the number of rows in a data frame. Here, it's used to get the number of rows in the `trainexit` data frame.
-   [if_else](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/if_else) from the **dplyr** package is a vectorized conditional function that returns a value based on whether a condition is true or false. In this code, it's used to check if the number of unique geometries is equal to the number of rows in `trainexit`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

##### distribution map

since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.

```{r}
# visualize the data
tm_shape(mpsz)+
  tm_polygons(alpha = 0.5)+
  tm_shape(trainexit)+
  tm_dots(col = 'blue',
          id = 'exit_id') +
  tm_layout(main.title = 'Train Station Exit Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used here for `mpsz` and `trainexit` datasets.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_polygons) adds a layer of polygons to the map, with an alpha parameter to adjust the transparency of these polygons for `mpsz`.
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) adds a layer of dots to the map, representing the `trainexit` data. The color of the dots is set to blue, and the identifier (`id`) is set to 'exit_id'.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the layout of the map, including the main title and its position.
-   This code snippet creates a thematic map displaying the distribution of train station exits (`trainexit`) over a polygonal map of planning zones (`mpsz`), with moderate transparency set for the polygons.
:::
:::

#### Business

::: panel-tabset
##### import

the following code will import the business point of interest dataset and assign the correct coordinate reference.

```{r}
biz <- st_read(dsn = "../data/geospatial",
                   layer = "Business") %>%
  st_transform(crs = 3414)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "Business" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `biz` to 3414.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
glimpse(biz)
```

::: {.callout-note collapse="true" title="Functions"}
[glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

##### check duplicates

next, check for possible duplicates using this code. *considering some business might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(biz) == nrow(biz), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/n_distinct) from the **dplyr** package is used to count the number of unique values in a vector. In this code, it's used to count the number of unique geometries in the `trainexit` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R is used to get the number of rows in a data frame. Here, it's used to get the number of rows in the `trainexit` data frame.
-   [if_else](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/if_else) from the **dplyr** package is a vectorized conditional function that returns a value based on whether a condition is true or false. In this code, it's used to check if the number of unique geometries is equal to the number of rows in `trainexit`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since duplicates is found, we will try to check what are the duplicated value using the following code.

```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- biz[duplicated(biz) | duplicated(biz, fromLast = TRUE), ] %>%
  arrange(POI_NAME)

# Display the sorted rows with duplicate BUS_STOP_N
head(duplicates)
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `biz` data frame to find duplicates. The logical condition `duplicated(biz) | duplicated(biz, fromLast = TRUE)` keeps both the first and last occurrence of each duplicate.
-   The subset operation (`biz[...]`) is used to keep only the rows for each duplicate in `biz`.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from the **dplyr** package is used to arrange rows by column values. Here, it's used to arrange the rows of `duplicates` by 'POI_NAME'.
-   [head](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head) from **base** R is used to return the first parts of a vector, matrix, table, data frame or function. In this code, it's used to display the first few rows of the `duplicates` data frame after removing duplicates and arranging by 'POI_NAME'.
:::

Based on the table, we can see that the duplicates does comes from the same business point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 6550 to 6549

```{r}
# Keep one row of the duplicates in the original dataset
biz <- biz[!duplicated(biz) | duplicated(biz, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(biz)
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `biz` data frame to find duplicates. The logical condition `!duplicated(biz) | duplicated(biz, fromLast = TRUE)` keeps both the first and last occurrence of each duplicate.
-   The subset operation (`biz[...]`) is used to keep only one row for each duplicate in `biz`.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `biz` data frame after removing duplicates.
:::

##### distribution map

after cleaning the duplicates, now we can visualize the distribution of business point of interest using the following code

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(biz)+
  tm_dots(col = 'red') +
  tm_layout(main.title = 'Business Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_shape) from the **tmap** package is used to specify the spatial object that you want to visualize. In this code, it's used twice to specify two different spatial objects: `mpsz` and `trainexit`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_polygons) from the **tmap** package is used to create a layer of polygons. Here, it's used to create a layer of polygons from `mpsz` with 0.5 alpha (transparency).
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_dots) from the **tmap** package is used to create a layer of dots. Here, it's used to create a layer of dots from `trainexit` with blue color. The `id` argument is set to 'exit_id', which means the dots are grouped by the 'exit_id' attribute of `trainexit`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_layout) from the **tmap** package is used to set the layout of the map. Here, it's used to set the main title of the map to 'Train Station Exit Distribution Map' and position it in the center.
:::
:::

#### Entertainment

::: panel-tabset
##### import

the following code will import the entertainment point of interest dataset and assign the correct coordinate reference.

```{r}
entertn <- st_read(dsn = '../data/geospatial',
                     layer = 'entertn') %>%
  st_transform(crs = 3414)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "entertn" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `entertn` to 3414.
-   This code snippet reads and transforms spatial data related to entertainment venues, converting it to the specified CRS.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
glimpse(entertn)
```

##### check duplicates

next, check for possible duplicates using this code. *considering some entertainment might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(entertn) == nrow(entertn), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. Here, it's used to count the unique values in the entire `entertn` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `entertn`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this code, it checks if the number of unique values in `entertn` equals the total number of rows. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since duplicates is found, we will try to check what are the duplicated value using the following code.

```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- entertn[duplicated(entertn) | duplicated(entertn, fromLast = TRUE), ] %>%
  arrange(POI_NAME)

# Display the sorted rows with duplicate
head(duplicates)
```

Based on the table, we can see that the duplicates does comes from the same entertainment point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 114 to 113

```{r}
# Keep one row of the duplicates in the original dataset
entertn <- entertn[!duplicated(entertn) | duplicated(entertn, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(entertn)
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R identifies duplicate elements in a vector or rows in a data frame. Here, it's used to find duplicate rows in the `entertn` data frame.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from **dplyr** package sorts a data frame by one or more columns. In this case, `arrange` is used to sort the `duplicates` data frame by `POI_NAME`.
-   [head](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head) from **base** R retrieves the first `n` rows of a data frame or matrix. Here, it's used to display the first few rows of the sorted `duplicates` data frame.
-   This code snippet identifies and displays the first few duplicate rows in the `entertn` data frame, sorted by the Point of Interest (POI) name.
:::

##### distribution map

next, we try to visualize how does the entertainment point of interest distribution looks like in the map using the following code.

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(entertn)+
  tm_dots(col = 'cyan',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Entertainment Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_shape) from **tmap** package is used to specify the spatial object from which the spatial data will be visualized. In this code, it's applied to `mpsz` and `entertn`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_polygons) is used to create a polygons layer. The `alpha` parameter is used to adjust the transparency of the polygons.
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_dots) is used to create a dots layer. The `col` parameter is used to specify the color of the dots, and the `id` parameter is used to label the dots.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_layout) is used to set the layout parameters. In this code, it's used to set the main title of the map and its position.
:::
:::

#### Food & Beverage

::: panel-tabset
##### import

the following code will import the F&B point of interest dataset and assign the correct coordinate reference.

```{r}
fnb <- st_read(dsn = '../data/geospatial',
                     layer = 'F&B') %>%
  st_transform(crs = 3414)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "F&B" layer (likely representing food and beverage establishments) from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `fnb` to 3414.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
glimpse(fnb)
```

::: {.callout-note collapse="true" title="Functions"}
[glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

##### check duplicates

next, check for possible duplicates using this code. *considering some F & B might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(fnb) == nrow(fnb), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. Here, it's used to count the unique values in the entire `fnb` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `fnb`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this code, it checks if the number of unique values in `fnb` equals the total number of rows. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

##### distribution map

since no duplicate is found, we can immediately try to visualize how does the F&B distribution looks like in the map using the following code.

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(fnb)+
  tm_dots(col = 'magenta',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Food & Beverage Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used here for both `mpsz` (planning zones) and `fnb` (food and beverage locations).
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_polygons) adds a layer of polygons to the map, with the alpha parameter setting the transparency for the `mpsz` polygons.
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) adds a layer of dots to the map, representing `fnb` data. The dots are colored magenta and identified by `POI_NAME`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the layout of the map, including the main title and its position.
-   This code snippet creates a thematic map displaying the distribution of food and beverage outlets (`fnb`) over a minimally styled polygonal map of planning zones (`mpsz`).
:::
:::

#### Financial Services

::: panel-tabset
##### import

the following code will import the financial services point of interest dataset and assign the correct coordinate reference.

```{r}
finance <- st_read(dsn = '../data/geospatial',
                   layer = 'FinServ') %>%
  st_transform(crs = 3414)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "FinServ" layer (likely representing financial services) from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `finance` to 3414.
:::

##### glimpse

```{r}
glimpse(finance)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

##### check duplicates

next, check for possible duplicates using this code. *considering some financial services might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(finance) == nrow(finance), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. Here, it's used to count the unique values in the entire `finance` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `finance`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this code, it checks if the number of unique values in `finance` equals the total number of rows. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since duplicates is found, we will try to check what are the duplicated value using the following code.

```{r}
# Subset rows for duplicates and arrange by POI_NAME and POI_ST_NAM
duplicates <- finance[duplicated(finance) | duplicated(finance, fromLast = TRUE), ] %>%
  arrange(POI_NAME, POI_ST_NAM)

# Display the sorted rows with duplicatew
kable(head(duplicates, n=20))
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R identifies duplicate elements in a vector or rows in a data frame. Here, it's used to find duplicate rows in the `finance` data frame.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from **dplyr** package sorts a data frame by one or more columns. In this case, `arrange` is used to sort the `duplicates` data frame by `POI_NAME` and `POI_ST_NAM`.
-   [kable](https://www.rdocumentation.org/packages/knitr/versions/1.28/topics/kable) from the **knitr** package creates a simple table from a data frame or matrix. This function is used to display the first 20 rows of the `duplicates` data frame in a markdown table format.
-   This code snippet identifies, sorts, and displays the first 20 duplicate rows in the `finance` data frame, sorted by the Point of Interest (POI) name and street name.
:::

Based on the table, we can see that the duplicates does comes from the same financial service point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 3320 to 3058

```{r}
# Keep one row of the duplicates in the original dataset
finance <- finance[!duplicated(finance) | duplicated(finance, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(finance)
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the entire `finance` data frame to find duplicates.
-   The subset operation (`finance[...]`) is used to keep only one row for each duplicate in `finance`. The logical condition `!duplicated(finance)` keeps the first occurrence of each duplicate, and `duplicated(finance, fromLast = TRUE)` keeps the last occurrence.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
-   This code snippet removes duplicate rows from the `finance` data frame, retaining only one occurrence of each duplicate, and then provides an overview of the resulting data frame.
:::

##### distribution map

next, we try to visualize how does the financial services point of interest distribution looks like in the map using the following code.

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(finance)+
  tm_dots(col = 'black',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Financial Service Distribution Map', main.title.position = "center")
```
:::

#### Leisure & Recreation

::: panel-tabset
##### import

the following code will import the leisure & recreation point of interest dataset and assign the correct coordinate reference.

```{r}
lnr <- st_read(dsn = '../data/geospatial',
                   layer = 'Liesure&Recreation') %>%
  st_transform(crs = 3414)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "Liesure&Recreation" layer (presumably representing leisure and recreation locations) from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `lnr` to 3414.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
glimpse(lnr)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
-   This function is used here to display an overview of the `lnr` data frame, which contains spatial data related to leisure and recreation facilities. The output will give insights into the types and structures of the data available in `lnr`, such as column names, data types, and sample values.
:::

##### check duplicates

next, check for possible duplicates using this code. *considering some leisure & recreation might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(lnr) == nrow(lnr), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. Here, it's used to count the unique values in the entire `lnr` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `lnr`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this code, it checks if the number of unique values in `lnr` equals the total number of rows. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

##### distribution map

since no duplicate is found, we can immediately try to visualize how does the leisure & entertainment point of interest distribution looks like in the map using the following code.

```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(lnr)+
  tm_dots(col = 'coral',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Leisure & Recreation Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used here for `mpsz` and `lnr` (Leisure & Recreation) datasets.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_polygons) adds a layer of polygons to the map. For `mpsz`, the polygons are set with high transparency (`alpha = 0.01`).
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) adds a layer of dots to the map, representing the `lnr` data. The color and identifier (`id`) of the dots are specified.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the layout of the map, including the main title and its position.
-   This code snippet creates a thematic map displaying the distribution of leisure and recreation points of interest (POIs) over a minimally styled polygonal map of planning zones (`mpsz`).
:::
:::

#### Retails

::: panel-tabset
##### import

the following code will import the retails point of interest dataset and assign the correct coordinate reference.

```{r}
retails <- st_read(dsn = '../data/geospatial',
                   layer = 'Retails') %>%
  st_transform(crs = 3414)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used to read simple features from a file or database. In this code, it's applied to the `Retails` layer of the `geospatial` data source to import the spatial data as an `sf` object.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package is used to transform the coordinate reference system (CRS) of the `sf` object. In this code, it's applied to the `retails` object to change its CRS to `3414`, which is the code for SVY21 / Singapore TM projection.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R is used to provide a summary of the `sf` object, including its dimensions, geometry type, CRS, and a summary of each column. It's used to display the summary of the `retails` object after transforming its CRS.
:::

##### glimpse

using glimpse, we can see the structure of the data

```{r}
glimpse(retails)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
-   This function is used here to display an overview of the `retails` data frame. The output will give insights into the types and structures of the data available in `retails`, such as column names, data types, and sample values. Since the content and structure of the `retails` data frame are not detailed in the prompt, it's assumed to contain retail-related spatial data.
:::

##### check duplicates

next, check for possible duplicates using this code. *considering some financial services might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(retails) == nrow(retails), "no duplicates detected", "possible duplicates detected")
```

::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. Here, it's used to count the unique values in the entire `retails` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `retails`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this code, it checks if the number of unique values in `retails` equals the total number of rows. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since duplicates is found, we will try to check what are the duplicated value using the following code.

```{r}
# Subset rows for duplicates and arrange by POI_NAME and POI_ST_NAM
duplicates <- retails[duplicated(retails) | duplicated(retails, fromLast = TRUE), ] %>%
  arrange(POI_NAME, POI_ST_NAM)

# Display the sorted rows with duplicatew
kable(head(duplicates, n=20))
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `retails` data frame to find duplicates.
-   The subset operation (`retails[...]`) is used to keep only one row for each duplicate in `retails`. The logical condition `duplicated(retails)` keeps the first occurrence of each duplicate, and `duplicated(retails, fromLast = TRUE)` keeps the last occurrence.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from **dplyr** package is used to reorder rows by column values. In this code, it's used to sort the `duplicates` data frame by `POI_NAME` and `POI_ST_NAM`.
-   [kable](https://bookdown.org/yihui/rmarkdown-cookbook/kable.html) from **knitr** package is used to create a markdown table from a data frame. In this code, it's used to display the first 20 rows of the `duplicates` data frame.
:::

Based on the table, we can see that the duplicates does comes from the same retail point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 37635 to 37463

```{r}
# Keep one row of the duplicates in the original dataset
retails <- retails[!duplicated(retails) | duplicated(retails, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(retails)
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the entire `retails` data frame to find duplicates.
-   The subset operation (`retails[...]`) is used to keep only one row for each duplicate in `retails`. The logical condition `!duplicated(retails)` keeps the first occurrence of each duplicate, and `duplicated(retails, fromLast = TRUE)` keeps the last occurrence.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
-   This code snippet removes duplicate rows from the `retails` data frame, retaining only one occurrence of each duplicate, and then provides an overview of the resulting data frame.
:::

##### distribution map

next, we try to visualize how does the retail point of interest distribution looks like in the map using the following code.

```{r}
# visualize the output
tm_shape(mpsz) +
  tm_polygons(alpha = 0.01) +
  tm_shape(retails) +
  tm_dots(col = 'lightcoral', id = 'POI_NAME') +
  tm_layout(main.title="Retail Distribution Map", main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used here for `mpsz` (planning zones) and `retails` (retail locations).
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_polygons) adds a layer of polygons to the map. For `mpsz`, the polygons are set with high transparency (`alpha = 0.01`).
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) adds a layer of dots to the map, representing `retails` data. The dots are colored light coral and are identified by `POI_NAME`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the layout of the map, including the main title and its position.
-   This code snippet creates a thematic map displaying the distribution of retail outlets (`retails`) over a minimally styled polygonal map of planning zones (`mpsz`).
:::

:::

:::

### Aspatial

This subsection will import the aspatial data used in this project and check it. The data includes Passenger Volume by Origin Destination Bus Stops, HDB data which explanations can be found in [Overview] section.

#### Passenger Volume

Firstly, the following code will import the Passenger Volume by Origin Destination Bus Stops dataset. At the same time, it will also set the reference bus stop code data type to `factor` for easing compatibility issue and more efficient processing. As previously mentioned, this project will focus on weekday morning peak, so the code will also filter the data by that criteria, grouping it using the reference column, while summing the total trip for each unique grouped reference value. Finally, the code will generate output of summary statistics of the resulting dataset.

```{r}
# Load csv file
odb10 <- read_csv("../data/aspatial/origin_destination_bus_202310.csv.gz")

# change georeference data type into factors
odb10 <- odb10 %>%
  mutate(
    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)
  )

# filter and group the data
odb10 <- odb10 %>%
  filter(DAY_TYPE == "WEEKDAY", 
         TIME_PER_HOUR >= 6 & TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

# check the summary statistics of resulting dataframe
describe(odb10)
```

::: {.callout-note collapse="true" title="Functions"}
-   [read_csv](https://readr.tidyverse.org/reference/read_delim.html) from **readr** package reads a CSV file into R, converting it to a data frame.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new variables to a data frame or modify existing ones. Here, it converts `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE` to factors.
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to subset rows based on specified conditions. In this code, it filters data for weekdays during morning peak hours (6 to 9 AM).
-   [group_by](https://dplyr.tidyverse.org/reference/group_by.html) from **dplyr** package groups the data by specified columns, here by `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE`.
-   [summarise](https://dplyr.tidyverse.org/reference/summarise.html) from **dplyr** package calculates summary statistics for each group, in this case summing up `TOTAL_TRIPS`.
-   [describe](https://www.rdocumentation.org/packages/Hmisc/versions/4.4-0/topics/describe) from **Hmisc** package provides a detailed summary of an object's contents, typically offering statistics like mean, standard deviation, frequency, and others.
:::

::: {.callout-note collapse="true" title="How to Read the Output?"}
The data provides details about the origin and destination points, along with the corresponding number of total trips, time information, and categorization based on day types. The variable summaries indicate the data distribution, with details such as the unique values, frequency, and descriptive statistics for each column.
:::

#### HDB

the following code will import the HDB dataset and check the data frame.

```{r}
# Load csv file
hdb <- read_csv("../data/aspatial/hdb.csv")

# check the data
glimpse(hdb)
```

::: {.callout-note collapse="true" title="Functions"}
-   [read_csv](https://readr.tidyverse.org/reference/read_delim.html) from **readr** package reads a CSV file into R, converting it to a data frame. In this code, it reads data from a file located at `"../data/aspatial/hdb.csv"`, which likely contains information about Housing Development Board (HDB) properties.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
-   This code snippet imports and provides an overview of the `hdb` data frame, which presumably contains data related to HDB properties.
:::

To standardize the geospatial reference, we need to set the CRS code to 3414 based on the latitude and langitude values. The following code will execute it. Simultaneously, the code will also visualize the distribution of HDB across Singapore.

```{r}
hdb_sf <- st_as_sf(hdb,
                   coords = c("lng", "lat"),
                   crs = 4326) %>%
  st_transform(crs = 3414)

# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(hdb_sf) +
  tm_dots(col = 'lightblue',
          id = 'building') +
  tm_layout(main.title = 'HDB Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   The `hdb` data frame, which contains information on the HDB buildings, is converted to a spatial data frame using the `st_as_sf` function from the **sf** package. The `coords` argument specifies the columns that contain the longitude and latitude coordinates, and the `crs` argument specifies the coordinate reference system (CRS) of the data. The CRS is then transformed to 3414, which is the EPSG code for the SVY21 projection used in Singapore.
-   The `mpsz` data frame, which contains information on the subzones in Singapore, is also a spatial data frame. It is used as the base layer for the map, and the `tm_polygons` function from the **tmap** package is used to draw the polygons of the subzones. The `col` argument sets the color of the polygons to white, and the `alpha` argument sets the transparency to 0.01, making them barely visible.
-   The `hdb_sf` data frame, which contains the spatial information on the HDB buildings, is used as the overlay layer for the map, and the `tm_dots` function from the **tmap** package is used to draw the points representing the buildings. The `col` argument sets the color of the points to light blue, and the `id` argument sets the column that contains the building names.
-   The `tm_layout` function from the **tmap** package is used to customize the layout of the map. The `main.title` argument sets the title of the map to "HDB Distribution Map", and the `main.title.position` argument sets the position of the title to center.
:::

#### Schools

Geocoding involves converting an address or postal code into geographic coordinates like latitude and longitude. The [OneMap API](https://www.onemap.gov.sg/apidocs/), particularly the [Search](https://www.onemap.gov.sg/apidocs/apidocs) API by the Singapore Land Authority, facilitates this process, retrieving latitude, longitude, and x,y coordinates based on a given address or postal code.

In R, the provided code utilizes the *read_csv* function from the **readr** package to read input data from a CSV file. The geocoding is executed using the [SLA OneMap API](https://www.onemap.gov.sg/docs/#onemap-rest-apis). The **httr** package's HTTP call functions then send individual records to the OneMap geocoding server.

The geocoding operation results in two data frames: `found` for successfully geocoded records and `not_found` for unsuccessful ones. The `found` data table is merged with the initial CSV data table using a unique identifier (POSTAL) and saved as a new CSV file named `found`.

```{r}
url <- "https://www.onemap.gov.sg/api/common/elastic/search"

csv <- read_csv("../data/aspatial/Generalinformationofschools.csv")
postcodes <- csv$postal_code

found <- data.frame()
not_found <- data.frame()

for(postcode in postcodes){
  query <-list('searchVal' = postcode, 'returnGeom'='Y', 'getAddrDetails'='Y', 'pageNum' = '1')
  res  <- GET(url, query=query)
  
  if((content(res)$found)!=0)
    found<-rbind(found, data.frame(content(res))[4:13])
  else {
  not_found = data.frame(postcode)
  }
}

glimpse(found)
```

::: {.callout-note collapse="true" title="Functions"}
-   [read_csv](https://readr.tidyverse.org/reference/read_delim.html) from **readr** package reads a CSV file into R, converting it to a data frame.
-   [GET](https://httr.r-lib.org/reference/GET.html) from **httr** package sends an HTTP GET request to a specified URL. In this code, it's used to query a web API for each postcode.
-   [content](https://httr.r-lib.org/reference/content.html) from **httr** package extracts content from a response object returned by `GET`.
-   [rbind](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/rbind) from **base** R combines data frames by rows. Here, it's used to append rows to `found` or `not_found` based on the API response.
-   A for loop is used to iterate over `postcodes` and make API requests for each postcode.
-   Conditional logic (`if...else`) checks if the `found` key in the API response is non-zero (indicating successful location finding) to decide whether to append to `found` or `not_found`.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
:::

From previous in-class exercise, we already know that the not found data consist of Zhenghua Secondary School. Therefore, the following code will merge the data from previous process, then manually input the latitude and longitude data of the school.

```{r}
schools = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)

# manually add the Zhenghua Secondary School data
schools[schools$school_name == "ZHENGHUA SECONDARY SCHOOL", "results.LATITUDE"] <- 1.3887
schools[schools$school_name == "ZHENGHUA SECONDARY SCHOOL", "results.LONGITUDE"] <- 103.7652

# check the output
glimpse(schools)
```

::: {.callout-note collapse="true" title="Functions"}
-   [merge](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/merge) from **base** R combines two data frames by matching rows based on specified columns. In this code, `csv` and `found` are merged using `postal_code` from `csv` and `results.POSTAL` from `found`, with the `all = TRUE` option to include all rows from both data frames.
-   Manual data assignment is used to add specific latitude and longitude values for "ZHENGHUA SECONDARY SCHOOL" using base R's subsetting and assignment operations.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

To standardize the geospatial reference, we need to set the CRS code to 3414 based on the latitude and langitude values. The following code will execute it. Simultaneously, the code will also visualize the distribution of Schools across Singapore.

```{r}
schools_sf <- schools %>%
  rename(
    latitude = "results.LATITUDE",
    longitude = "results.LONGITUDE"
  ) %>%
  select(
    postal_code, 
    school_name, 
    latitude, 
    longitude
  ) %>%
  st_as_sf(
    coords = c("longitude", "latitude"),
    crs=4326
  ) %>%
  st_transform(
    crs = 3414
  )

# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(schools_sf) +
  tm_dots(col = 'lightgreen',
          id = 'building') +
  tm_layout(main.title = 'Schools Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [rename](https://dplyr.tidyverse.org/reference/rename.html) from **dplyr** package changes the names of specific columns in a data frame for clarity. Here, it renames columns in `schools` to `latitude` and `longitude`.
-   [select](https://dplyr.tidyverse.org/reference/select.html) from **dplyr** package subsets specific columns from a data frame, retaining only `postal_code`, `school_name`, `latitude`, and `longitude`.
-   [st_as_sf](https://r-spatial.github.io/sf/reference/st_as_sf.html) from **sf** package converts a data frame to a simple features (sf) object, specifying the coordinates for spatial data.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of the sf object.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting, used here for `mpsz` and `schools_sf`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_polygons) adds a layer of polygons to the map, representing `mpsz`.
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) adds a layer of dots to the map, representing `schools_sf`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the layout of the map, including the main title and its position.
-   This code snippet creates a thematic map showing the distribution of schools over a map with minimal polygon representation.
:::

# Data Wrangling

::: panel-tabset

The following section will do various data wrangling from creating the hexagon layer, combining the data, duplicate checking, constructing Origin-Destination matrix, creating distance variable, and removing intra-zonal flows.

## Create Hexagon Layer

In this part, the following code will create honeycomb grid with hexagons with a distance of 375m from the center to the midpoint of the edge as the traffic analysis zone (TAZ). These hexagons will be the zones for the analysis. Simultaneously, the code will also intersect the grid with the bus stop data, and filter out grid that does not have any bus stop.

```{r}
# Create hexagonal grid based on bus stop locations
hexagonal_grid <- st_make_grid(busstop,
                               cellsize = 750,
                               what = "polygons",
                               square = FALSE)

# Convert hexagonal grid to spatial dataframe
hex_grid_sf <- st_sf(hex_grid = hexagonal_grid) %>%
  mutate(hexagon_id = 1:length(lengths(hexagonal_grid)))

# Count the number of bus stops within each hexagon
hex_grid_sf$num_bus_stops = lengths(st_intersects(hex_grid_sf, busstop))

# Filter hexagons with at least one bus stop
hexagons_with_bus_stops <- filter(hex_grid_sf, num_bus_stops > 0)

# check the output
glimpse(hexagons_with_bus_stops)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_make_grid](https://r-spatial.github.io/sf/reference/st_make_grid.html) from **sf** package creates a grid over a spatial object (`busstop`). Here, it generates a hexagonal grid with a specified cell size, forming polygons instead of squares.
-   [st_sf](https://r-spatial.github.io/sf/reference/st_sf.html) from **sf** package converts the hexagonal grid into an `sf` (simple features) object.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new columns to a data frame or modify existing ones. Here, it's used to create a `hexagon_id` column.
-   [lengths](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lengths) from **base** R calculates the lengths of the elements in a list. It's used here to assign IDs to each hexagon.
-   [st_intersects](https://r-spatial.github.io/sf/reference/st_intersects.html) from **sf** package finds the intersection between two spatial objects, in this case, between each hexagon and the bus stops.
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to retain rows based on a specified condition. Here, it filters hexagons that contain at least one bus stop.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
:::

Next, we will visualize the distribution of bus stop across Singapore.

```{r}
# Plot the number of bus stops per hexagon with a color gradient and legend
tm_shape(hexagons_with_bus_stops) +
  tm_borders() +
  tm_fill("num_bus_stops",
          title = "Bus Stop Density",
          style = "jenks",
          palette = "YlOrRd",
          legend.show = TRUE) +
  tm_layout(main.title = 'Bus Stop Density in Hexagonal Grid',
            main.title.position = "center",
            legend.position = c("right", "top"))
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_make_grid](https://r-spatial.github.io/sf/reference/st_make_grid.html) from **sf** package is used to create a grid over a spatial object (`busstop`). Here, it generates a hexagonal grid with a specified cell size.
-   [st_sf](https://r-spatial.github.io/sf/reference/st_sf.html) from **sf** package converts the hexagonal grid into an `sf` (simple features) object.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new columns to a data frame or modify existing ones. Here, it's used to create a `hexagon_id` column.
-   [lengths](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lengths) from **base** R calculates the lengths of the elements in a list. It's used here to assign IDs to each hexagon.
-   [st_intersects](https://r-spatial.github.io/sf/reference/st_intersects.html) from **sf** package finds the intersection between two spatial objects, in this case, between each hexagon and the bus stops.
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to retain rows based on a specified condition. Here, it filters hexagons that contain at least one bus stop.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
:::

## Combine Bus Stop and Hexagon

In this step, the following code will join the bus stop and hexagon data, and filter out only the required columns for following sections.

```{r}
# Perform a spatial join
busstop_with_hex_info <- st_join(busstop, hexagons_with_bus_stops, by = NULL, join = st_within)

# Select the relevant columns
busstop_with_hex_info <- busstop_with_hex_info %>%
  select(BUS_STOP_N, BUS_ROOF_N, LOC_DESC, hexagon_id, geometry) %>%
  mutate(
    BUS_STOP_N = as.factor(BUS_STOP_N)
  )

# Display the resulting data
glimpse(busstop_with_hex_info)
```

::: {.callout-note collapse="true" title="Functions"}
-   [st_join](https://r-spatial.github.io/sf/reference/st_join.html) from **sf** package performs a spatial join between two spatial objects. In this case, it joins `busstop` with `hexagons_with_bus_stops`. The `join = st_within` argument specifies that bus stops are joined with hexagons in which they are located.
-   [select](https://dplyr.tidyverse.org/reference/select.html) from **dplyr** package subsets specific columns from the joined spatial data frame. This code retains columns like `BUS_STOP_N`, `BUS_ROOF_N`, `LOC_DESC`, `hexagon_id`, and `geometry`.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new variables or modify existing ones. Here, it converts `BUS_STOP_N` to a factor.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, giving a quick look at its structure, including the types of columns and the first few entries in each column.
:::

## Combine Trips and Hexagon

now, we will finally join the passenger trip data with the hex and bus stop data. considering that the data have origin and destination bus stop, we will also have hex for both.

```{r}
# Add origin_hex to odb10
odb_hex <- odb10 %>%
  left_join(busstop_with_hex_info %>% select(BUS_STOP_N, hexagon_id), 
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(origin_hex = hexagon_id)

# Add destination_hex to odb10
odb_hex <- odb_hex %>%
  left_join(busstop_with_hex_info %>% select(BUS_STOP_N, hexagon_id), 
            by = c("DESTINATION_PT_CODE" = "BUS_STOP_N")) %>%
  rename(destination_hex = hexagon_id)

# Remove additional geometry columns
odb_hex <- odb_hex %>%
  select(-contains("geometry."))

# Convert hex id to factor
odb_hex <- odb_hex %>%
  mutate(
    origin_hex = as.factor(origin_hex),
    destination_hex = as.factor(destination_hex)
  )

# check the output
glimpse(odb_hex)
```

::: {.callout-note collapse="true" title="Functions"}
-   [left_join](https://dplyr.tidyverse.org/reference/join.html) from **dplyr** package merges two data frames by matching values in specified columns. Here, it's used twice to merge `busstop_with_hex_info` with `odb10`, first for origin and then for destination bus stops, based on matching bus stop codes.
-   [select](https://dplyr.tidyverse.org/reference/select.html) from **dplyr** package subsets specific columns from a data frame. It's used to select `BUS_STOP_N` and `hexagon_id` from `busstop_with_hex_info`.
-   [rename](https://dplyr.tidyverse.org/reference/rename.html) from **dplyr** package changes the names of specific columns in a data frame. In this code, `hexagon_id` is renamed to `origin_hex` and `destination_hex`.
-   [contains](https://dplyr.tidyverse.org/reference/select.html) within `select(-contains("geometry."))` is a helper function from **dplyr** used to remove columns that contain a specific string (`"geometry."` in this case).
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to convert `origin_hex` and `destination_hex` to factors.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
:::

## Final Duplicate Checking

Before going into the next step, the following code will firstly make sure that there are no duplicates in the dataset.

```{r}
if_else(n_distinct(odb_hex) == nrow(odb_hex), "no duplicates detected", "possible duplicates detected")
```
::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. Here, it's used to count the unique values in the entire `odb_hex` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `odb_hex`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this code, it checks if the number of unique values in `odb_hex` equals the total number of rows. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

## Construct O-D Matrix

Next, using the following code, we will build the O-D matrix which represent various which aggregate the trips on a one on one origin vs destination basis.

```{r}
# Remove rows with missing values
clean_odb <- odb_hex[complete.cases(odb_hex), ]

# Aggregate data based on origin_hex and destination_hex
odb_flow <- clean_odb %>%
  group_by(origin_hex, destination_hex) %>%
  summarise(trips = sum(TRIPS),
            origin_hex = unique(origin_hex),
            destination_hex = unique(destination_hex)) %>%
  ungroup()

# check the output
glimpse(odb_flow)
```

::: {.callout-note collapse="true" title="Functions"}
-   [complete.cases](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/complete.cases) from **base** R finds rows with no missing values (`NA`). Here, it's used to filter `odb_hex` and remove rows with missing data.
-   [group_by](https://dplyr.tidyverse.org/reference/group_by.html) from **dplyr** package groups the data by specified columns, here by `origin_hex` and `destination_hex`.
-   [summarise](https://dplyr.tidyverse.org/reference/summarise.html) from **dplyr** package calculates summary statistics for each group, here summing up `TRIPS` and getting unique values for `origin_hex` and `destination_hex`.
-   [ungroup](https://dplyr.tidyverse.org/reference/ungroup.html) from **dplyr** package removes the grouping structure from the data frame.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
:::

## Create Distance Variable

After getting the origin-destination matrix, we need to calculate the distance between each origin and destination pair which will be used later for creating desire lines. The following code chunk will execute the required steps.

```{r}
# set busstop to spatial
busstop_sp <- as(busstop_with_hex_info, "Spatial")

# calculate the distance
tp_dist <- sp::spDists(busstop_sp, longlat = FALSE)

# add column names to the variable
hex_id_names <- busstop_sp$hexagon_id
colnames(tp_dist) <- paste0(hex_id_names)
rownames(tp_dist) <- paste0(hex_id_names)

# melt the table
dist <- reshape2::melt(tp_dist) %>%
  rename(origin_hex = Var1,
         destination_hex = Var2,
         distance = value) %>%
  mutate(origin_hex = as.factor(origin_hex),
         destination_hex = as.factor(destination_hex))

# remove duplicates
dist <- distinct(dist, origin_hex, destination_hex, .keep_all = TRUE)

# Perform left join to odb_flow with factor conversion
odb_flow <- odb_flow %>%
  left_join(
    dist %>% select(origin_hex, destination_hex, distance) %>%
      mutate(across(c(origin_hex, destination_hex), as.character)),
    by = c("origin_hex", "destination_hex")
  ) %>%
  mutate(across(c(origin_hex, destination_hex), as.factor))

# check the output
glimpse(odb_flow)
```

::: {.callout-note collapse="true" title="Functions"}
-   [as](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/as) from **base** R converts an object from one class to another, here converting `busstop_with_hex_info` to a Spatial object (`Spatial`).
-   [spDists](https://www.rdocumentation.org/packages/sp/versions/1.4-2/topics/spDists) from **sp** package calculates distances between points in a spatial object. The `longlat = FALSE` parameter indicates that the coordinates are in a planar projection.
-   [paste0](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/paste) from **base** R concatenates strings without a separator. It's used to create column and row names based on `hexagon_id`.
-   [melt](https://www.rdocumentation.org/packages/reshape2/versions/1.4.3/topics/melt) from **reshape2** package reshapes data from wide to long format. Here, it's used to transform the distance matrix into a long format data frame.
-   [rename](https://dplyr.tidyverse.org/reference/rename.html) and [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package are used to rename columns and modify their values, respectively.
-   [distinct](https://dplyr.tidyverse.org/reference/distinct.html) from **dplyr** package removes duplicate rows from a data frame.
-   [left_join](https://dplyr.tidyverse.org/reference/join.html) from **dplyr** package merges two data frames based on matching values in specified columns.
-   [across](https://dplyr.tidyverse.org/reference/across.html) from **dplyr** package is used in `mutate` to apply a function (here, `as.character` or `as.factor`) to multiple columns.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
:::

## Remove Intra-Zonal Flows

This analysis is focused on the traffic between zones. Therefore the following code chunk will remove intra-zonal flows.

```{r}
odb_flow <- odb_flow %>%
  filter(as.character(origin_hex) != as.character(destination_hex))

# check the output
glimpse(odb_flow)
```

::: {.callout-note collapse="true" title="Functions"}
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to retain rows in a data frame based on a specified condition. In this code, it filters the `odb_flow` data frame to keep only the rows where `origin_hex` and `destination_hex` are different.
-   The `as.character` function from **base** R is used to ensure that the comparison between `origin_hex` and `destination_hex` is done as character strings, likely because these columns are factors or some other class.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
-   This code snippet modifies the `odb_flow` data frame to exclude rows where the origin and destination hexagons are the same, then provides an overview of the resulting data.
:::

::: {.callout-note collapse="true" title="Why intra-zonal flows is removed?"}
The exclusion of intra-zonal flows from the Spatial Interaction Model calibration process is driven by the desire to focus on modeling and understanding interactions between different zones, without interference from flows within the same zone.

Intra-zonal flows, representing movements within a single zone, may not contribute significantly to the overall spatial interaction patterns that the model aims to capture. By removing intra-zonal flows, the analysis can concentrate on the dynamics and factors influencing interactions between distinct zones, providing a clearer and more interpretable representation of spatial relationships.

Including intra-zonal flows in the model might introduce unnecessary complexity or noise, potentially affecting the accuracy and interpretability of the model's results. Therefore, excluding intra-zonal flows is a methodological choice to streamline the analysis and enhance the model's ability to capture meaningful inter-zonal interactions.
:::

:::

# Visualize the Desire Lines

The od2line function plays a pivotal role in transforming origin-destination data into a format suitable for generating flow lines on a map. This function requires three essential arguments: flow, zones, and zone_code.

1.  **flow:** This is a data frame that encapsulates the origin-destination data. The first column of this data frame should align with the first column of the data present in the zones dataframe. In the context of the provided R code snippet, the variable 'odb_flow' is used as the flow data frame.

2.  **zones:** Here, the origin and destination points of the travels are specified. This argument is crucial for mapping out the flow lines accurately. The actual geographical data, represented by the dataframe 'hexagons_with_bus_stops,' is employed as the zones dataframe in the given code.

3.  **zone_code:** This parameter involves providing the name of the variable within the 'zones' dataframe that contains the unique identifiers (IDs) of the zones. In the presented R code, the variable 'hexagon_id' serves this purpose, linking the flow data with the corresponding geographical zones.

To demonstrate the practical implementation of the od2line function, the provided code snippet utilizes the 'odb_flow' as the origin-destination data, 'hexagons_with_bus_stops' as the geographical zones, and designates 'hexagon_id' as the variable containing zone IDs. The resulting flowline object is then explored using the 'glimpse' function, offering a concise overview of its structure and contents.

```{r}
flowline <- od2line(flow = odb_flow, 
        zones = hexagons_with_bus_stops,
        zone_code = 'hexagon_id')

glimpse(flowline)
```

::: {.callout-note collapse="true" title="Functions"}
-   `od2line` is function from \[od\](https://itsleeds.github.io/od/#:\~:text=The%20od2line()%20function%2C%20for,movement%20between%20origins%20and%20destinations%3A&text=%23%3E%202%20LINESTRING%20(%2D1.518911%2053.79...) used for converting origin-destination data (`odb_flow`) into spatial lines. It takes flow data and corresponding spatial zones (`hexagons_with_bus_stops`), and uses a zone identifier (`zone_code` set as 'hexagon_id') to create spatial lines representing the flow between origins and destinations.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

## 3rd Quantile Trips (175)

Considering the volume of the flowline, we will try to focus on the most important part of the flows. Firstly, we will try to filter on the 3rd Quantile. To do that, the following code will get the quantile of the trips.

```{r}
summary(flowline$trips)
```

::: {.callout-note collapse="true" title="Functions"}
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R provides a concise summary of a vector, list, data frame, or other R objects. In this code, it is used to produce a summary of the `trips` column in the `flowline` data frame.
-   This summary typically includes the minimum, maximum, median, first quartile, third quartile, and mean of the numeric data in the `trips` column.
-   This code snippet is useful for getting a quick statistical overview of the `trips` data, such as understanding its distribution, central tendency, and spread.
:::

We can see that the 3rd quantile is at 175 trips. The number will be used to plot the desire lines on the following code.

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(trips >= 175) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7),
           alpha = 0.2)+
  tm_layout(main.title = 'Weekday Morning Peak Flow for 3rd Quantile Trips', main.title.position = 'center')
```

::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tmap_mode) and [tmap_options](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tmap_options) from **tmap** package are used to set up the mode and options for thematic maps.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tm_shape) is used to specify the spatial object to be visualized.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tm_polygons) is used to add polygons to the map.
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to filter rows in the `flowline` data frame where `trips` is greater than or equal to 175.
-   [tm_lines](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tm_lines) is used to add lines to the map.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tm_layout) is used to modify the layout of the map.
:::

The central and eastern regions of Singapore show a denser accumulation of flows, indicated by the more intense coral coloration. This suggests higher bus trip volumes in these areas, possibly reflecting a greater demand for public transportation due to higher population density and commercial activity.

The visual spread of the flow lines, with varying line widths representing trip volumes, illustrates that the central region, likely encompassing the Central Business District (CBD), is a major hub of commuter activity, with thick lines indicating a high concentration of bus trips.

The eastern region, possibly including areas like Changi and the vicinity of the East Coast, shows a significant amount of flow as well. This could be attributed to the presence of residential areas, business parks, and the airport, all generating substantial public transport usage.

The map also shows a relatively lighter flow in the northern and western regions, suggesting either fewer bus trips or lower volumes within the 3rd quantile range.

## 99th Percentile Trips (6082)

Since filtering on 3rd quantile shows that the map is still hardly distinguishable, next we will try to use percentile. The following code will show the cutoff for the percentiles.

```{r}
quantile(flowline$trips, probs = seq(0, 1, 0.01), na.rm = TRUE)
```

::: {.callout-note collapse="true" title="Functions"}
-   [quantile](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/quantile) from **stats** package in base R computes sample quantiles for a given set of data. In this code, it's used to calculate quantiles for the `trips` column in the `flowline` data frame.
-   `probs = seq(0, 1, 0.01)` specifies the probabilities for which quantiles should be calculated, creating a sequence from 0 to 1 in increments of 0.01 (i.e., percentiles).
-   `na.rm = TRUE` indicates that NA (missing) values should be removed before the calculation.
-   This code snippet produces a detailed set of percentiles (from 0th to 100th, at every 1%) for the `trips` data, useful for understanding its distribution in greater detail, including identifying any potential outliers or patterns in the data spread.
:::

We can see that distinguishable increase can be seen on the 99th percentile at 6082 trips. The number will be used to plot the desire lines on the following code.

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(trips >= 6082) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7))+
  tm_layout(main.title = 'Weekday Morning Peak Flow for 99th Percentile Trips', main.title.position = 'center',
            main.title.size = 1.2)
```

::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_mode) from **tmap** package sets the mode for creating maps. Here, the mode is set to `'plot'` for static map rendering.
-   [tmap_options](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_options) configures options for tmap functions. The `check.and.fix = TRUE` option is used to automatically check and fix invalid polygons in spatial data.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) prepares spatial data for plotting, used here for `mpsz` and `hexagons_with_bus_stops`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_polygons) adds polygon layers. For `mpsz`, polygons are filled with white color; for `hexagons_with_bus_stops`, polygons are filled with gray color and partial transparency (`alpha = 0.4`).
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to subset `flowline` data based on the number of trips, retaining lines with trips of at least 6082.
-   [tm_lines](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_lines) adds a layer of lines representing `flowline`. Line properties like color (`col`), width (`lwd`), and scaling are specified.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the map layout, including the main title and its style.
-   This code snippet creates a thematic map that visualizes "Weekday Morning Peak Flow for 99th Percentile Trips", combining polygonal maps of planning zones and hexagons with flow lines based on the specified criteria.
:::

The major flow lines, represented in coral, indicate a concentration of the highest volume bus trips. These lines are more pronounced, showing that certain routes experience significantly higher bus traffic compared to others.

There is a noticeable pattern of high-volume bus traffic from the north towards the downtown area, indicating a substantial commuter influx into the central business district from northern residential areas.

Similarly, increased bus flows from the west towards downtown suggest a heavy commuting pattern, which could be due to residential-to-business daily travel.

The north-eastern region also exhibits a strong flow towards the central areas, again possibly reflecting the movement of residents to central workplaces or educational institutions.

Short trips within regions have become more visible at this percentile, indicating that there are specific short-distance routes that are heavily utilized and might be of interest for targeted transport planning.

## 3rd Quantile Distance (8923)

Next, we will try to show desire lines with a filter on the distance to see where are the longer distant travels happening. Firstly, we need to check the quantile cutoff with the following code.

```{r}
summary(flowline$distance)
```
::: {.callout-note collapse="true" title="Functions"}
-   [quantile](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/quantile) from **stats** package in base R computes sample quantiles for a given set of data. In this code, it's used to calculate quantiles for the `distance` column in the `flowline` data frame.
-   `probs = seq(0, 1, 0.01)` specifies the probabilities for which quantiles should be calculated, creating a sequence from 0 to 1 in increments of 0.01 (i.e., percentiles).
-   `na.rm = TRUE` indicates that NA (missing) values should be removed before the calculation.
-   This code snippet produces a detailed set of percentiles (from 0th to 100th, at every 1%) for the `distance` data, useful for understanding its distribution in greater detail, including identifying any potential outliers or patterns in the data spread.
:::

::: {.callout-note collapse="true" title="Functions"}
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R provides a concise summary of a vector, list, data frame, or other R objects. In this code, it is used to produce a summary of the `distance` column in the `flowline` data frame.
-   This summary typically includes the minimum, maximum, median, first quartile, third quartile, and mean of the numeric data in the `distance` column.
-   This code snippet is useful for getting a quick statistical overview of the `distance` data, such as understanding its distribution, central tendency, and spread.
:::

We can see that the 3rd quantile is at 8923 distant. The number will be used to plot the desire lines on the following code.

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(distance >= 8923) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7),
           alpha = 0.2)+
  tm_layout(main.title = 'Weekday Morning Peak Flowline for 3rd Quantile Distance', main.title.position = 'center',
            main.title.size = 1.2)
```

::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tmap_mode) from **tmap** package sets the mode for creating thematic maps. In this code, it's set to 'plot' to produce static maps.
-   [tmap_options](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tmap_options) from **tmap** package sets the global options for thematic maps. In this code, it's used to enable the check and fix option, which automatically fixes invalid geometries.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tm_shape) from **tmap** package specifies the spatial data to be mapped. In this code, it's used to map the `mpsz` and `hexagons_with_bus_stops` data frames, which contain polygon geometries, and the `flowline` data frame, which contains line geometries.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tm_polygons) from **tmap** package adds polygons to the map. In this code, it's used to fill the polygons with white and gray colors, and adjust the transparency with the alpha argument.
-   [tm_lines](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tm_lines) from **tmap** package adds lines to the map. In this code, it's used to color the lines with coral, vary the line width with the `trips` variable, and scale the line width with the scale argument.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-2/topics/tm_layout) from **tmap** package sets the layout of the map. In this code, it's used to add a main title, position it at the center, and adjust its size.
:::

The visualization indicates that longer-distance travel is a significant component of morning commuting, highlighting the importance of efficient transportation links from suburban areas to central regions.

The gray hexagonal overlay helps to provide context regarding the distribution of bus stops; however, the flow lines' density makes it difficult to ascertain the exact starting and ending points of these longer commutes.

Although the map is crowded, the saturated areas of flow could signal potential bottlenecks or areas that may benefit from transportation infrastructure improvements, such as increased bus services or the development of express routes.

## 99th Percentile Distance (18587)

Since the previous filtering on 3rd quantile resulting in hardly distinguishable pattern, we will try to use percentile next. The following code will check what are the percentile for distance.

```{r}
quantile(flowline$distance, probs = seq(0, 1, 0.01), na.rm = TRUE)
```
::: {.callout-note collapse="true" title="Functions"}
-   [quantile](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/quantile) from **stats** package in base R computes sample quantiles for a given set of data. In this code, it's used to calculate quantiles for the `distance` column in the `flowline` data frame.
-   `probs = seq(0, 1, 0.01)` specifies the probabilities for which quantiles should be calculated, creating a sequence from 0 to 1 in increments of 0.01 (i.e., percentiles).
-   `na.rm = TRUE` indicates that NA (missing) values should be removed before the calculation.
-   This code snippet produces a detailed set of percentiles (from 0th to 100th, at every 1%) for the `distance` data, useful for understanding its distribution in greater detail, including identifying any potential outliers or patterns in the data spread.
:::

Following the previous filter, we also use 99th percentile at 18587 distance for the following plot.

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(distance >= 18587) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7))+
  tm_layout(main.title = 'Weekday Morning Peak Flowline for 99th Percentile Distance', main.title.position = 'center',
            main.title.size = 1.2)
```

::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_mode) from **tmap** package sets the mode for creating maps. Here, the mode is set to `'plot'` for static map rendering.
-   [tmap_options](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_options) configures options for tmap functions. The `check.and.fix = TRUE` option is used to automatically check and fix invalid polygons in spatial data.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) prepares spatial data for plotting, used here for `mpsz` and `hexagons_with_bus_stops`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_polygons) adds polygon layers. For `mpsz`, polygons are filled with white color; for `hexagons_with_bus_stops`, polygons are filled with gray color and partial transparency (`alpha = 0.4`).
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to subset `flowline` data based on distance, retaining lines with a distance of at least 18587 units.
-   [tm_lines](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_lines) adds a layer of lines representing `flowline`. Line properties like color (`col`), width (`lwd`), and scaling are specified.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the map layout, including the main title and its style.
-   This code snippet creates a thematic map that visualizes "Weekday Morning Peak Flowline for 99th Percentile Distance", combining polygonal maps of planning zones and hexagons with flow lines based on the specified criteria.
:::

There is a pronounced convergence of bus flows towards the downtown area from all directions, which indicates a significant influx of commuters heading to the central business district. This could reflect the centralized location of employment, educational institutions, and other key services in the city center.

The flow between the north and east regions also shows a heightened density, suggesting a well-traveled corridor that connects these areas. This could be indicative of important residential to commercial commuting patterns or the presence of specific transit routes that facilitate travel between these regions.

The map, with its dense coral lines, indicates that even at the 99th percentile for distance, there are numerous long-distance routes with substantial ridership, underlining the importance of these bus services in the overall transportation network.

## Spread Across Decile of Trips

The last desire line map chart will try to see the spread of trips across decile. Firstly, we check the decile cutoff of trips using the following code.

```{r}
quantile(flowline$trips, probs = seq(0, 1, 0.1), na.rm = TRUE)
```

::: {.callout-note collapse="true" title="Functions"}
-   [quantile](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/quantile) from **stats** package in base R computes sample quantiles for a given set of data. In this code, it's used to calculate quantiles for the `distance` column in the `flowline` data frame.
-   `probs = seq(0, 1, 0.01)` specifies the probabilities for which quantiles should be calculated, creating a sequence from 0 to 1 in increments of 0.01 (i.e., percentiles).
-   `na.rm = TRUE` indicates that NA (missing) values should be removed before the calculation.
-   This code snippet produces a detailed set of percentiles (from 0th to 100th, at every 1%) for the `distance` data, useful for understanding its distribution in greater detail, including identifying any potential outliers or patterns in the data spread.
:::

Next, we define the breaks and plot the chart that will show the spread of trips across decile of cutoff (with an additional arbitrary cutoff on high increase in trip volume). Then, the following code will also assign the cutoff as a column in the flowline dataframe.

```{r}
flowline <- flowline %>%
  mutate(
    decile = cut(trips, 
                         breaks = c(0, 2, 5, 11, 21, 38, 67, 125, 254, 661, 5000, Inf),
                         labels = c("<2", "2-5", "5-11", "11-21",
                                    "21-38", "38-67", "67-125", "125-254",
                                    "254-661", "661-5000", ">5000"),
                         ordered_result=TRUE
))

# check the output
glimpse(flowline)
```

::: {.callout-note collapse="true" title="Functions"}
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new variables or modify existing ones in a data frame. In this code, it adds a `decile` column to the `flowline` data frame.
-   [cut](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/cut) from **base** R divides the range of a numeric vector (`trips` in this case) into intervals and codes the values according to which interval they fall. The `breaks` parameter defines the interval boundaries, and `labels` provides labels for these intervals. `ordered_result = TRUE` makes the result an ordered factor.
-   This code snippet categorizes the `trips` data into deciles based on specified break points and labels each interval accordingly.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
:::

Based on the new ordinal column of decile group, we can plot the following map chart that shows the spread of trips volume.

```{r}
#| eval: false
trips_spread <- tm_shape(mpsz) +
  tm_fill(col = "white") +
  tm_borders(col = "black", lwd = .5) +
tm_shape(flowline) +
  tm_lines(lwd = "trips", scale = 1.5, col = "coral", alpha = .7) +
  tm_layout(title = "Weekday Morning Peak Traffic Flow", title.position = c("right", "top")) +
  tm_facets(along = "decile", free.coords = FALSE)

# Save animation as gif
tmap_animation(trips_spread, "../images/trips_spread.gif", loop = TRUE, delay = 300,
               outer.margins = NA, restart.delay=500)
```

![](../images/trips_spread.gif) for the first 8 decile, the trips slightly change but still more dense on central. on the extra category of \>5000, north-downtown, west-downtown, north-east, and short trip becomes more visible

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used for both `mpsz` and `flowline` datasets.
-   [tm_fill](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_fill) adds a layer of filled polygons, and [tm_borders](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_borders) adds polygon borders.
-   [tm_lines](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_lines) adds a layer of lines, with properties (like width and color) set based on the `trips` variable.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the layout of the map, including the title and its position.
-   [tm_facets](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_facets) creates multiple panels (facets) in the map based on a variable, here "decile".
-   [tmap_animation](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_animation) from **tmap** package creates an animation from a series of thematic maps. It's used to save the animation of `trips_spread` as a GIF file, with specified loop, delay, margins, and restart delay settings.
-   This code creates an animated map visualizing "Weekday Morning Peak Traffic Flow" across different deciles, saved as a GIF file.
:::

Initial Deciles (\<2 to 125-254): In the earlier frames representing the lower deciles, the trip volumes show gradual changes, with a consistent pattern of denser traffic in the central region. This likely reflects a stable flow of bus trips in the heart of the city, where there is a concentration of work, education, and commercial activities.

Higher Deciles (254-661 to \>5000): As the deciles increase, the traffic flow becomes more pronounced, particularly in routes leading from the north, west, and northeast towards downtown. These routes exhibit significant growth in trip volumes, highlighting key commuter corridors.

Extreme Volume Category (\>5000): In the final frames, where the cutoff is for volumes greater than 5000 trips, the visualization emphasizes the most heavily utilized routes. The north to downtown, west to downtown, northeast, and certain short-distance routes stand out more distinctly, showing where the highest demand for bus transport exists.

# Assembling Variables

To better design our Spatial Interaction Model performance, we need to assemble the variables that. In the gravity model, those variables include distance, and push and pull factors.

## Push Factors

For the push factors we choose counts of HDB, station, train exit, and bus stop in the zone. Additionally we also try to estimate proxy population data using the HDB sales data.

::: {.callout-note collapse="true" title="why those variables are chosen?"}
HDB counts: in a zone serve as a proxy for residential density. Residential areas are likely to generate a substantial number of bus trips, as residents use buses for daily commuting, shopping, and other activities. Higher HDB counts may indicate higher population density and, subsequently, increased demand for bus services.

Station Count: The count of stations within a zone is crucial, especially if these stations connect to other modes of transportation or are busy hubs. Stations can act as both push and pull factors. Residents in a zone with multiple stations might use buses to access these stations, and conversely, individuals arriving at stations might use buses for the next leg of their journey.

Train Exit Count: Similar to stations, the count of train exits represents points of convergence where commuters might either board buses to reach their final destination or disembark from buses to access train services. This bidirectional interaction makes train exits a significant factor in predicting bus trip volumes.

Bus Stop Count: The count of bus stops is a straightforward push factor. More bus stops in a zone imply greater accessibility and convenience for residents, encouraging higher bus usage. Bus stops are critical nodes for the initiation of bus trips, and their density reflects the local transportation network's coverage.

Proxy Population Data using HDB Sales: While not a direct count of population, HDB sales data provides an indirect measure of population changes over time. Increased HDB sales may indicate population growth or influx, affecting the demand for public transportation. It serves as a dynamic proxy for the population, capturing changes in residency patterns and influencing bus trip volumes.
:::

The following code will create a dataframe to hold all the factors before merged into the main dataframe. Then it assign all the common push factors count to the dataframe.

```{r}
factors_holder <- hexagons_with_bus_stops %>%
  rename(push_num_bus_stops = num_bus_stops)

# define function to add push poi counts columns
add_push_poi_counts <- function(factors_holder, poi_datasets) {
  # Loop through each POI dataset
  for (poi_name in poi_datasets) {
    # Add a new column with the count using st_intersects and lengths
    factors_holder[[paste0("push_", poi_name, "_count")]] <- 
      ifelse(
        lengths(st_intersects(factors_holder, get(poi_name))) == 0,
        0.99,
        lengths(st_intersects(factors_holder, get(poi_name)))
      )
  }
  
  # Return the updated factors_holder dataframe
  return(factors_holder)
}

# List of POI dataset names
push_poi_datasets <- c("station", "trainexit", "hdb_sf")

# Call the function
factors_holder <- add_push_poi_counts(factors_holder, push_poi_datasets)

# check the output
glimpse(factors_holder)
```

::: {.callout-note collapse="true" title="Functions"}
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new variables or modify existing ones in a data frame. Here, it creates a `capacity` column based on a calculation involving various room types.
-   [select](https://dplyr.tidyverse.org/reference/select.html) from **dplyr** package is used to subset specific columns in the data frame.
-   [st_join](https://r-spatial.github.io/sf/reference/st_join.html) from **sf** package performs a spatial join between two spatial objects, joining `hdb_capacity` with `hexagons_with_bus_stops` based on their spatial relationship (`st_within`).
-   [group_by](https://dplyr.tidyverse.org/reference/group_by.html) and [summarise](https://dplyr.tidyverse.org/reference/summarise.html) from **dplyr** package are used for grouping data and calculating summary statistics, here summing up `capacity` for each `hexagon_id`.
-   [st_drop_geometry](https://r-spatial.github.io/sf/reference/st_geometry.html) from **sf** package is used to remove the geometry column from a spatial object.
-   [left_join](https://dplyr.tidyverse.org/reference/join.html) from **dplyr** package merges two data frames based on matching values in a specified column.
-   [mutate_at](https://dplyr.tidyverse.org/reference/mutate_all.html) from **dplyr** package is used to apply a function to multiple columns that match a condition (here, columns that contain "push_est_pop").
-   [coalesce](https://dplyr.tidyverse.org/reference/coalesce.html) from **dplyr** package replaces `NA` values with a specified value (0.99 in this case).
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

Next, we will try to approximate population of the location, based on estimated hdb capacity, and assign it as a push factor as well.

```{r}
hdb_capacity <- hdb_sf %>%
  mutate(capacity = `1room_sold` *1 +
           `2room_sold` * 2+
           `3room_sold` * 3+
           `4room_sold` * 4+
           `5room_sold` * 5+
           exec_sold * 4+
           multigen_sold * 6+
           studio_apartment_sold *1+
           `1room_rental` * 1+
           `2room_rental` * 2+
           `3room_rental` * 3+
           other_room_rental * 2) %>%
  select(blk_no, geometry, capacity)

# get hexagon_id for hdb, then sum the capacity
hdb_capacity <- st_join(hdb_capacity, hexagons_with_bus_stops, by = NULL, join = st_within) %>%
  group_by(hexagon_id) %>%
  summarise(push_est_pop = sum(capacity)) %>%
  st_drop_geometry()

# Convert hexagon_id to character in both dataframes
factors_holder <- factors_holder %>% mutate(hexagon_id = as.character(hexagon_id))
hdb_capacity <- hdb_capacity %>% mutate(hexagon_id = as.character(hexagon_id))

# Left join factors_holder and hdb_capacity by hexagon_id
factors_holder <- left_join(factors_holder, hdb_capacity, by = "hexagon_id") %>%
  mutate(hexagon_id = as.factor(hexagon_id)) %>%  # Convert back to factor
  mutate_at(vars(contains("push_est_pop")), list(~coalesce(., 0.99)))  # Fill missing values with 0.99 

glimpse(factors_holder)
```

::: {.callout-note collapse="true" title="Functions"}
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new variables or modify existing ones in a data frame. Here, it creates a `capacity` column based on a calculation involving various room types.
-   [select](https://dplyr.tidyverse.org/reference/select.html) from **dplyr** package is used to subset specific columns in the data frame.
-   [st_join](https://r-spatial.github.io/sf/reference/st_join.html) from **sf** package performs a spatial join between two spatial objects, joining `hdb_capacity` with `hexagons_with_bus_stops` based on their spatial relationship (`st_within`).
-   [group_by](https://dplyr.tidyverse.org/reference/group_by.html) and [summarise](https://dplyr.tidyverse.org/reference/summarise.html) from **dplyr** package are used for grouping data and calculating summary statistics, here summing up `capacity` for each `hexagon_id`.
-   [st_drop_geometry](https://r-spatial.github.io/sf/reference/st_geometry.html) from **sf** package is used to remove the geometry column from a spatial object.
-   [left_join](https://dplyr.tidyverse.org/reference/join.html) from **dplyr** package merges two data frames based on matching values in a specified column.
-   [mutate_at](https://dplyr.tidyverse.org/reference/mutate_all.html) from **dplyr** package is used to apply a function to multiple columns that match a condition (here, columns that contain "push_est_pop").
-   [coalesce](https://dplyr.tidyverse.org/reference/coalesce.html) from **dplyr** package replaces `NA` values with a specified value (0.99 in this case).
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

::: {.callout-note collapse="true" title="How does the HDB capacity estimated?"}
To enhance our dataset with information from [Housing Development Board (HDB)](https://www.hdb.gov.sg/cs/infoweb/homepage) records, it's essential to recognize that HDB residential blocks are designed to house a varying number of residents. This diversity is due to the different types of flats available, each intended to cater to different family sizes and living arrangements. To accurately gauge the population within each HDB block, we employ a methodology that combines the count of dwelling units with an estimated occupancy rate.

The occupancy rate is derived based on standard family compositions associated with each flat type as detailed on the HDB's types of flats page, which outlines the typical configurations offered to residents. Here's how we estimate the population for each type of flat:

-   **1-room flats (sold or rental)**: Generally occupied by a single resident.

-   **2-room flats (sold or rental)**: Typically house two individuals, which may be a couple or two single persons sharing a space.

-   **3-room flats (sold or rental)**: Commonly accommodate a small family unit, presumed to be two adults and one child.

-   **4-room flats and executive flats (sold)**: Usually inhabited by a standard family of four members, such as two adults and two children.

-   **5-room flats (sold)**: Are spacious enough for larger families, presumed to consist of two adults and three children.

-   **Multi-generation flats (sold)**: Designed to support extended family living, these are estimated to house six individuals, including a pair of grandparents, a couple, and two children.

-   **Studio apartments (sold)**: Intended for elderly residents living alone.

-   **Other rental units**: If the categorization is not explicit, a general assumption of two residents is applied.
:::

## Pull Factors

For the pull factors we choose counts of station, train exit, business, entertainment, F&B, leisure & recreation, retails, and schools in the zone. Additionally we also try to estimate proxy population data using the HDB sales data.

::: {.callout-note collapse="true" title="why those variables are chosen?"}
Counts of Stations: Stations act as major transportation nodes, serving as crucial points for commuters to access different modes of transportation. A higher count of stations in a zone suggests greater connectivity and accessibility, attracting more passengers to use buses for their onward journeys.

Counts of Train Exits: Similar to stations, train exits represent key points of arrival for commuters. The availability of multiple train exits in a zone indicates a higher likelihood of people disembarking from trains and subsequently using buses for the next leg of their journey.

Counts of Businesses: Business establishments contribute significantly to commuter traffic, especially during weekday morning peaks. Employees traveling to work generate demand for bus services. A higher count of businesses in a zone reflects increased economic activity and, consequently, a greater need for public transportation.

Counts of Entertainment Venues: Zones with more entertainment options tend to attract residents and visitors alike. The availability of entertainment venues can stimulate bus ridership, as individuals may utilize buses for leisure activities, contributing to passenger volume during specific time periods.

Counts of Food & Beverage (F&B) Establishments: F&B venues are essential attractors for local and visiting populations. Morning peak hours often witness a surge in commuters seeking breakfast or coffee on their way to work. The count of F&B establishments is indicative of potential bus ridership during these hours.

Counts of Leisure & Recreation Facilities: Similar to entertainment venues, leisure and recreation facilities contribute to the overall attractiveness of a zone. People traveling for recreational purposes may use buses to reach these destinations, contributing to passenger volume, especially during specific time frames.

Counts of Retail Establishments: Retail establishments influence daily commuting patterns as people travel to shop for various goods and services. Higher counts of retail outlets can imply increased footfall and bus usage for shopping purposes.

Counts of Schools: Schools play a pivotal role in shaping travel patterns, particularly during weekday mornings. The count of schools in a zone reflects the potential demand for buses, as students and staff rely on public transportation to reach educational institutions.
:::

The following code will assign all the common pull factors count to the dataframe.

```{r}
# define function to add pull poi counts columns
add_pull_poi_counts <- function(factors_holder, poi_datasets) {
  # Loop through each POI dataset
  for (poi_name in poi_datasets) {
    # Add a new column with the count using st_intersects and lengths
    factors_holder[[paste0("pull_", poi_name, "_count")]] <- 
      ifelse(
        lengths(st_intersects(factors_holder, get(poi_name))) == 0,
        0.99,
        lengths(st_intersects(factors_holder, get(poi_name)))
      )
  }
  
  # Return the updated factors_holder dataframe
  return(factors_holder)
}

# List of POI dataset names
pull_poi_datasets <- c("station", "trainexit", "biz", "entertn", "fnb", "finance", "lnr", "retails", "schools_sf")

# Call the function
factors_holder <- add_pull_poi_counts(factors_holder, pull_poi_datasets)

glimpse(factors_holder)
```

::: {.callout-note collapse="true" title="Functions"}
-   The `add_pull_poi_counts` function is a custom-defined function. It is used to add new columns to the `factors_holder` data frame, each representing the count of points of interest (POIs) nearby.
-   The function loops through each POI dataset name provided in `poi_datasets` (a vector of dataset names like "station", "trainexit", etc.).
-   [st_intersects](https://r-spatial.github.io/sf/reference/st_intersects.html) from **sf** package checks for intersections between spatial objects. It's used here to identify intersecting POIs for each hexagon.
-   [lengths](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lengths) from **base** R calculates the number of intersecting points for each hexagon.
-   [ifelse](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/ifelse) from **base** R is used to replace zero counts with a small number (0.99) for later analytical purposes.
-   [get](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/get) from **base** R fetches the value of a named object (in this case, the POI datasets).
-   [paste0](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/paste) from **base** R concatenates strings to create new column names.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package is used at the end to display the structure of the updated `factors_holder` data frame.
:::

## Final Dataset

After assembling all the pull and push factors, next we will merge the factors to the main dataset. The push factors will be merged by using origin_hex, while the pull factors will be merged by using destination_hex. The following code will execute the task.

```{r}
final_df <- left_join(odb_flow, factors_holder %>% select(starts_with("push_"), hexagon_id), by = c("origin_hex" = "hexagon_id"))

final_df <- left_join(final_df, factors_holder %>% select(starts_with("pull_"), hexagon_id), by = c("destination_hex" = "hexagon_id"))

final_df <- final_df %>%
  select(-hex_grid.x, -hex_grid.y)

final_df <- final_df %>%
  mutate(push_est_pop = ifelse(push_est_pop == 0, 0.99, push_est_pop))

# Check the output
glimpse(final_df)
```

::: {.callout-note collapse="true" title="Functions"}
-   [left_join](https://dplyr.tidyverse.org/reference/join.html) from **dplyr** package merges two data frames based on matching values in specified columns. Here, it's used twice: first to join `odb_flow` with `push` columns from `factors_holder`, and then to join `pull` columns to the resulting data frame.
-   [select](https://dplyr.tidyverse.org/reference/select.html) from **dplyr** package is used to subset specific columns from a data frame. `starts_with` is a helper used within `select` to choose columns that start with certain strings.
-   The second `select` statement with `-hex_grid.x, -hex_grid.y` removes unwanted columns from `final_df`.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to create or modify columns in a data frame. Here, it modifies `push_est_pop` to replace zero values with 0.99.
-   [ifelse](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/ifelse) from **base** R applies a conditional test to each element of a vector.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, giving a quick look at its structure, including the types of columns and the first few entries in each column.
:::

To avoid redundant processing on running this qmd file or in rendering it, we can save the final processed data to rds file, then import it on the later step. The following code will write the rds file.

```{r}
#| eval: false
write_rds(final_df, "../data/rds/final_df_the2.rds")
```
::: {.callout-note collapse="true" title="Functions"}
-   [write_rds](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/readRDS) from **base** R saves an R object (in this case, the `final_df` data frame) to a file in R's binary RDS format. This format is specific to R and allows for efficient storage of R objects.
-   The first argument is the object to be saved (`final_df`), and the second argument is the file path where the object will be saved (`"../data/rds/final_df_the2.rds"`).
-   The `#| eval: false` at the beginning of the code is likely an instruction for an R Markdown document or similar tool, indicating that this line of code should not be evaluated when the document is run.
-   This code snippet saves the `final_df` data frame as an RDS file for future use, providing a convenient way to store and later reload the data frame in its current state.
:::

::: {.callout-note collapse="true" title="What & Why RDS Format?"}
The RDS format in R is a specialized file format used for storing single R objects. It's a compact binary format that preserves the exact structure of the saved object, including metadata. This format is particularly efficient for saving and loading objects in R, as it ensures that the object is restored exactly as it was when saved, without any need for reformatting or reassembling data.

Using the RDS format is beneficial because it allows for fast and efficient storage and retrieval of R objects, making it ideal for situations where you need to save an object and reload it later in another session without any loss of information. The functions `write_rds` and `read_rds` (or `writeRDS` and `readRDS` in base R) are used for saving to and reading from this format, respectively. RDS is especially useful for large datasets or complex objects where preservation of structure is crucial.
:::

# Spatial Interaction Model

As explained in [Overview] section, the Spatial Interaction Model used here are various type of the Gravity Model. Firstly, based on the previous saved data, this will be the checkpoint on importing the final dataset for the modelling.

```{r}
final_df <- read_rds("../data/rds/final_df_the2.rds")
glimpse(final_df)
```
::: {.callout-note collapse="true" title="Functions"}
-   [read_rds](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/readRDS) from **base** R loads an R object saved in RDS format. In this code, it's used to read the `final_df` data frame from a file located at `"../data/rds/final_df_the2.rds"`.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
-   This code snippet loads `final_df` from an RDS file and provides an overview of its structure, which is useful for understanding the types and arrangement of data contained in the data frame after it has been reloaded.
:::

Another thing to note before starting, is that the `glm` model function that will be used does not have a built in R2 value calculation. Therefore the following code will build a function to calculate the R2.

```{r}
CalcRSquared <- function(observed, estimated){
  r <- cor(observed, estimated)
  R2 <- r^2
  R2
}
```

::: {.callout-note collapse="true" title="Functions"}
-   [cor](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor) from **stats** R package is used to compute the correlation between `observed` and `estimated`. In this code, it's used to calculate the correlation coefficient `r`.
-   The square of the correlation coefficient `r^2` is calculated to obtain the R-squared value `R2`, which is a measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination.
:::

::: {.callout-note collapse="true" title="What is GLM?"}
A Generalized Linear Model (GLM) is a statistical framework used for modeling relationships between a dependent variable and one or more independent variables. Unlike ordinary linear regression, GLM accommodates a broader range of data distributions, making it suitable for situations where the response variable follows a non-normal distribution.

In R Studio, the `glm` function is utilized to fit a GLM. It allows you to specify the form of the model, the type of error distribution, and the link function connecting the predictors to the expected value of the response. Commonly used error distributions include Gaussian (for normal data), binomial (for binary data), and Poisson (for count data).

Related to the spatial interaction model, the Poisson distribution is often employed in GLMs for modeling count data, which aligns well with the nature of spatial interaction models dealing with the prediction of flows or counts of interactions between locations. The choice of Poisson distribution in GLM is particularly apt for scenarios where the response variable represents the number of events (e.g., trips, movements) occurring in a fixed period or area.

In the context of spatial interaction modeling using the `glm` function in R Studio, the Poisson GLM can be applied to capture the dependence of interaction counts between different spatial entities (e.g., zones, hexagons) on various predictor variables. The spatial interaction model, in this case, aims to understand and predict the flow or volume of interactions between locations, with the Poisson distribution addressing the discrete and non-negative nature of such count data.
:::

## Correlation Analysis

Before running the model, its a good practice to check collinearity between the variables to prevent bumping into multicollinearity problem in the model.

```{r}
# Calculate correlation
correlation_matrix <- cor(final_df[,3:18])

# Generate correlation plot
corrplot.mixed(correlation_matrix,
               lower = "color",      # Use ellipse for lower part
               upper = "number",     # Display correlation numbers in the upper part
               tl.pos = "lt",        # Place variable names to the left and top
               tl.col = "black",     # Set variable names color to black
               tl.cex = 0.7,         # Set variable names font size
               number.cex = 0.4      # Set correlation numbers font size
               )
```

::: {.callout-note collapse="true" title="Functions"}
-   [cor](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor) from **stats** package in base R computes a correlation matrix. In this code, it's used to calculate the correlation matrix for selected columns (columns 3 through 18) of `final_df`.
-   `corrplot.mixed` is a function from the **corrplot** package. It visualizes a correlation matrix, offering different styles for the upper and lower matrix. Here, the lower part of the matrix uses color coding (`"color"`), while the upper part displays correlation coefficients as numbers (`"number"`). The function also allows customization of text position (`tl.pos`), color (`tl.col`), and size (`tl.cex` for variable names, `number.cex` for correlation numbers).
-   This code creates a mixed-style correlation plot, providing a visual and numerical representation of the correlation between variables in `final_df`.
:::

Upon examining the correlation matrix, using a conservative threshold of correlation value above 0.7, the highly correlated variables are as follows:

1.  all station count and train exit count for each push and pull respectively have a strong positive correlation, suggesting that areas with a higher number of stations also tend to have a higher count of train exits. This result is quite expected.

2.  push_hdb_sf_count and push_est_pop also show a strong positive correlation, which may imply that regions with a higher number of HDB (Housing Development Board) flats tend to have a larger estimated population, a logical relationship given that the data is derived from the same source. Nevertheless, this result might also be caused by many of the hexagonal grid actually do not have HDB in the vicinity.

3.  pull_finance_count and pull_train_exit_count.

4.  pull_fnb_count and pull_finance_count.

5.  pull_fnb_count and pull_lnr_count.

6.  pull_fnr_count and pull_retails_count

::: {.notebox .lightbulb data-latex="lightbulb"}
this finding will be considered as basis for eliminating some variables in the model. Nevertheless, for experimental and comparison purpose, models without variables elimination will also be run in the next section.
:::

## The 4 Models Without Elimination

### Unconstrained

```{r}
uncSIM <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                log(push_station_count) +
                log(push_trainexit_count) +
                log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                log(pull_finance_count) +
                log(pull_lnr_count) +
                log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(uncSIM)
CalcRSquared(uncSIM$data$trips, uncSIM$fitted.values)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** package in base R fits generalized linear models. Here, `orcSIM` is a Poisson regression model with a log link function, predicting `trips` based on various `pull_*_count` variables, `log(distance)`, and `origin_hex`. The `log` function is applied to the count variables and distance, indicating a log-linear relationship.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R provides a detailed summary of the model's fit, including coefficients, statistical significance, and other diagnostics.
-   `CalcRSquared` is a customfunction for calculating the R-squared value of the model. It's used here to calculate R-squared using the actual `trips` data and the fitted values from `orcSIM`.
:::

**Key Coefficients:** The estimated coefficients reveal the impact of various predictor variables on the logarithm of trip counts. Notably, a positive coefficient (e.g., log(push_num_bus_stops)) suggests that an increase in the corresponding variable is associated with a higher number of trips, while a negative coefficient (e.g., log(push_station_count)) implies a negative influence. The large z-values and highly significant p-values (\<2e-16) underscore the robustness and statistical significance of these associations.

-   **Intercept:** 13.24 (p \< 2e-16)
-   **log(push_num_bus_stops):** 0.302 (p \< 2e-16)
-   **log(push_station_count):** -0.600 (p \< 2e-16)
-   **log(push_trainexit_count):** 0.389 (p \< 2e-16)
-   **log(push_hdb_sf_count):** 0.454 (p \< 2e-16)
-   **log(push_est_pop):** -0.051 (p \< 2e-16)
-   **log(pull_station_count):** -0.469 (p \< 2e-16)
-   **log(pull_trainexit_count):** 0.670 (p \< 2e-16)
-   **log(pull_biz_count):** 0.089 (p \< 2e-16)
-   **log(pull_entertn_count):** -0.054 (p \< 2e-16)
-   **log(pull_fnb_count):** -0.166 (p \< 2e-16)
-   **log(pull_finance_count):** 0.267 (p \< 2e-16)
-   **log(pull_lnr_count):** -0.228 (p \< 2e-16)
-   **log(pull_retails_count):** 0.039 (p \< 2e-16)
-   **log(pull_schools_sf_count):** 0.258 (p \< 2e-16)
-   **log(distance):** -1.180 (p \< 2e-16)

**Model Fit:** The coefficient of determination (R-squared) for the model is 0.188, indicating that approximately 18.8% of the variability in trip counts can be explained by the selected predictor variables. While this suggests a moderate level of explanatory power, it also highlights the existence of unaccounted factors influencing trip counts. The significance of the individual coefficients supports the model's ability to capture meaningful relationships within the dataset, despite the overall modest R-squared value.

### Origin Constrained

```{r}
orcSIM <- glm(formula = trips ~
                log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                log(pull_finance_count) +
                log(pull_lnr_count) +
                log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance) +
                origin_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(orcSIM)
CalcRSquared(orcSIM$data$trips, orcSIM$fitted.values)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** R package is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution. In this code, it's applied to the `trips` as the response variable and a set of predictor variables in the `final_df` data frame.
-   The formula `trips ~ log(push_num_bus_stops) + log(push_station_count) + log(push_trainexit_count) + log(push_hdb_sf_count) + log(push_est_pop) + log(distance) + destination_hex` specifies that `trips` is modeled as a function of `push_num_bus_stops`, `push_station_count`, `push_trainexit_count`, `push_hdb_sf_count`, `push_est_pop`, `distance`, and `destination_hex`. The `log` function is applied to transform the predictor variables.
-   The argument `family = poisson(link = "log")` specifies that a Poisson regression model is fitted with a log link function.
-   The argument `na.action = na.exclude` specifies that any missing values should be excluded from the model fitting process.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R package is used to produce result summaries of the results of various model fitting functions. In this code, it's applied to the fitted model `decSIM` to obtain a summary of the model fit.
-   `CalcRSquared` is a user-defined function which is used to calculate the R-squared value of the fitted model. It takes the observed values `decSIM$data$trips` and the fitted values `decSIM$fitted.values` as inputs.
:::

**Key Coefficients:** The estimated coefficients from the Poisson regression model provide insights into the relationship between predictor variables and the logarithm of trip counts. Each coefficient represents the change in the log count of trips associated with a one-unit change in the corresponding predictor, holding other variables constant. Highly significant p-values (\< 2e-16) emphasize the robustness of these associations.

-   **Intercept:** 13.44 (p \< 2e-16)
-   **log(pull_station_count):** -0.403 (p \< 2e-16)
-   **log(pull_trainexit_count):** 0.662 (p \< 2e-16)
-   **log(pull_biz_count):** 0.108 (p \< 2e-16)
-   **log(pull_entertn_count):** -0.089 (p \< 2e-16)
-   **log(pull_fnb_count):** -0.134 (p \< 2e-16)
-   **log(pull_finance_count):** 0.265 (p \< 2e-16)
-   **log(pull_lnr_count):** -0.166 (p \< 2e-16)
-   **log(pull_retails_count):** 0.060 (p \< 2e-16)
-   **log(pull_schools_sf_count):** 0.230 (p \< 2e-16)
-   **log(distance):** -1.239 (p \< 2e-16)
-   **origin_hex1003:** 2.486 (p \< 2e-16)
-   **origin_hex1004:** 3.300 (p \< 2e-16)
-   **origin_hex1011:** -0.784 (p \< 2e-16)
-   **origin_hex1012:** -0.841 (p \< 2e-16)
-   and various other dummy origin variables in which many are statistically significant.

**Model Fit:** The coefficient of determination (R-squared) for the model is 0.283, indicating that approximately 28.3% of the variability in trip counts can be explained by the selected predictor variables. The significance of individual coefficients and the model's goodness-of-fit metrics, such as the residual deviance and AIC, support its effectiveness in capturing patterns within the dataset. The p-values associated with each coefficient highlight their statistical significance in influencing trip counts.

### Destination Constrained

```{r}
decSIM <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                log(push_station_count) +
                log(push_trainexit_count) +
                log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(distance) +
                destination_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(decSIM)
CalcRSquared(decSIM$data$trips, decSIM$fitted.values)
```

**Key Coefficients:** The Poisson regression model provides estimates of the coefficients, shedding light on the relationship between predictor variables and the logarithm of trip counts. Significant p-values (\< 2e-16) highlight the importance of each predictor in influencing trip counts.

-   **Intercept:** 13.90 (p \< 2e-16)
-   **log(push_num_bus_stops):** 0.320 (p \< 2e-16)
-   **log(push_station_count):** -0.548 (p \< 2e-16)
-   **log(push_trainexit_count):** 0.455 (p \< 2e-16)
-   **log(push_hdb_sf_count):** 0.398 (p \< 2e-16)
-   **log(push_est_pop):** -0.019 (p \< 2e-16)
-   **log(distance):** -1.260 (p \< 2e-16)
-   **destination_hex1003:** 1.793 (p \< 2e-16)
-   **destination_hex1004:** 0.856 (p \< 2e-16)
-   **destination_hex1011:** -0.046 (p = 0.0165)
-   **destination_hex1012:** 1.545 (p \< 2e-16)
-   **destination_hex1013:** 0.134 (p = 3.71e-15)
-   **destination_hex1014:** -1.525 (p \< 2e-16)
-   **destination_hex1015:** 1.265 (p \< 2e-16)
-   **destination_hex1016:** 1.288 (p \< 2e-16)
-   **destination_hex1018:** -0.200 (p \< 2e-16)
-   **destination_hex1019:** 0.308 (p \< 2e-16)
-   **destination_hex1023:** -0.778 (p \< 2e-16)
-   **destination_hex1024:** 0.413 (p \< 2e-16)
-   **destination_hex1025:** -3.483 (p \< 2e-16)
-   **destination_hex1033:** 0.481 (p \< 2e-16)
-   and various other dummy destination variables in which many are statistically significant.

**Model Fit:** The R-squared value for the model is approximately 0.367, indicating that around 36.7% of the variability in trip counts can be explained by the selected predictor variables. The model's goodness-of-fit metrics, including the residual deviance and AIC, support its efficacy in capturing patterns within the dataset. Each predictor's p-value underscores its significance in explaining the observed variation in trip counts.

### Doubly Constrained

```{r}
docSIM <- glm(formula = trips ~ 
                origin_hex +
                destination_hex +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(docSIM)
CalcRSquared(docSIM$data$trips, docSIM$fitted.values)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** package in base R fits generalized linear models. In this code, `docSIM` is a Poisson regression model with a log link function. The model predicts `trips` based on `origin_hex`, `destination_hex`, and the logarithm of `distance`.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R provides a summary of the model's fit, including coefficients, statistical significance, and other diagnostics.
-   `CalcRSquared` is a custom function for calculating the R-squared value of the model.
:::

**Key Coefficients:** This Poisson regression model explores the relationship between trip counts and predictor variables, including origin, destination, and the logarithm of distance. The estimated coefficients, accompanied by their p-values, reveal the significance of each predictor in influencing trip counts.

-   **Intercept:** 14.34 (p \< 2e-16)
-   **origin_hex1003:** 2.59 (p \< 2e-16)
-   **origin_hex1004:** 3.41 (p \< 2e-16)
-   **origin_hex1011:** -1.06 (p \< 2e-16)
-   **origin_hex1012:** -0.91 (p \< 2e-16)
-   **origin_hex1013:** -0.27 (p \< 2e-16)
-   **origin_hex1014:** 0.36 (p \< 2e-16)
-   **origin_hex1015:** -0.44 (p \< 2e-16)
-   **origin_hex1016:** 1.34 (p \< 2e-16)
-   **origin_hex1018:** 1.91 (p \< 2e-16)
-   and various other dummy origin and destination variables in which many are statistically significant.

**Model Fit:** The model's R-squared value is approximately 0.534, indicating that around 53.4% of the variability in trip counts is explained by the selected predictor variables. The goodness-of-fit metrics, including the residual deviance and AIC, support the model's effectiveness in capturing patterns within the dataset. The p-values for each predictor underscore their significance in explaining the observed variation in trip counts.

::: {.notebox .lightbulb data-latex="lightbulb"}
Note that `push_est_pop` which represent proxy for population in the origin location counterintuitively have a negative coefficient. Nevertheless,it might simply be a symptomps of **multicollinearity**. As we can see, the `push_hdb_sf_count` which previously was identified to be highly correlated with `push_hdb_sf_count`, have positive coefficient. This in a way suggest that **the impact of the two offset each other to some extent, and the net effect most likely be positive**. We can see on the later whether this hunch is correct in the next section, where highly correlated variables are eliminated from the model.
:::

## The 3 Models With Elimination

### Unconstrained

```{r}
uncSIM2 <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                #log(push_station_count) +
                log(push_trainexit_count) +
                #log(push_hdb_sf_count) +
                log(push_est_pop) +
                #log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                #log(pull_finance_count) +
                #log(pull_lnr_count) +
                #log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(uncSIM2)
CalcRSquared(uncSIM2$data$trips, uncSIM2$fitted.values)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** R package is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution. In this code, it's applied to the `trips` as the response variable and a set of predictor variables in the `final_df` data frame.
-   The formula `trips ~ log(pull_trainexit_count) + log(pull_biz_count) + log(pull_entertn_count) + log(pull_fnb_count) + log(pull_schools_sf_count) + log(distance) + origin_hex` specifies that `trips` is modeled as a function of `pull_trainexit_count`, `pull_biz_count`, `pull_entertn_count`, `pull_fnb_count`, `pull_schools_sf_count`, `distance`, and `origin_hex`. The `log` function is applied to transform the predictor variables.
-   The argument `family = poisson(link = "log")` specifies that a Poisson regression model is fitted with a log link function.
-   The argument `na.action = na.exclude` specifies that any missing values should be excluded from the model fitting process.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R package is used to produce result summaries of the results of various model fitting functions. In this code, it's applied to the fitted model `orcSIM2` to obtain a summary of the model fit.
-   `CalcRSquared` is user-defined function , which is used to calculate the R-squared value of the fitted model. It takes the observed values `orcSIM2$data$trips` and the fitted values `orcSIM2$fitted.values` as inputs.
:::

**Key Coefficients:** The Poisson regression model reveals crucial coefficients that help interpret the impact of predictor variables on the logarithm of trip counts. With highly significant p-values (\< 2e-16), each coefficient contributes significantly to explaining the variability in trip counts.

-   **Intercept:** 13.22 (p \< 2e-16)
-   **log(push_num_bus_stops):** 0.383 (p \< 2e-16)
-   **log(push_trainexit_count):** 0.164 (p \< 2e-16)
-   **log(push_est_pop):** 0.130 (p \< 2e-16)
-   **log(pull_trainexit_count):** 0.780 (p \< 2e-16)
-   **log(pull_biz_count):** 0.073 (p \< 2e-16)
-   **log(pull_entertn_count):** -0.240 (p \< 2e-16)
-   **log(pull_fnb_count):** -0.193 (p \< 2e-16)
-   **log(pull_schools_sf_count):** 0.303 (p \< 2e-16)
-   **log(distance):** -1.169 (p \< 2e-16)

**Model Fit:** The R-squared value for the model is approximately 0.131, suggesting that the selected predictor variables collectively explain around 13.1% of the variability in trip counts. The goodness-of-fit metrics, including the residual deviance and AIC, demonstrate the model's efficacy in capturing patterns within the data. Notably, all predictor variables exhibit highly significant p-values, emphasizing their importance in explaining the observed variation in trip counts.

### Origin Constrained

```{r}
orcSIM2 <- glm(formula = trips ~
                #log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                #log(pull_finance_count) +
                #log(pull_lnr_count) +
                #log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance) +
                origin_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(orcSIM2)
CalcRSquared(orcSIM2$data$trips, orcSIM2$fitted.values)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** R package is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution. In this code, it's applied to the `trips` as the response variable and a set of predictor variables in the `final_df` data frame.
-   The formula `trips ~ log(push_num_bus_stops) + log(push_trainexit_count) + log(push_est_pop) + log(distance) + destination_hex` specifies that `trips` is modeled as a function of `push_num_bus_stops`, `push_trainexit_count`, `push_est_pop`, `distance`, and `destination_hex`. The `log` function is applied to transform the predictor variables.
-   The argument `family = poisson(link = "log")` specifies that a Poisson regression model is fitted with a log link function.
-   The argument `na.action = na.exclude` specifies that any missing values should be excluded from the model fitting process.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R package is used to produce result summaries of the results of various model fitting functions. In this code, it's applied to the fitted model `decSIM2` to obtain a summary of the model fit.
-   `CalcRSquared` is a user-defined function which is used to calculate the R-squared value of the fitted model. It takes the observed values `decSIM2$data$trips` and the fitted values `decSIM2$fitted.values` as inputs.
:::

**Key Coefficients:** The Poisson regression model reveals insights into the relationship between predictor variables and the logarithm of trip counts. The following coefficients are statistically significant (p \< 2e-16), indicating their importance in explaining variations in trip counts:

-   **Intercept:** 13.56
-   **log(pull_trainexit_count):** 0.831
-   **log(pull_biz_count):** 0.0971
-   **log(pull_entertn_count):** -0.1845
-   **log(pull_fnb_count):** -0.102
-   **log(pull_schools_sf_count):** 0.2595
-   **log(distance):** -1.2396
-   **origin_hex1003:** 2.632
-   **origin_hex1004:** 3.491
-   **origin_hex1011:** -0.5583
-   **origin_hex1012:** -0.6222
-   **origin_hex1013:** -0.0155
-   **origin_hex1014:** 0.4643
-   **origin_hex1015:** -0.1897
-   and various other dummy origin variables in which many are statistically significant.

**Model Fit:** The R-squared value for the model is approximately 0.250, indicating that around 25% of the variability in trip counts can be explained by the selected predictor variables. The model's goodness-of-fit metrics, including the residual deviance and AIC, support its efficacy in capturing patterns within the dataset. Each predictor's p-value underscores its significance in explaining the observed variation in trip counts. The non-significant p-value like **origin_hex1013** suggests that this variable may not be a strong predictor of trip counts in this context.

### Destination Constrained

```{r}
decSIM2 <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                #log(push_station_count) +
                log(push_trainexit_count) +
                #log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(distance) +
                destination_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(decSIM2)
CalcRSquared(decSIM2$data$trips, decSIM2$fitted.values)
```

::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** R package is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution. In this code, it's applied to the `trips` as the response variable and a set of predictor variables in the `final_df` data frame.
-   The formula `trips ~ log(push_num_bus_stops) + log(push_trainexit_count) + log(push_est_pop) + log(distance) + destination_hex` specifies that `trips` is modeled as a function of `push_num_bus_stops`, `push_trainexit_count`, `push_est_pop`, `distance`, and `destination_hex`. The `log` function is applied to transform the predictor variables.
-   The argument `family = poisson(link = "log")` specifies that a Poisson regression model is fitted with a log link function.
-   The argument `na.action = na.exclude` specifies that any missing values should be excluded from the model fitting process.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R package is used to produce result summaries of the results of various model fitting functions. In this code, it's applied to the fitted model `decSIM2` to obtain a summary of the model fit.
-   `CalcRSquared` is a user-defined function which is used to calculate the R-squared value of the fitted model. It takes the observed values `decSIM2$data$trips` and the fitted values `decSIM2$fitted.values` as inputs.
:::

**Key Coefficients:** The Poisson regression model reveals insightful coefficients, offering a glimpse into the relationships between predictor variables and the logarithm of trip counts. Each predictor's significance is denoted by the associated p-values, all indicating high statistical significance (\< 2e-16).

-   **Intercept:** 13.64 (p \< 2e-16)
-   **log(push_num_bus_stops):** 0.395 (p \< 2e-16)
-   **log(push_trainexit_count):** 0.291 (p \< 2e-16)
-   **log(push_est_pop):** 0.137 (p \< 2e-16)
-   **log(distance):** -1.254 (p \< 2e-16)
-   **destination_hex1003:** 1.820 (p \< 2e-16)
-   **destination_hex1004:** 0.917 (p \< 2e-16)
-   **destination_hex1011:** -0.072 (p = 0.000194)
-   **destination_hex1012:** 1.443 (p \< 2e-16)
-   **destination_hex1013:** 0.059 (p = 0.000490)
-   **destination_hex1014:** -1.564 (p \< 2e-16)
-   **destination_hex1015:** 1.247 (p \< 2e-16)
-   **destination_hex1016:** 1.292 (p \< 2e-16)
-   **destination_hex1018:** -0.196 (p \< 2e-16)
-   and various other dummy destination variables in which many are statistically significant.

**Model Fit:** The model's R-squared value is approximately 0.346, indicating that around 34.6% of the variability in trip counts can be explained by the selected predictor variables. The goodness-of-fit metrics, such as the residual deviance and AIC, support the model's effectiveness in capturing patterns within the dataset. All predictor variables demonstrate statistical significance, emphasizing their roles in explaining the observed variation in trip counts.

## Compare The Result

### Performance Table

```{r}
model_list <- list(
  Unconstrained = uncSIM,
  Unconstrained_with_Elimination = uncSIM2,
  Origin_Constrained = orcSIM,
  Origin_Constrained_with_Elimination = orcSIM2,
  Destination_Constrained = decSIM,
  Destination_Constrained_with_Elimination = decSIM2,
  Doubly_Constrained = docSIM
)

# Compare performance with multiple metrics
compare_performance(model_list, metrics = c("AIC", "BIC", "RMSE"))
```

::: {.callout-note collapse="true" title="Functions"}
-   The `list` function from **base** R is used to create a list named `model_list` that contains seven elements. Each element is a model simulation, and they are named as follows: 'Unconstrained', 'Unconstrained_with_Elimination', 'Origin_Constrained', 'Origin_Constrained_with_Elimination', 'Destination_Constrained', 'Destination_Constrained_with_Elimination', and 'Doubly_Constrained'. The values of these elements are the corresponding model simulations (`uncSIM`, `uncSIM2`, `orcSIM`, `orcSIM2`, `decSIM`, `decSIM2`, `docSIM`).
-   The `compare_performance` function is then used to compare the performance of the models in `model_list` using multiple metrics. The `metrics` argument is set to a vector containing "AIC", "BIC", and "RMSE". This means that the function will calculate the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and Root Mean Square Error (RMSE) for each model in `model_list`.
:::

In comparing the performance of various spatial interaction models, the model indices indicate that the Doubly Constrained model outperforms the others. It has the lowest values for the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), suggesting better model fit with lower complexity. Moreover, it boasts the smallest Root Mean Square Error (RMSE) of 1267.730, implying it has the highest predictive accuracy.

The Destination Constrained models follow, displaying improved performance over the Unconstrained models, as indicated by their lower AIC, BIC, and RMSE values. The addition of variable elimination to the Origin and Destination Constrained models shows slight increases in AIC and BIC values with corresponding minor increases in RMSE, which suggests that while the elimination process simplifies the models, it does so at the cost of some predictive performance.

Note that these results are inline with what the R2 suggested in previous finding.

::: {.callout-note collapse="true" title="What is AIC?"}
The **Akaike Information Criterion (AIC)** is a measure of the relative quality of a statistical model for a given set of data. It provides a means for model selection by comparing models and choosing the one that minimizes information loss.
:::

::: {.callout-note collapse="true" title="What is AIC Weights?"}
These weights are derived from the AIC values of each model and are used to compare each model's likelihood of being the best model among the set of models being considered. A lower AIC value indicates a better model fit, and the corresponding weight represents the model's probability relative to the others. A weight close to 1 suggests a high likelihood that it is the best model, while a weight close to 0 suggests a lower likelihood.
:::

::: {.callout-note collapse="true" title="What is BIC?"}
The **Bayesian Information Criterion (BIC)** is similar to AIC, but it introduces a penalty term for the number of parameters in the model to discourage overfitting.
:::

::: {.callout-note collapse="true" title="What is BIC Weights?"}
Similar to AIC weights, BIC weights are calculated from the BIC values, which include a penalty for the number of parameters in the model to avoid overfitting. The BIC is more stringent than the AIC, especially as the sample size increases. As with AIC, a BIC weight close to 1 indicates a stronger model relative to the others in terms of both fit and parsimony.
:::

::: {.callout-note collapse="true" title="What is RMSE?"}
The **Root Mean Square Error (RMSE)** is a frequently used measure of the differences between values predicted by a model and the values observed. It represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences.
:::

### Visualized Fitted Value

Next, we will visualize to compare the predicted value against the original value for each of the seven models that we trained previously using the following code chunk.

```{r}
# Function to round fitted values and create a data frame
round_and_rename <- function(sim_data, sim_name) {
  as.data.frame(sim_data$fitted.values) %>%
    round(digits = 0) %>%
    setNames(paste0(sim_name, "_trips"))
}

# Round and rename fitted values for each simulation
uncSIM_fitted <- round_and_rename(orcSIM, "uncSIM")
orcSIM_fitted <- round_and_rename(orcSIM, "orcSIM")
decSIM_fitted <- round_and_rename(decSIM, "decSIM")
docSIM_fitted <- round_and_rename(docSIM, "docSIM")
uncSIM2_fitted <- round_and_rename(orcSIM2, "uncSIM2")
orcSIM2_fitted <- round_and_rename(orcSIM2, "orcSIM2")
decSIM2_fitted <- round_and_rename(decSIM2, "decSIM2")

# Combine the rounded and renamed fitted values
final_df_viz <- final_df %>%
  cbind(uncSIM_fitted, orcSIM_fitted, decSIM_fitted, docSIM_fitted, uncSIM2_fitted, orcSIM2_fitted, decSIM2_fitted)

# Create a function to generate a ggplot
generate_ggplot <- function(data, x_col, color, title) {
  ggplot(data = data, aes(x = !!sym(x_col), y = trips)) +
    geom_point(
      size = data$trips / 10000,
      alpha = .6,
      shape = 21  # Change point shape
    ) +
    xlim(0, 50000) +
    geom_smooth(
      method = lm,
      se = TRUE,
      color = "blue"  # Change smooth line color
    ) +
    labs(title = title) +
    theme(
      plot.title = element_text(size = 10),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.text.y = element_blank(),
      axis.title.y = element_blank()
    )
}

# Generate ggplots for each simulation
p_unc <- generate_ggplot(final_df_viz, "uncSIM_trips", "black", "Unconstrained")
p_orc <- generate_ggplot(final_df_viz, "orcSIM_trips", "black", "Origin-constrained")
p_dec <- generate_ggplot(final_df_viz, "decSIM_trips", "black", "Destination-constrained")
p_doc <- generate_ggplot(final_df_viz, "docSIM_trips", "black", "Doubly-constrained")
p_unc2 <- generate_ggplot(final_df_viz, "uncSIM2_trips", "black", "Unconstrained with Elimination")
p_orc2 <- generate_ggplot(final_df_viz, "orcSIM2_trips", "black", "Origin-constrained with Elimination")
p_dec2 <- generate_ggplot(final_df_viz, "decSIM2_trips", "black", "Destination-constrained with Elimination")

# Combine the plots using patchwork
p_unc + p_unc2 + p_orc + p_orc2 + p_dec + p_dec2+ p_doc 
```

::: {.callout-note collapse="true" title="Functions"}
-   The `round_and_rename` function is a user-defined function that takes two arguments: `sim_data` and `sim_name`. It rounds the fitted values in `sim_data` to 0 decimal places, converts them to a data frame, and renames the column using `sim_name`. The new column name is the concatenation of `sim_name` and "\_trips".
-   The `round_and_rename` function is then applied to each simulation data (`orcSIM`, `decSIM`, `docSIM`, `orcSIM2`, `decSIM2`) to round and rename the fitted values. The results are stored in new variables (`uncSIM_fitted`, `orcSIM_fitted`, `decSIM_fitted`, `docSIM_fitted`, `uncSIM2_fitted`, `orcSIM2_fitted`, `decSIM2_fitted`).
-   The `cbind` function from **base** R is used to combine the rounded and renamed fitted values into a new data frame `final_df_viz`.
-   The `generate_ggplot` function is another user-defined function that takes four arguments: `data`, `x_col`, `color`, and `title`. It generates a ggplot of `data` with `x_col` on the x-axis and `trips` on the y-axis. The points are sized according to `trips` and shaped as hollow circles (`shape = 21`). A linear model is fitted to the data (`method = lm`), and the smooth line color is set to blue. The title of the plot is set to `title`, and the text size, axis text, and axis title are customized using the `theme` function.
-   The `generate_ggplot` function is then applied to `final_df_viz` for each simulation to generate ggplots. The results are stored in new variables (`p_unc`, `p_orc`, `p_dec`, `p_doc`, `p_unc2`, `p_orc2`, `p_dec2`).
-   The `+` operator from the **patchwork** package is used to combine the plots into a single plot.
:::

Summary analysis of each model's fitted values compared to the original trips:

-   **Unconstrained Model**: This model shows a broad dispersion of points, with many falling far from the line of best fit. This suggests the model may not be very accurate in predicting the original trips, as indicated by the spread of fitted values.

-   **Unconstrained with Elimination**: A similar pattern to the unconstrained model, but with a slight shift in the points, indicating that variable elimination may not have significantly improved the model's predictive ability.

-   **Origin-Constrained Model**: Points are more aligned with the trend line compared to the unconstrained model, suggesting a better fit for predicting the number of trips.

-   **Origin-Constrained with Elimination**: The scatter is close to the trend line, much like the origin-constrained model, implying that variable elimination has little effect on improving this model's fit.

-   **Destination-Constrained Model**: The concentration of points around the line of best fit is even tighter here, indicating an improved accuracy over the origin-constrained models.

-   **Destination-Constrained with Elimination**: This shows a pattern similar to the destination-constrained model, with a close clustering of points around the trend line, suggesting a good model fit.

-   **Doubly-Constrained Model**: This model exhibits the tightest clustering of points along the line, indicating the highest predictive accuracy and the best fit among all models presented.

The size of the points, representing the number of trips, varies across models, with the doubly-constrained model showing the most consistent scaling in relation to the trend line, suggesting this model's superior capability in capturing the trip distribution pattern in the data.

## Conclusion

In this study, various Spatial Interaction Models (SIMs) were constructed to understand and predict bus commuter flow during the Weekday Morning Peak in Singapore. The models included Unconstrained, Origin-Constrained, Destination-Constrained, and Doubly-Constrained versions, each incorporating multiple features such as bus stops, train stations, HDB units, and more. This project also tried to experiment with inclusion of highly correlated variables and without its inclusion.

Based on the goodness-of-fit and linearity were conducted, revealing consistent outperformance of the Doubly-Constrained SIM over Origin-Constrained, Destination-Constrained, and Unconstrained SIMs. Key observations and conclusions include:

-   **Model Complexity:** The Doubly-Constrained SIM is consistently outperfoming the other models. Nevertheless, it might be ***simply be due to the complexity*** of the models since it contains thousands of classes from two categorical variables, which are the origin hex and the destination hex. In a way, this simply suggests that ***uniqueness of each origin and destination hex is the best predictor of the amount of trips***

-   **Push and Pull Factor Significance:** All push and pull factor variables becomes significant explanatory variable influencing the number of trips during Weekday Morning Peak Periods. Those variables includes counts of bus stop, train station exits, business, schools, entertainment, food & beverages, and schools. This is ***inline with the gravity model theory***. Nevertheless, some of these variables unexpectedly have ***inconsistent coefficients*** across the models (it came as negative sometimes, which is counterintuitive). This indicates ***spurious regression*** and/or multicollinearity exists, even among those variables that were observed to be not highly correlated.

-   **Distance Significance:** Distance between zones emerged as the crucial explanatory variable influencing the number of trips during Weekday Morning Peak Periods. Shorter distances correlated with higher trip numbers, highlighting the dominant role of distance in commuter behavior. This is also ***inline with the gravity model theory***.

-   **Push and Pull Factor Significance:** Distance between zones emerged as significant explanatory variable influencing the number of trips during Weekday Morning Peak Periods. Shorter distances correlated with higher trip numbers, highlighting the dominant role of distance in commuter behavior. This is ***inline with the gravity model theory***.

Despite the relatively good predictive performance of the best model (\>50% explained variance), the better result of Doubly-Constrained SIM also might simply suggests that ***uniqueness of each origin and destination hex is the best predictor of the amount of trips***. This implied various things for future research, such as: - The current best model (doubly-constrained) might have ***reduced performance in the future if the condition in some hexagon grid changed***, as it relies on it as the variable. Therefore, future modelling should consider ***relying more on the push and pull factors variables***.

-   The current push and pull factors variables are ***currently not good enough to represent the uniqueness*** that each hexagon grid has in determining trips. This might be due to various factors such as ***point of interest that is not yet identified***, and each ***point of interest might also have different characteristic that affect the magnitude*** of it as a push or pull factors. For example, business and schools might have different capacities which in turn translated into different amount of commuters. Another important factor that might be improved is the estimated population of each hexagon. If the estimated population can be more accurate, it might yield better result.

-   Lastly, future project can also consider to use ***Spatial Econometric Interaction Models*** where factors can be given spatial weight. This might in turn be better to represent the characteristic of various geospatial conditions.

::: {.callout-note collapse="true" title="What is Spurious Regression?"}
Spurious regression occurs when two unrelated variables show a strong statistical relationship in a regression analysis, even though there is no true cause-and-effect connection between them. In other words, the apparent relationship is misleading and arises by chance. This phenomenon can mislead researchers into thinking they have discovered a meaningful connection when, in fact, there is none. It's a cautionary concept, reminding us to carefully consider the theoretical basis and real-world relevance of variables before concluding that they are genuinely related based solely on statistical results.
:::

::: {.callout-note collapse="true" title="What is Spatial Econometric Interaction Model (SIEM)?"}
A Spatial Econometric Interaction Model is a statistical tool used to analyze how the interactions between different locations or units influence economic phenomena. In simpler terms, it helps us understand how economic variables in one area can affect or be affected by variables in neighboring areas. This model accounts for spatial dependencies, recognizing that nearby locations may share similarities or influence each other.

Imagine you want to study housing prices in different neighborhoods. A Spatial Econometric Interaction Model would not only consider local factors like the number of bedrooms and neighborhood amenities but also account for how prices in one neighborhood might be influenced by prices in nearby neighborhoods. It helps capture the idea that economic conditions in one area can spill over and impact neighboring areas, providing a more realistic and accurate analysis of regional economic relationships.
:::

# References

Cengel. [Introduction to spatial data in R](https://cengel.github.io/R-spatial/intro.html)

Co≈ükun, et al. (2020). [Performance Matters on Identification of Origin-Destination Matrix on Geospatial Big Data](https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/449/2020/isprs-archives-XLIII-B4-2020-449-2020.pdf)

Daniels & Mulley. [Explaining walking distance to public transport: The dominance of public transport supply](https://www.jstor.org/stable/26202654)

Haynes & Fotheringham (1985). [Gravity and Spatial Interaction Models](https://researchrepository.wvu.edu/rri-web-book/16/)

Kam Tin Seong. [16 Calibrating Spatial Interaction Models with R](https://r4gdsa.netlify.app/chap16)

Miller (2021). [TRAFFIC ANALYSIS ZONE DEFINITION: ISSUES & GUIDANCE](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf)

R. [Spatial interaction models with R](https://cran.r-project.org/web/packages/simodels/vignettes/simodels.html.)

Sekste and Kazakov. ["H3 hexagonal grid: Why we use it for data analysis and visualization"](https://www.kontur.io/blog/h3-hexagonal-grid/).

Sid Dhuri (2020). ["Spatial Data Analysis With Hexagonal Grids"](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e)

Tao Ran (2021). [Big Spatial Flow Data Analytics. In: Werner, M., Chiang, YY. (eds) Handbook of Big Geospatial Data](https://doi.org/10.1007/978-3-030-55462-0_7)

Land Transport Authority. [Land Transport Data Mall](https://datamall.lta.gov.sg/content/datamall/en.html)
