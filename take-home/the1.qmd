---
title: "Take-home 1 - Geospatial Analytics for Public Good"
author: "Muhamad Ameer Noor"
date: "2 December 2023"
date-modified: "last-modified"

editor: visual
format: 
  html:
    code-fold: true
    code-summary: "code chunk"
    fontsize: 18px
    number-sections: true
    number-depth: 2
execute:
  echo: true # all code chunk will appear
  eval: true # all code chunk will running live (be evaluated)
  warning: false # don't display warning
  message: false
---

# Overview

::: panel-tabset
## The Scene

As cities upgrade to digital systems, like smart buses and GPS, a lot of data is created about how people move around. This data is super useful for understanding patterns in both space (where) and time (when). For example, on public buses, smart cards and GPS devices collect information about routes and how many people are riding. This data holds clues about how people behave in the city, and understanding these patterns can help manage the city better and give an edge to transportation providers.

But, often, this valuable data is only used for basic things like tracking and mapping using Geographic Information System (GIS) applications. This is because regular GIS tools struggle to fully analyze and model this kind of complex spatial and time-related data.

In this exercise, we're going to focus on bus trips during specific peak hours:

| Peak hour period             | Bus tap on time |
|------------------------------|-----------------|
| Weekday morning peak         | 6am to 9am      |
| Weekday afternoon peak       | 5pm to 8pm      |
| Weekend/holiday morning peak | 11am to 2pm     |
| Weekend/holiday evening peak | 4pm to 7pm      |

To better understand these movement patterns and behaviors, we're going to use something called Exploratory Spatial Data Analysis (ESDA). It's like a powerful tool that helps us tackle complex challenges in society. In this project, we're going to use specific methods like Local Indicators of Spatial Association ([LISA](https://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.1995.tb00338.x)) and Emerging Hot Spot Analysis ([EHSA](https://www.azavea.com/blog/2017/08/15/emerging-hot-spot-spatial-statistics/)) to reveal the interesting spatial and time-related mobility patterns of people who take public buses in Singapore. These methods will help us dig deep into the data and uncover valuable insights.

::: {.callout-note collapse="true" title="What is LISA?"}
Local Indicators of Spatial Association (LISA) statistics serve two primary purposes. Firstly, they act as indicators of local pockets of nonstationarity, or hotspots. Secondly, they are used to assess the influence of individual locations on the magnitude of a global statistic. In simpler terms, LISA helps in identifying specific areas (like hotspots) that differ significantly from their surrounding areas and evaluates how these individual areas contribute to the overall pattern observed in a larger region.
:::

::: {.callout-note collapse="true" title="What is EHSA?"}
Emerging Hot Spot Analysis (EHSA) is a type of spatial statistical analysis that examines data over space and time to identify trends and patterns, specifically focusing on the detection of statistical hot and cold spots within a study region. This method is applicable in various fields, including crime patterns, disease outbreaks, and environmental issues. EHSA allows users to set parameters and then determines whether identified hot or cold spots are persistent, increasing, or decreasing in a given area.
:::

## The Data

the content of the following panel explained what aspatial and geospatial data are used in this project.

::: panel-tabset
### Aspatial

Monthly ***Passenger Volume by Origin Destination Bus Stops:***

-   August, September and October 2023 Period

-   downloaded from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html) via [API](https://datamall.lta.gov.sg/content/dam/datamall/datasets/LTA_DataMall_API_User_Guide.pdf)

-   `csv` format.

-   Columns/Fields in the dataset includes YEAR_MONTH, DAY_TYPE, TIME_PER_HOUR, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE, and TOTAL_TRIPS.

### Geospatial

Two geospatial data in `shp` format are used in this project, which includes:

-   *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html). It provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.

-   *hexagon*, a [hexagon](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm) layer of 500m (perpendicular distance between the centre of the hexagon and its edges.) Each spatial unit is regular in shape and finer than the Master Plan 2019 Planning Sub-zone GIS data set of URA.

::: {.callout-note collapse="true" title="why hexagon?"}
-   ***Uniform Distances Everywhere***: Think of hexagons as honeycomb cells. Each cell (hexagon) touches its neighbors at the same distance from its center. It's like standing in the middle of a room and being the same distance from every wall, making it easier to measure and compare things.

-   ***Outlier-Free Shape***: Hexagons are like well-rounded polygons without any pointy tips. Sharp corners can create odd spots in data, but hexagons smoothly cover space without sticking out anywhere. This helps prevent weird data spikes that don't fit the pattern.

-   ***Consistent Spatial Relationships***: Imagine a beehive where every hexagon is surrounded by others in the same pattern. This regular pattern is great for analyzing data because you can expect the same relationships everywhere, making the data predictable and easier to work with.

-   ***Ideal for Non-Perpendicular Features***: Real-world features like rivers and roads twist and turn. Squares can be awkward for mapping these, but hexagons, which are more circular, can follow their flow better. This way, a hexagon-based map can mimic the real world more closely than a checkerboard of squares.

Summarized from: [Dhuri](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e), and [Sekste & Kazakov](https://www.kontur.io/blog/h3-hexagonal-grid/).
:::
:::
:::

# Preparation

Before starting with the analysis, we have to load the library (and install it if it does not exist in the environment yet), import the data, and process the data

## Import Library

```{r}
pacman::p_load(tmap, sf, tidyverse, sfdep, knitr, Hmisc, mapview, DT)
```

Explanations for the imported library:

-   [tmap](https://cran.r-project.org/web/packages/tmap/) for visualizing geospatial

-   [sf](https://r-spatial.github.io/sf/) for handling geospatial data

-   [tidyverse](https://www.tidyverse.org/) for handling aspatial data

-   [sfdep](https://sfdep.josiahparry.com/) for computing spatial weights, global and local spatial autocorrelation statistics, and

-   [knitr](https://www.rdocumentation.org/packages/knitr/versions/1.45) for creating html tables

-   [Hmisc](https://www.rdocumentation.org/packages/Hmisc/versions/5.1-1) for summary statistics

-   [mapview](https://cran.r-project.org/web/packages/mapview/index.html) for interactive map backgrouds

-   [DT](https://rstudio.github.io/DT/) library to create interactive html tables

## Import and Setup the Data

### Aspatial

the following code will import all the origin destination bus data and check a sample dataframe. The process also involved transforming georeference data type into [factors](https://r4ds.hadley.nz/factors) for easing compatibility issue and more efficient processing.

::: panel-tabset

#### August 2023
```{r}
# Load each csv file
odb8 <- read_csv("../data/aspatial/origin_destination_bus_202308.csv.gz")

# change georeference data type into factors
odb8 <- odb8 %>%
  mutate(
    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)
  )

# check the dataframe
describe(odb8)
```

#### September 2023
```{r}
# Load each csv file
odb9 <- read_csv("../data/aspatial/origin_destination_bus_202309.csv.gz")

# change georeference data type into factors
odb9 <- odb9 %>%
  mutate(
    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)
  )


# check the dataframe
describe(odb9)
```

#### October 2023
```{r}
# Load each csv file
odb10 <- read_csv("../data/aspatial/origin_destination_bus_202310.csv.gz")

# change georeference data type into factors
odb10 <- odb10 %>%
  mutate(
    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)
  )

# check the dataframe
describe(odb10)
```
:::


Explanations for each field in the data can be found in the following metadata.

::: {.callout-note collapse="true" title="metadata"}
-   YEAR_MONTH: Represent year and Month in which the data is collected. Since it is a monthly data frame, only one unique value exist in each data frame.

-   DAY_TYPE: Represent type of the day which classified as *weekdays* or *weekends/holidays*.

-   TIME_PER_HOUR: Hour which the passenger trip is based on, in intervals from 0 to 23 hours.

-   PT_TYPE: Type of public transport, Since it is bus data sets, only one unique value exist in each data frame (i.e. *bus*)

-   ORIGIN_PT_CODE: ID of origin bus stop

-   DESTINATION_PT_CODE: ID of destination bus stop

-   TOTAL_TRIPS: Number of trips
:::

### Geospatial
the following panel show the import process for the bus stop geospatial data. The geospatial layer of the data shows ***point*** location of bus stops across Singapore.

::: panel-tabset

#### Import and Check the Data
As previously done, while reading the data, categorical data that can serve as reference are converted into [factors](https://r4ds.hadley.nz/factors) for easing compatibility issue and more efficient processing.
```{r}
busstop <- st_read(
    dsn = "../data/geospatial",
    layer = "BusStop"
  ) %>%
  mutate(
    BUS_STOP_N = as.factor(BUS_STOP_N),
    BUS_ROOF_N = as.factor(BUS_ROOF_N),
    LOC_DESC = as.factor(LOC_DESC)
  )

# check the output
glimpse(busstop)
```
##### Check the unique values

::: panel-tabset

###### BUS_STOP_N

count of unique/distinct values are:

```{r}
n_distinct(busstop$BUS_STOP_N)
```

###### BUS_ROOF_N

count of unique/distinct values are:

```{r}
n_distinct(busstop$BUS_ROOF_N)
```

###### LOC_DESC

count of unique/distinct values are:

```{r}
n_distinct(busstop$LOC_DESC)
```

###### geometry

count of unique/distinct values are:

```{r}
n_distinct(busstop$geometry)
```

:::

Explanations for each field in the data can be found in the following metadata.

::: {.callout-note collapse="true" title="metadata"}
-   BUS_STOP_N: The unique identifier for each bus stop.
-   BUS_ROOF_N: The identifier for the bus route or roof associated with the bus stop.
-   LOC_DESC: Location description providing additional information about the bus stop's surroundings.
-   geometry: The spatial information representing the location of each bus stop as a point in the SVY21 projected coordinate reference system.
:::

#### Setup EPSG Code, CRS and Prepare the data for joining

To ensure that geospatial data from different sources can be processed together, both have to be projected using similar coordinate systems and be assigned the correct EPSG code based on CRS. The [st_crs](https://r-spatial.github.io/sf/reference/st_crs.html) function is used to check for ESPG Code and Coordinate System of both geospatial files. For Singapore, the coordinate system is *SVY21* with *EPSG 3414* (source: [epsg.io](https://epsg.io/?q=Singapore)).
The following code chunk output shows how the CRS was initially not *3414*, then corrected using [st_set_crs](https://www.rdocumentation.org/packages/sf/versions/1.0-14/topics/st_crs).
Simultaneously, the dataframe is also prepared for joining process.

```{r}
# Check the current Coordinate Reference System (CRS) of the busstop dataset
print("Current CRS of busstop dataset:")
print(st_crs(busstop))

# Assign a new EPSG code (Spatial Reference ID)
busstop <- st_set_crs(
   busstop, 
   3414
) %>%
# Rename the bus stop origin column for easier joining with the main dataframe
mutate(
   ORIGIN_PT_CODE = as.factor(BUS_STOP_N)
) %>%
select(
   ORIGIN_PT_CODE, 
   LOC_DESC,
   geometry
) %>%
# Change all column names to lowercase for consistency
rename_with(
   tolower, everything()
)

# Confirm the updated EPSG code after the assignment
print("Updated CRS of busstop dataset:")
print(st_crs(busstop))
```
:::

## Data Wrangling

### check for duplicates
In this step, we inspect the dataset for duplicate entries. This precautionary step is crucial to avoid the inadvertent repetition of records, which could lead to the overcounting of passenger trips. By identifying and addressing duplicates at this stage, we ensure the accuracy and reliability of our analysis, laying the groundwork for more precise and trustworthy results in the subsequent analytical processes. Checking for duplicates is a fundamental practice that contributes to the integrity of the data and the overall robustness of the geospatial analysis.

::: panel-tabset
#### Aspatial - Origin Destination Bus
the following code is used to check duplicates and show how many duplicates exist in each *odb*.
```{r}
# Count the number of rows in each dataframe with duplicates
count_duplicate_rows <- function(df, df_name) {
  df %>%
    group_by_all() %>%
    filter(n() > 1) %>%
    ungroup() %>%
    summarise(df_name = df_name, row_count = n())
}

# Apply the function to each dataframe
duplicate1 <- count_duplicate_rows(odb8, "odb8")
duplicate2 <- count_duplicate_rows(odb9, "odb9")
duplicate3 <- count_duplicate_rows(odb10, "odb10")

# Combine the results
all_duplicates <- bind_rows(duplicate1, duplicate2, duplicate3)

# Print the counts
print(all_duplicates)
```

::: {.notebox .lightbulb data-latex="lightbulb"}
The result shows that there are no duplicated data in the origin destination bus dataset. Note that the checking was based on a very loose criteria of duplicate, in which duplicated rows need to have the same value across all columns.
:::

#### Geospatial - Bus Stop
The following codes 
```{r}
# Subset rows where origin_pt_code has duplicates
duplicates <- busstop[duplicated(busstop$origin_pt_code) | duplicated(busstop$origin_pt_code, fromLast = TRUE), ]

# Display the rows with duplicate origin_pt_code
kable(head(duplicates, n=32))
```

::: {.notebox .lightbulb data-latex="lightbulb"}
The result shows that there are some duplicated data in the geospatial bus stop dataset. This might interfere with the joining data process and generated double count on later on. Note that the checking was based on the origin_pt_code field only, because it will be the basis of reference for joining the dataset later on.
At a glance, the table above also show that, the ***duplicated code are actually located near each other***. Therefore, removing the duplicates can be considered safe as the remaining *bus stop code* can represent the deleted one. The folowing code chunk will execute the duplicate removal and show the result where number of rows have reduced.
:::

```{r}
# Keep one row of the duplicates in the original dataset
busstop <- busstop[!duplicated(busstop$origin_pt_code) | duplicated(busstop$origin_pt_code, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(busstop)
```

:::



### categorical peak time variable
on the interest of analyzing the peak time, the following code will assign new column that categorize peak time, filter data that is not on peak time, and show a sample of the output.

```{r}
# Function to categorize and aggregate trips
categorize_and_aggregate <- function(odb) {
  odb <- odb %>%
    # Categorize trips under periods based on day and timeframe
    mutate(period = case_when(
      DAY_TYPE == "WEEKDAY" & TIME_PER_HOUR >= 6 & TIME_PER_HOUR <= 9 ~ "Weekday morning peak",
      DAY_TYPE == "WEEKDAY" & TIME_PER_HOUR >= 17 & TIME_PER_HOUR <= 20 ~ "Weekday evening peak",
      DAY_TYPE == "WEEKENDS/HOLIDAY" & TIME_PER_HOUR >= 11 & TIME_PER_HOUR <= 14 ~ "Weekend/holiday morning peak",
      DAY_TYPE == "WEEKENDS/HOLIDAY" & TIME_PER_HOUR >= 16 & TIME_PER_HOUR <= 19 ~ "Weekend/holiday evening peak",
      TRUE ~ "non-peak"
    )) %>%
    # Only retain needed periods for analysis
    filter(period != "non-peak") %>%
    # Compute the number of trips per origin bus stop per month for each period
    group_by(YEAR_MONTH, period, ORIGIN_PT_CODE) %>%
    summarise(num_trips = sum(TOTAL_TRIPS)) %>%
    # Change all column names to lowercase
    rename_with(tolower, everything()) %>%
    ungroup()

  return(odb)
}

# Apply the function to each dataset
odb8 <- categorize_and_aggregate(odb8)
odb9 <- categorize_and_aggregate(odb9)
odb10 <- categorize_and_aggregate(odb10)

# sample the result
glimpse(odb10)
```

### choosing the month
In order to decide which month is better to perform LISA or whether analyzing all month separately will yield interesting result, it is a good step to check the data distribution of each month. By comparing the data distribution for each month, we can make an educated guess whether the LISA result for each month would be starkly different or just similar.  
```{r}
# Combine odb8, odb9, and odb10 into one dataframe
combined_data <- bind_rows(
  mutate(odb8, month = "odb8"),
  mutate(odb9, month = "odb9"),
  mutate(odb10, month = "odb10")
)

# Create a standard vertical box plot
bus_boxplot <- combined_data %>%
  ggplot(aes(x = period, y = num_trips, fill = period)) + # Assigning aesthetics for x and y axes, and fill color based on period
  geom_boxplot() + # Adding the box plot layer
  facet_wrap(~month, scales = 'free_x') + # Faceting by month, with free scales for x axis
  labs(
    title = "Distribution of Trips across Peak Periods",
    subtitle = "Comparison across different months",
    x = "Period",
    y = "Number of Trips"
  ) +
  theme_minimal() + # Using a minimal theme for a cleaner look
  theme(axis.text.x = element_blank(), axis.title.x = element_blank()) # Removing x-axis category labels and label  
bus_boxplot

```
The box plot to show the data distribution was not too helpful as it shows that all peak time across months has extreme outliers. Therefore, the next code chunk modify the boxplot by log-transforming the number of trips.
```{r}
# Create a modified vertical box plot
bus_boxplot <- combined_data %>%
  ggplot(aes(x = period, y = log(num_trips), fill = period)) + # Modified aesthetics with log-transformed y-axis
  geom_boxplot() + # Adding the box plot layer
  facet_wrap(~month, scales = 'free_x') + # Faceting by month, with free scales for x axis
  labs(
    title = "Distribution of Log-Transformed Trips",
    subtitle = "Comparison across different months",
    y = "Log(Number of Trips)"
  ) +
  theme_minimal() + # Using a minimal theme for a cleaner look
  theme(axis.text.x = element_blank(), axis.title.x = element_blank()) # Removing x-axis category labels and label

bus_boxplot
```
::: {.notebox .lightbulb data-latex="lightbulb"}
The log-transformed box plot show that **the distribution of each peak periods across months is quite similar**. In general, number of trips in the weekday peaks are typically higher than weekend/holiday peak. The similarity also extend to extreme outliers. For example, the green box plot (*Weekday morning peak*) always have a single point extreme outlier on the top of the box plot. Based on this observation, it can be inferred that performing LISA on one of month is most likely enough to discover the pattern. The month October, as the latest data available, is chosen for the next processes.
:::


### extract trip numbers into wide form of peak period categories

```{r}
# Extract data from odb10 and store as a separate dataframe
odb10_wide <- odb10 %>%
  # Pivot the data wider, treating each row as a bus stop code with peak period trips as columns
  pivot_wider(
    names_from = period,
    values_from = num_trips
  ) %>%
  select(2:6)

DT::datatable(odb10_wide,
              options = list(pageLength = 8),
              rownames = FALSE)

# check the output
glimpse(odb10_wide)
```

### join aspatial and geospatial data
To retain the geospatial property, use `left_join` by using *busstop* as the main dataframe and *bus stop code* as the reference.  

```{r}
odb10_sf <- left_join(busstop, odb10_wide, by = "origin_pt_code")

glimpse(odb10_sf)
```

### Creating Hexagon Spatial Grid
*odb10_sf* represents a spatial point dataframe, which might not be optimal for spatial autocorrelation analysis where the definition of 'areas' requires the representation of spatial entities as polygons instead of individual points. To address this, the subsequent code sections transform these bus stops into a grid of hexagons.

::: panel-tabset

#### 1. Generate Hexagon Grid

- Utilize the `st_make_grid()` function to create a hexagon grid.
- The `cellsize` parameter, measured in the same units as the dataframe's spatial reference system, determines the width of each hexagon. Given that *odb10_sf* is projected in ESPG code 3414 with meters as the unit, the hexagon width is set to 500m.
- Each hexagon is assigned a unique identifier, known as `hex_id`, which serves as the primary key for future reference.

**Output: Spatial Polygons with Hexagonal Geometry and Hex_id Identification**

```{r}
odb10_hex <- st_make_grid(
    odb10_sf,
    cellsize = 500,
    square = FALSE
  ) %>%
  st_sf() %>%
  rowid_to_column("hex_id")

# check the output
glimpse(odb10_hex)
```

This code utilizes the `st_make_grid` function to create a hexagon grid based on the specified parameters. The resulting hexagon grid is then converted into a spatial dataframe (`st_sf()`) to maintain its geospatial properties. The `rowid_to_column` function is employed to assign a unique identifier (`hex_id`) to each hexagon, facilitating subsequent analyses or referencing.

#### 2. Generate Attribute Dataframe using Hexagon Identifiers

Given that each hexagonal area may encompass multiple bus stops, the `hex_id` serves as the primary key for aggregation. The subsequent procedures outline how to organize attributes based on `hex_id`:

- Utilize the `st_join()` function with `join = st_within` to associate bus stop points with hexagon areas.
- The `st_set_geometry(NULL)` argument eliminates the geospatial layer, focusing on attribute aggregation.
- Employ `group_by()` to obtain a unique `hex_id` for each row.
- Utilize `summarise()` to calculate the aggregate count of bus stops and trips per peak period within each hexagon area.
- Replace all `NA` values with 0 using `replace(is.na(.), 0)`.



**Output: Attribute Dataframe, with Hex_id as the Primary Key**

```{r}
odb10_stops <- st_join(
  odb10_sf, 
  odb10_hex, 
  join = st_within
) %>%
  st_set_geometry(NULL) %>%
  group_by(hex_id) %>%
  summarise(
    n_busstops = n(),
    busstops_code = str_c(origin_pt_code, collapse = ","),
    loc_desc = str_c(loc_desc, collapse = ","),
    `Weekday morning peak` = sum(`Weekday morning peak`, na.rm = TRUE),
    `Weekday evening peak` = sum(`Weekday evening peak`, na.rm = TRUE),
    `Weekend/holiday morning peak` = sum(`Weekend/holiday morning peak`, na.rm = TRUE),
    `Weekend/holiday evening peak` = sum(`Weekend/holiday evening peak`, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  mutate(across(where(is.numeric), ~ replace_na(., 0)),
         across(where(is.character), ~ replace_na(., "")))

# check the output
glimpse(odb10_stops)
```

#### 3. Join the grid and attribute dataframe

- Use `left_join` to combine the new **odb10_stops** attribute dataframe with the existing **odb10_hex** hexagon geospatial layer, linking them by `hex_id` to integrate attributes into the spatial polygon dataframe.
- Employ `filter` to exclude hexagons without bus stops.

**Output: Spatial Polygon Dataframe**


```{r}
odb10_complete <- odb10_hex %>%
  left_join(odb10_stops,
            by = "hex_id"
  ) %>%
  replace(is.na(.), 0)

odb10_final <- filter(odb10_complete,
                       n_busstops > 0)

# check the output
glimpse(odb10_final)
```
:::

In Step 2, the code outlines a process for associating bus stop attributes with hexagonal areas using `st_join`. This involves grouping and summarizing the attributes based on the unique `hex_id`. The resulting dataframe, **odb10_stops**, serves as an attribute-centric representation with the `hex_id` as the primary key.

Moving to Step 3, the code then combines this attribute dataframe with the original hexagon geospatial layer, **odb10_hex**, utilizing `left_join`. The resulting spatial polygon dataframe, **bustraffic10**, integrates both geometric and attribute information. The `filter` operation ensures that only hexagons containing bus stops are retained in the final dataframe.


# Making Sense of the Data (GeoVisualization and Analysis)

Transform Data Here if exploration show sumtin glimpse (wankee) every dataset or think of a better method bus stop, population, passenger volume, subzone. Isn't this also better be put on data checking?

## Distribution of Bus Stop
```{r}
# Plot the hexagon grid based on n_busstops
tmap_mode("view")
busstop_map = tm_shape(odb10_final)+
  tm_fill(
    col = "n_busstops",
    palette = "Blues",
    style = "cont",
    title = "Number of Bus Stops",
    id = "loc_desc",
    showNA = FALSE,
    alpha = 0.6,
    popup.format = list(
      grid_id = list(format = "f", digits = 0)
    )
  )+
  tm_borders(col = "grey40", lwd = 0.7)
busstop_map
```

::: {.notebox .lightbulb data-latex="lightbulb"}
From the map, we can infer that the central region, likely encompassing the Central Business District (CBD) and surrounding residential areas, which are known to be highly populated and active, has a higher number of bus stops. This correlates with the need for robust public transportation in densely populated areas to support the daily commute of residents and workers. Lighter shades are observed towards the periphery of the island, suggesting fewer bus stops, which could correspond to less urbanized or industrial areas, like the Western and North-Eastern regions where the population density is lower. The map reflects Singapore's urban planning and transportation strategy, which aims to provide accessibility and connectivity, particularly in high-density regions where demand for public transit is greatest.
:::



## Distribution of Trips Across Peak Hours
find the ideal breaks for plotting the trips using `summary`
```{r}
summary(odb10_final)
```

::: {.notebox .lightbulb data-latex="lightbulb"}
The summary result confirmed that the trips data is right-skewed and contains extreme outliers. This knowledge is then used to customize the break in the comparison map.
:::

the following code plot the comparison map
```{r}
# Define the columns for which we want to find the global min and max
value_columns <- c("Weekday morning peak", "Weekday evening peak", "Weekend/holiday morning peak", "Weekend/holiday evening peak")

# Set tmap to interactive viewing mode
tmap_mode("view")

# Define a function to create each map with a customized legend
create_map <- function(value_column) {
  tm_shape(odb10_final) +
    tm_fill(
      col = value_column,
      palette = "-RdBu",
      style = "fixed",
      title = value_column,
      id = "loc_desc",
      showNA = FALSE,
      alpha = 0.6,
      breaks = c(0, 500, 1000, 2000, 3000, 5000, 10000, 50000, 100000, 300000, 550000)
    ) +
    tm_borders(col = "grey40", lwd = 0.7) +
    tm_layout(
      legend.position = c("right", "bottom"), # Adjust legend position
      legend.frame = TRUE,
      legend.width = 0.15, # Adjust the width of the legend
      legend.height = 0.5  # Adjust the height of the legend
    )
}

# Apply the function to each value column and store the maps
map_list <- lapply(value_columns, create_map)

# Combine the maps into a 2x2 grid
combined_map <- tmap_arrange(map_list[[1]], map_list[[2]], map_list[[3]], map_list[[4]], ncol = 1, nrow = 4)

# Show the combined map
combined_map
```

::: {.notebox .lightbulb data-latex="lightbulb"}
Using the same classification scaling for all maps, it clearly shows that weekdays trips volume is higher in general than weekend/holiday. Nevertheless, at a glance, the crowded area are more or less the same across all days and time. This confirmed the previous finding on bus stops in which the area with most traffics are likely to encompassing the Central Business District (CBD) and surrounding residential areas, which are known to be highly populated and active the area of Singapore. At the same time, it also reflects Singapore's urban planning and transportation strategy, in which the busiest area with potential high traffics are supported by more bus stops.
:::

## Distribution of Average Trips per Bus Stop
The next distribution will see which bus stop is the busiest on average, in terms of number of trips per bus stop. To do that, firstly the following code will generate new columns of trip per bus stop for each hexagon.
```{r}
# Create a new dataframe with transformed columns
odb10_final_avg <- odb10_final %>%
  mutate(across(all_of(value_columns), ~ .x / n_busstops, .names = "avg_{.col}"))
# check the summary for determining break points
summary(odb10_final_avg)
```
the following code plot the comparison map
```{r}
# Define the columns for which we want to find the global min and max
value_columns <- c("avg_Weekday morning peak", "avg_Weekday evening peak", "avg_Weekend/holiday morning peak", "avg_Weekend/holiday evening peak")

# Set tmap to interactive viewing mode
tmap_mode("view")

# Define a function to create each map with a customized legend
create_map <- function(value_column) {
  tm_shape(odb10_final) +
    tm_fill(
      col = value_column,
      palette = "-RdBu",
      style = "fixed",
      title = value_column,
      id = "loc_desc",
      showNA = FALSE,
      alpha = 0.6,
      breaks = c(0, 100, 200, 300, 400, 500, 750, 1000, 1500, 5000, 10000, 50000, 140000)
    ) +
    tm_borders(col = "grey40", lwd = 0.7) +
    tm_layout(
      legend.position = c("right", "bottom"), # Adjust legend position
      legend.frame = TRUE,
      legend.width = 0.15, # Adjust the width of the legend
      legend.height = 0.5  # Adjust the height of the legend
    )
}

# Apply the function to each value column and store the maps
map_list <- lapply(value_columns, create_map)

# Combine the maps into a 2x2 grid
combined_map <- tmap_arrange(map_list[[1]], map_list[[2]], map_list[[3]], map_list[[4]], ncol = 1, nrow = 4)

# Show the combined map
combined_map
```

::: {.notebox .lightbulb data-latex="lightbulb"}
at a glance, using the average trips per bus stop shows slightly different insight. Comparatively to the total trips, average number of trips shows that the area around **Jurong**, **Woodlands**, and bus stops in **Johor** (part of Malaysia) is actually busier than what total trips suggest. In the context of transport policy, this might be the first lead to expand the number of bus stops in the particular area to cater for more commuters.
:::


# Local Indicators of Spatial Association analysis

In this section, we will employ Local Indicators of Spatial Association (LISA) statistics to explore the presence of clusters or outliers in the total number of trips generated at the hexagon layer. Specifically, we will utilize Moran's I statistic to detect spatial patterns such as clustering or dispersion of total trips.

## Constructing a Distance-Based Spatial Weights Matrix

Before delving into global/local spatial autocorrelation statistics, we need to create a spatial weights matrix for our study area. This matrix defines the neighbors of hexagonal spatial units based on their distances. Here are some considerations:

- Each feature should have at least one neighbor, and no feature should be a neighbor with all other features.

- For skewed data, each feature should ideally have at least 8 neighbors (12 is used here).

Given the sparse distribution of bus stops in certain regions (e.g., central catchment areas, military training areas, and airports), distance-based methods are favored over contiguity methods.

**Adaptive Distance-Based (Fixed Number of Neighbors)** method is chosen over **Fixed-Distance Threshold:**. This method is chosen due to the right-skewed nature of our data. In this method, Each hexagon is guaranteed at least ***n***neighbors, facilitating later statistical significance testing.

We will set each hexagon to have 12 neighbors using the provided R code. The process involves using `st_knn` to obtain a list of neighbors, and then `st_weights` to generate row-standardized spatial weights.

```{r}
wm_all <- odb10_final %>% 
  mutate(
    nb = st_knn(geometry, k = 12),
    wt = st_weights(nb, style = 'W'),
    .before = 1
  )

# check the output
kable(head(wm_all))
```

## Global Autocorrelation of Spatial Association (Global Moran's I with Simulation)

Global spatial association evaluates the overall spatial pattern of a variable, in this case, the total number of trips across the entire study area. It provides a single metric summarizing the extent to which similar values cluster together or disperse across the geographic space.

Compute Global Moran's I for each peak time trips generated at the hexagon level, utilizing simulated data to avoid assumptions of normality and randomness. The number of simulations is set to 99+1 = 100.

```{r}
# Set the seed to ensure reproducibility
set.seed(1234)

# define the value_columns
value_columns <- c("Weekday morning peak", "Weekday evening peak", "Weekend/holiday morning peak", "Weekend/holiday evening peak")

# Create a function to perform global Moran's I test
perform_global_moran <- function(data, value_column, k) {
  # Compute spatial weights
  nb <- st_knn(data$geometry, k = k)
  wt <- st_weights(nb, style = 'W')

  # Perform global Moran's I test
  moran_result <- global_moran_perm(data[[value_column]], nb, wt, nsim = 99)
  
  # Include the value_column in the result
  result <- list(
    value_column = value_column,
    moran_result = moran_result
  )
  
  return(result)
}

# Apply the function for each time interval
results <- lapply(value_columns, function(vc) perform_global_moran(wm_all, vc, k = 12))

# Print the results
print(results)
```
::: {.notebox .lightbulb data-latex="lightbulb"}
For all four time intervals, since the p-value for global Moran's I is smaller than 0.05, we can reject the null hypothesis that the spatial patterns are random. Moreover, as the Moran's I statistics are greater than 0, we can infer that the spatial distribution exhibits signs of clustering for all four time intervals, consistent with the choropleth maps plotted earlier.
:::

## Local Autocorrelation of Spatial Association

Local spatial association provides a more detailed examination of spatial patterns at the local level, identifying specific areas with strong or weak spatial association. Local Moran's I categorizes regions as high-high, low-low, high-low, or low-high, indicating clustering or outlier behavior.

Compute Local Indicators of Spatial Association (LISA) for passenger trips generated by origin at the hex level. The function `local_moran` from the `sfdep` package is utilized, automatically computing neighbor lists and weights using simulated data. The results are then unnested for further analysis and displayed in an interactive datatable format.

```{r}
# Create a function to perform local Moran's I analysis
get_lisa <- function(wm, value_column, k) {
  # Compute spatial weights
  nb <- st_knn(wm$geometry, k = k)
  wt <- st_weights(nb, style = 'W')

  # Perform local Moran's I analysis and create a new data frame
  result <- wm %>% 
    mutate(
      local_moran = local_moran(.data[[value_column]], nb, wt, nsim = 99),
      .before = 1
    ) %>%
    unnest(local_moran)
  
  return(result)
}

# Initialize a list to store results for each value_column
lisa_results <- list()

# Apply the function for each time interval and store results in the list
for (vc in value_columns) {
  lisa_results[[vc]] <- get_lisa(wm_all, vc, k = 12)
  
  # Remove columns that don't belong to the specific time interval
  unwanted_columns <- setdiff(value_columns, vc)
  lisa_results[[vc]] <- lisa_results[[vc]][, !(names(lisa_results[[vc]]) %in% unwanted_columns)]
}

# show sample output in an interactive table
datatable(lisa_results[["Weekday morning peak"]], 
          class = 'cell-border stripe', 
          options = list(pageLength = 5))

```

## Visualizing Significant Local Moran's I at 95% Confidence Level

Utilize `tmap` core functions to construct choropleth maps displaying the Local Moran's I field and p-value field for all four time intervals. Only significant values of Local Moran's I at the 95% confidence level are plotted.

```{r}
get_sig_lmi_map <- function(lisa_table, title) {
  
  sig_lisa_table <- lisa_table %>%
    filter(p_ii_sim < 0.05)
  
  result <- tm_shape(lisa_table) +
    tm_polygons() +
    tm_borders(alpha = 0.5) +
    tm_shape(sig_lisa_table) +
    tm_fill("ii") + 
    tm_borders(alpha = 0.4) +
    tm_layout(
      main.title = title,
      main.title.size = 1.3
    )
  
  return(result)
  
}

sig_lmi_1 <- get_sig_lmi_map(lisa_results[["Weekday morning peak"]], "Local Moran's I of Total Trips generated on Weekday Morning at 95% CL" )
sig_lmi_2 <- get_sig_lmi_map(lisa_results[["Weekday evening peak"]], "Local Moran's I of Total Trips generated on Weekday Afternoon at 95% CL" )
sig_lmi_3 <- get_sig_lmi_map(lisa_results[["Weekend/holiday morning peak"]], "Local Moran's I of Total Trips generated on Weekend Morning at 95% CL" )
sig_lmi_4 <- get_sig_lmi_map(lisa_results[["Weekend/holiday morning peak"]], "Local Moran's I of Total Trips generated on Weekend Afternoon at 95% CL" )

tmap_mode('plot')

tmap_arrange(
  sig_lmi_1,
  sig_lmi_2,
  sig_lmi_3,
  sig_lmi_4,
  asp = 2,
  nrow = 2,
  ncol = 2
)
```

::: {.notebox .lightbulb data-latex="lightbulb"}
The choropleth maps displaying Local Moran's I reveal that **darker orange** and **darker green** areas signify **outlier regions**. To classify it into low-high or high-low cluster, further analysis is required using LISA. Similarly, the **light green** are indicates either high-high or low-low regions. Notably, the the area around **Tuas and Jurong**is most likelya low-low area based on the previous geovisualization.
:::

```{r}
get_sig_lisa_map <- function(lisatable, title) {
  
  sig_lisatable <- lisatable  %>%
  filter(p_ii_sim < 0.05)
  
  result <- tm_shape(lisatable) +
    tm_polygons(alpha = 0.5) +
    tm_borders(alpha = 0.5) +
    
    tm_shape(sig_lisatable) +
    tm_fill("median",
            palette = c("#2c7bb6",  "#fdae61", "#abd9e9", "#d7191c"),
            alpha= 0.7) + 
    tm_dots('grid_id', alpha=0.05) +
    tm_borders(alpha = 0.4) +
    tm_layout(main.title = title,
              main.title.size = 1.5,
              legend.position = c("left", "top"))
    
  return(result)
  
}

sig_lisa_1 <- get_sig_lisa_map(lisa_results[["Weekday morning peak"]],"LISA categories generated on Weekday Morning at 95% CL" )
sig_lisa_2 <- get_sig_lisa_map(lisa_results[["Weekday evening peak"]], "LISA categories generated on Weekday Afternoon at 95% CL" )
sig_lisa_3 <- get_sig_lisa_map(lisa_results[["Weekend/holiday morning peak"]], "LISA categories generated on Weekend Morning at 95% CL" )
sig_lisa_4 <- get_sig_lisa_map(lisa_results[["Weekend/holiday morning peak"]], "LISA categories generated on Weekend Afternoon at 95% CL" )

The choropleth maps above show significant clusters and outliers for passenger trips generated by origin at each hexagon. The confidence level used is 95%.

**Low-low regions** represent Low-Low clusters.

**Low-High regions** indicate Low-High outliers.

**High-Low regions** represent High-Low outliers.

**High-High regions** indicate High-High clusters.

The spatial pattern exhibits higher concentrations in the East, North, and West residential regions compared to the Southern part of Singapore. Major interchanges contributing to high clusters include Ang Mo Kio, Bedok, Boon Lay, Bukit Batok, Chua Chu Kang, Clementi, Hougang, Joo Koon, Seng Kang, Tampines, Toa Payoh, Woodlands, and Yishun.

**Analysis by Regions**

- **High Clusters:** North, East, and West residential regions.
- **Low Clusters:** Western parts of Singapore, mainly industrial zones with company-provided transport.

**Analysis of Edges**
Clusters of low passenger trips are observed along the Western and Southern edges, with notable outliers such as Gul Circle, Jurong port, Pandan subzones, Penjuru Crescent, Pioneer Sector, Samulun, Shipyard, and Tuas.

**Analysis of Weekends vs. Weekdays**
Hexagons along Bukit Timah/Dunearn Road, Singapore Botanic Gardens, and Woodlands Checkpoint exhibit interesting patterns during weekends, transitioning into high-low outliers or high clusters.

While this analysis identifies clusters and outliers and provides potential reasons, other factors like income levels, population density, and demographics may contribute to the observed spatial patterns.


# References

Andrey Sekste and Eduard Kazakov. ["H3 hexagonal grid: Why we use it for data analysis and visualization"](https://www.kontur.io/blog/h3-hexagonal-grid/).

Anselin, L. (1995). ["Local indicators of spatial association -- LISA"](https://onlinelibrary.wiley.com/doi/10.1111/j.1538-4632.1995.tb00338.x). Geographical Analysis, 27(4): 93-115.

Sarah Gates (2017). ["Emerging Hot Spot Analysis: Finding Patterns over Space and Time"](https://www.azavea.com/blog/2017/08/15/emerging-hot-spot-spatial-statistics/)

Sid Dhuri (2020). ["Spatial Data Analysis With Hexagonal Grids"](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e)

Tin Seong Kam. ["2 Choropleth Mapping with R"](https://r4gdsa.netlify.app/chap02) From R for Geospatial Data Science and Analytics 

Tin Seong Kam. ["9 Global Measures of Spatial Autocorrelation"](https://r4gdsa.netlify.app/chap09) From R for Geospatial Data Science and Analytics 

Tin Seong Kam. ["10 Local Measures of Spatial Autocorrelation"](https://r4gdsa.netlify.app/chap10) From R for Geospatial Data Science and Analytics 

Land Transport Authority. [Land Transport Data Mall](https://datamall.lta.gov.sg/content/datamall/en.html)

Wong, Kenneth. [Create spatial square/hexagon grids and count points inside in R with sf](https://urbandatapalette.com/post/2021-08-tessellation-sf/) from Urban Data Pallete
