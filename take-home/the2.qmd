---
title: "Take-home 2 - Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows"
author: "Muhamad Ameer Noor"
date: "15 December 2023"
date-modified: "last-modified"
editor: source
format: 
  html:
    self_contained: false
    code-fold: true
    code-summary: "code chunk"
    fontsize: 17px
    number-sections: true
    number-depth: 2
execute:
  echo: true # all code chunks will appear
  eval: true # all code chunks will run live (be evaluated)
  warning: false # don't display warnings
  message: false
---

![Illustration](../images/the2.png)

# Overview

::: panel-tabset
## The Scene

Understanding why city residents wake up early to travel from home to work and assessing the consequences of discontinuing a public bus service along a specific route are key challenges faced by transport operators and urban managers in the realm of urban mobility. Traditionally, answering such questions relied on expensive, time-consuming commuter surveys. However, these surveys not only demanded considerable resources but also yielded data that took a substantial amount of time to clean and analyze, often rendering it outdated by the time reports were ready.

In today's digital era, urban infrastructures, including public buses and mass rapid transits, are becoming increasingly digital. The proliferation of technologies like GPS on vehicles and SMART cards for public transport users generates vast geospatial data sets, offering insights into movement patterns over time and space. Despite this wealth of data, planners struggle to effectively leverage and transform it into valuable information, impacting the return on investment in data collection and management.

To address this gap, this exercise conducts a case study showcasing the potential of Geographic Information System and Spatial Data Analysis (GDSA). By integrating data from various sources, this approach aims to build spatial interaction models that unveil the factors influencing urban mobility patterns in the context of public bus transit.

## The Objective

This task aims to achieve specific goals, focusing on General Geospatial Data Science and Spatial Interaction Modeling.

::: panel-tabset
### General Geospatial Data Science

For the General Geospatial Data Science, the aim is to do the following: - Create a detailed hexagon map (375m distance from center to edges) representing Traffic Analysis Zones (TAZ). - Various time will have different pattern of traffic flow. For this study, the focus is on the period of ***weekday morning peak from 6am to 9am***. - Develop an Origin-Destination (O-D) matrix illustrating commuter flows during the chosen time interval. - Visualize passenger trip flows using geospatial methods, analyzing the observed spatial patterns. - Gather relevant data, both spatial and aspatial, from publicly available sources. - Generate a distance matrix based on the earlier derived hexagon data.

::: {.callout-note collapse="true" title="is 375m a magical number?"}
Studies have found that people are usually willing to walk approximately 750 meters to get to public transportation. A more specific comfortable walking distance that takes into account the local weather and how cities are laid out.

When planning city maps and deciding where to place public transport stops, we use hexagons to represent areas on the map. Why hexagons? They fit together perfectly without wasting any space, which makes them great for dividing the map into zones. To match the 750-meter walking distance, each hexagon is sized so that the distance from the center to any edge is half that distance, which is 375 meters. This ensures that anyone within the hexagon is no more than a 750-meter walk away from the center, where a public transport stop would be ideally located. This method is a smart way to make sure that everyone has easy access to transport within a reasonable walking distance.

*summarized from: [Daniels & Mulley](https://www.jstor.org/stable/26202654), [Dhuri](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e), [Sekste & Kazakov](https://www.kontur.io/blog/h3-hexagonal-grid/), and in-class explanation from [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486)*
:::

::: {.callout-note collapse="true" title="what is Traffic Analysis Zones (TAZ)?"}
A Traffic Analysis Zone (TAZ) is a way of dividing a city into smaller areas for transportation planning. Each TAZ has some information about the people and places in it, such as how many people live there, how many cars they have, and where they work or go to school. These information help planners understand how people travel and what kind of transportation they need.

Some key points about TAZs are:

-   The size and shape of a TAZ can vary depending on the location and the purpose of the study. For example, a TAZ in a downtown area might be smaller and more regular than a TAZ in a rural area.

-   The choice of a TAZ system is very important because it affects the accuracy and usefulness of the transportation models. A good TAZ system should reflect the reality of the travel patterns and demands in the city.

-   AZs are not fixed and can be changed or updated over time to reflect new data or changes in the city. However, changing TAZs can also cause some problems, such as losing historical data or making comparisons difficult.

*Summarized from: [Miller, 2021](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf)*
:::

::: {.callout-note collapse="true" title="what is Origin-Destination (O-D) matrix?"}
An Origin-Destination (O-D) matrix is a way of showing how many trips are made from one place to another in a given area. For example, an O-D matrix can tell us how many people travel from their home to their work, or from their hotel to a tourist attraction, or from one city to another. An O-D matrix can help us understand the travel patterns and demands of people, and plan for better transportation systems.

An O-D matrix usually looks like a table, where the rows represent the origins (where the trips start) and the columns represent the destinations (where the trips end). Each cell in the table shows the number of trips between a specific origin and destination. Sometimes, the table can also include other information, such as the mode of transportation (car, bus, bike, etc.), the time of the day, or the purpose of the trip.

Here is an example of a simple O-D matrix for a city with four zones (A, B, C, and D):

|       | A   | B   | C   | D   | Total |
|-------|-----|-----|-----|-----|-------|
| A     | 0   | 10  | 5   | 15  | 30    |
| B     | 20  | 0   | 10  | 10  | 40    |
| C     | 10  | 15  | 0   | 5   | 30    |
| D     | 5   | 5   | 5   | 0   | 15    |
| Total | 35  | 30  | 20  | 30  | 115   |

This table tells us that there are 115 trips in total in the city, and that the most common origin-destination pair is A-D, with 15 trips. It also tells us that no one travels within the same zone (the diagonal cells are zero).

An O-D matrix can be created from different sources of data, such as surveys, GPS, mobile phones, or social media. Depending on the data source, the level of detail and accuracy of the O-D matrix can vary. For example, a survey might ask people to report their exact home and work locations, while a GPS device might only record the coordinates of the start and end points of a trip. Therefore, different methods and techniques are needed to process and analyze the data, and to convert them into a meaningful O-D matrix.

*Summarized From: [Co≈ükun, et al., 2020](https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/449/2020/isprs-archives-XLIII-B4-2020-449-2020.pdf)*
:::

::: {.callout-note collapse="true" title="what is trip flows?"}
Trip flows are the movements of people or things from one place to another in a given area. For example, trip flows can show how many people travel from their home to their work, or from one city to another, or from one country to another. Trip flows can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society.

*Summarized from: [Tao Ran, 2021](https://doi.org/10.1007/978-3-030-55462-0_7)*
:::

::: {.callout-note collapse="true" title="spatial vs aspatial data"}
Spatial data is data that has a geographic or spatial component, meaning that it is related to a specific location on the Earth's surface. For example, the coordinates of a city, the shape of a lake, or the population density of a region are all spatial data. Spatial data can be represented using maps, graphs, or statistics, and can be analyzed using Geographic Information Systems (GIS).

Aspatial data is data that does not have a direct connection to a specific location. For example, the name of a person, the color of a car, or the price of a product are all aspatial data. Aspatial data can be represented using tables, charts, or text, and can be analyzed using various methods such as arithmetic, logic, or statistics.

The main difference between spatial and aspatial data is that spatial data can show the spatial relationships and patterns of the data, such as distance, direction, or proximity, while aspatial data cannot. Spatial data can also be combined with aspatial data to provide more information and insights. For example, a map of a city can show both the spatial data (the location and shape of the buildings) and the aspatial data (the name and use of the buildings).

*Summarized from: [Cengel](https://cengel.github.io/R-spatial/intro.html)*
:::

::: {.callout-note collapse="true" title="what is distance matrix?"}
A distance matrix is a way of showing how far away different places are from each other in a given area. For example, a distance matrix can tell us how many kilometers or minutes it takes to travel from one city to another by car, bus, or bike. Applying geospatial analytics on distance matrix can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society. *dive deeper at (ArcGIS)\[https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-analyst/distance-analysis.htm\]*
:::

### Spatial Interaction Modeling

For the General Spatial Interaction Model, the aim is to do the following:

-   Adjust spatial interaction models to identify factors influencing urban commuting during the specified time.

-   Present modeling results using suitable geovisualization and graphical methods.

-   Interpret the outcomes to gain insights into the factors impacting commuting flows in the urban landscape during the selected time frame.

::: {.callout-note collapse="true" title="what is spatial interaction model?"}
A spatial interaction model is a way of describing how people or things move from one place to another in a given area. For example, a spatial interaction model can show how many people commute from their home to their work, or how many goods are traded between different cities or countries. A spatial interaction model can help us understand the patterns and reasons of these movements, and how they affect the environment, the economy, and the society.

One of the most common spatial interaction models is the gravity model, which is based on an analogy to the physical law of gravity. The gravity model assumes that the movement between two places is proportional to their size (such as population or income) and inversely proportional to their distance. The gravity model can be written as:

$$T_{ij} = k \frac{v_i^{\lambda} w_j^{\alpha}}{d_{ij}^{\beta}}$$

where $T_{ij}$ is the movement from place $i$ to place $j$, $v_i$ is the propulsiveness factor from the origin($i$) place,$w_j$ is the attractiveness factor from the destination ($j$), and $d_{ij}$ is the distance between the two places. $k$ is a constant model parameter, while $\lambda$, $\alpha$, and $\beta$ are parameters that measures the effect of their respective variables.

There are other types of spatial interaction models, such as the potential model and retail model, which have different assumptions and formulations. These models can be used to explain different kinds of movements, such as migration, tourism, or disease spread.

References: [Kam Tin Seong](https://isss624-ay2023-24nov.netlify.app/lesson/lesson03/lesson03-sim#/title-slide) and [Spatial interaction models with R](https://cran.r-project.org/web/packages/simodels/vignettes/simodels.html.)
:::
:::

## The Method

The spatial interaction models being used in this task are the variants of **Gravity Model** including **Unconstrained**, **Origin Constrained**, **Destination Constrained**, and **Doubly Constraint**. The models' performance will be compared to see which one is more appropriate to use in the modelling.

::: {.callout-note collapse="true" title="What is Unconstrained Gravity Model?"}
The Unconstrained Gravity Model is a simple form of the gravity model where the principle of conservation is ignored. This means that the interaction between two locations is not limited by the total number of interactions at the origin or destination. In this model, the interaction between two locations is a constant scaling factor, independent of all origins and destinations.
:::

::: {.callout-note collapse="true" title="What is Origin Constrained Gravity Model?"}
The Origin/Production Constrained Gravity Model, also known as the production constrained model, includes origin-specific balancing factors that act as constraints. These constraints ensure that the estimated rows of the flow data matrix sum to the observed row totals. In other words, the total number of interactions originating from a location is fixed, and the model distributes these interactions across various destinations.
:::

::: {.callout-note collapse="true" title="What is Destination Constrained Gravity Model?"}
The Destination/Attraction Constrained Gravity Model, also known as the attraction constrained model, includes destination-specific balancing factors that act as constraints. These constraints ensure that the estimated columns of the flow data matrix sum to the observed column totals. This means that the total number of interactions attracted to a location is fixed, and the model distributes these interactions across various origins.
:::

::: {.callout-note collapse="true" title="What is Doubly Constrained Gravity Model?"}
The Doubly Constrained Gravity Model includes both origin and destination-specific balancing factors that act as constraints. These constraints ensure that the estimated rows and columns of the flow data matrix sum to the observed row and column totals. In other words, both the total number of interactions originating from a location and attracted to a location are fixed.
:::

To dive deeper on these models, explanation can be found on research by [Haynes & Fotheringham, 1985](https://researchrepository.wvu.edu/rri-web-book/16/) and class materials by [Prof Kam Tin Seong](https://r4gdsa.netlify.app/chap16)

## The Data

the content of the following panel explained what aspatial and geospatial data are used in this project.

::: panel-tabset
### Aspatial

::: panel-tabset
#### Passenger Volume by Origin Destination Bus Stops

-   October 2023 Period

-   downloaded from [LTA DataMall - Dynamic Dataset](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html) via [API](https://datamall.lta.gov.sg/content/dam/datamall/datasets/LTA_DataMall_API_User_Guide.pdf)

-   `csv` format.

-   Columns/Fields in the dataset includes YEAR_MONTH, DAY_TYPE, TIME_PER_HOUR, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE, and TOTAL_TRIPS.

::: {.callout-note collapse="true" title="metadata"}
-   YEAR_MONTH: Represent year and Month in which the data is collected. Since it is a monthly data frame, only one unique value exist in each data frame.

-   DAY_TYPE: Represent type of the day which classified as *weekdays* or *weekends/holidays*.

-   TIME_PER_HOUR: Hour which the passenger trip is based on, in intervals from 0 to 23 hours.

-   PT_TYPE: Type of public transport, Since it is bus data sets, only one unique value exist in each data frame (i.e. *bus*)

-   ORIGIN_PT_CODE: ID of origin bus stop

-   DESTINATION_PT_CODE: ID of destination bus stop

-   TOTAL_TRIPS: Number of trips which represent passenger volumes
:::

::: {.callout-note collapse="true" title="Tutorial on Fetching the Data"}
1.  Click this [link](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html), and click the `Request for API Access` ![](../images/tutorial1.jpg)

2.  Fill in the required form ![](../images/tutorial2.jpg)

3.  Check email for confirmation. The `API Account Key` will be required for later step. ![](../images/tutorial3.jpg)

4.  The user guide from LTA [here](https://datamall.lta.gov.sg/content/dam/datamall/datasets/LTA_DataMall_API_User_Guide.pdf) will explains how to make API calls. The user guide also provide the link required for various kind of dataset, keep the link for future use. ![](../images/tutorial4.jpg)

5.  *The following step assume usage of desktop apps version of `Postman` to make the API call*. Firstly, go to [Postman](https://www.postman.com/) and click on the logo of the OS system that you are using. ![](../images/tutorial5.jpg)

6.  *The following step is for Windows User, adjust accordingly if you use other OS*. Click on the dowload button, install the apps, and launch it. ![](../images/tutorial6.png)

7.  In the apps, copy-paste the url from step 4, and make sure that the option is set to `GET`. In this case where the data is monthly, you need to add a parameter of the month data that you want to download in the format of YYYYMM (202308 shown in the example). The parameter is under `Params` section. ![](../images/tutorial7.png)

8.  Next, go to `Headers` section and add AccountKey which value can be obtained from step 3. Click the blue `Send` button. ![](../images/tutorial8.png)

9.  Click the link that will come out on the bottom of the apps, it will be opened in a new tab. ![](../images/tutorial9.png)

10. In this last step, click `Send and Download` in the new tab. You can choose where to put the data and the download will start. ![](../images/tutorial10.jpg)
:::

#### HDB

The dataset contains comprehensive information about various Housing and Development Board (HDB) blocks situated in Singapore. The dataset not only includes details on the number and types of dwelling units but also crucially provides the geographic coordinates, specifically the longitudes and latitudes, corresponding to each HDB block. This geographical information opens the door to transforming the dataset into a spatial data frame, enabling us to treat the HDB block details as a spatial object. By leveraging these coordinates, we can engage in spatial analysis and visualization, gaining valuable insights into the spatial distribution and relationships among different HDB blocks across Singapore. This spatial perspective enhances the depth of understanding and opens avenues for exploring the geographical patterns inherent in the HDB block dataset. The original dataset can be downloaded from [Singapore's National Open Data Collection](https://beta.data.gov.sg/collections/150/view). However, the original dataset does not contain geoidentifier. The geoidentifier in the HDB dataset that is used here was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486).

#### School

The School Directory and Information dataset, sourced from [Singapore's National Open Data Collection](https://beta.data.gov.sg/collections/457/datasets/d_688b934f82c1059ed0a6993d2a829089/view), provides valuable information pertinent to the morning period, notably involving students commuting to school. The dataset encompasses details about the locations, implicitly indicated by the 'POSTAL_CODE' field, of MOE kindergartens, primary schools, secondary schools, and junior colleges. Notably, it excludes information on the locations of ITEs, polytechnics, and universities. With a total of 346 records, this dataset serves as a comprehensive resource for understanding the geographical distribution of various educational institutions.
:::

### Geospatial

Geospatial data in `shp` format are used in this project, as shown in the following panel:

::: panel-tabset
#### Bus Stop Location

-   provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.

-   downloaded from [LTA DataMall - Static Dataset](https://datamall.lta.gov.sg/content/datamall/en/static-data.html)

-   Columns/Fields in the dataset includes BUS_STOP_N, BUS_ROOF_N, LOC_DESC, and geometry.

::: {.callout-note collapse="true" title="metadata"}
-   BUS_STOP_N: The unique identifier for each bus stop.
-   BUS_ROOF_N: The identifier for the bus route or roof associated with the bus stop.
-   LOC_DESC: Location description providing additional information about the bus stop's surroundings.
-   geometry: The spatial information representing the location of each bus stop as a point in the SVY21 projected coordinate reference system.
:::

#### URA Master Plan 2019 (MPSZ)

-   Provides information about the sub-zone boundary of Urban Redevelopment Authority (URA) Master Plan 2019 (MPSZ-2019).

-   The original dataset can be downloaded from [Singapore's National Open Data Collection](https://beta.data.gov.sg/). However, the original dataset have a different file type from the one that is used in this project. The changed file type of the data was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486).

-   Columns/Fields in the dataset includes SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C, REGION_N, REGION_C, and geometry

-   While the analysis primarily focuses on hexagon cells, incorporating the Master Planning Sub-Zone 2019 file enables the integration of additional point layers like Retail and Leisure onto the Singapore map. This integration facilitates the visualization of their respective locations within various planning sub-zones across Singapore.

::: {.callout-note collapse="true" title="metadata"}
-   SUBZONE_N: The unique name for each subzone boundary.
-   SUBZONE_C: The unique identifier for each subzone boundary.
-   PLN_AREA_N: The unique name for each planning area.
-   PLN_AREA_C: The unique identifier for each planning area
-   REGION_N: The unique name for each region.
-   REGION_C: The unique identifier for each region.
-   geometry: The spatial information representing the location of each subzone boundary in Coordinate Reference System (CRS) from World Geodetic Systems (WGS) 84.
:::

#### Hexagon

A [hexagon](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm) layer of 375m (perpendicular distance between the centre of the hexagon and its edges.) Each spatial unit is regular in shape and finer than the Master Plan 2019 Planning Sub-zone GIS data set of URA.

::: {.callout-note collapse="true" title="why hexagon?"}
-   ***Uniform Distances Everywhere***: Think of hexagons as honeycomb cells. Each cell (hexagon) touches its neighbors at the same distance from its center. It's like standing in the middle of a room and being the same distance from every wall, making it easier to measure and compare things.

-   ***Outlier-Free Shape***: Hexagons are like well-rounded polygons without any pointy tips. Sharp corners can create odd spots in data, but hexagons smoothly cover space without sticking out anywhere. This helps prevent weird data spikes that don't fit the pattern.

-   ***Consistent Spatial Relationships***: Imagine a beehive where every hexagon is surrounded by others in the same pattern. This regular pattern is great for analyzing data because you can expect the same relationships everywhere, making the data predictable and easier to work with.

-   ***Ideal for Non-Perpendicular Features***: Real-world features like rivers and roads twist and turn. Squares can be awkward for mapping these, but hexagons, which are more circular, can follow their flow better. This way, a hexagon-based map can mimic the real world more closely than a checkerboard of squares.

Summarized from: [Dhuri](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e), and [Sekste & Kazakov](https://www.kontur.io/blog/h3-hexagonal-grid/).
:::

#### Other Supporting Data

The following dataset will support as propulsive/attractive factors for the modelling. The data was provided by [Prof Kam Tin Seong](https://faculty.smu.edu.sg/profile/kam-tin-seong-486). The data includes:

-   Business: provide information about business locations across Singapore

-   Rapid Transit System Station: encompasses the geographical positions of Mass Rapid Transit (MRT) and Light Rail Transit (LRT) stations in Singapore, represented as polygon shapes.

-   Train Station Exit Layer: includes exit points for all MRT and LRT stations in Singapore, stored as individual points.

-   Entertainment: highlights the locations of entertainment venues in Singapore, such as cinemas and theaters, presented as points.

-   Food & Beverage: captures the locations of Food & Beverage venues in Singapore, such as restaurants and cafes, organized as points.

-   Financial Services: showcases the locations of Financial Services in Singapore, encompassing ATMs, money changers, and banks, stored as individual points.

-   Leisure & Recreation: denotes the locations of Leisure and Recreation venues in Singapore, spanning sports venues, museums, and galleries, organized as points.

-   Retails: documents the locations of Retail venues in Singapore, encompassing all shops that may not fit into other categories, presented as points.
:::
:::
:::

# Preparation

Before starting with the analysis, we have to load the library and import the data. This section also contains minor checking and setup of the data.

## Import Library

The following code chunk utilizing [pacman](https://www.rdocumentation.org/packages/pacman/versions/0.5.1) will import the required library (and install it if it does not exist in the environment yet).

```{r}
#| code-fold: false
pacman::p_load(tmap, sf, tidyverse, sfdep, knitr, Hmisc, mapview, DT, sp, stplanr, reshape2, skimr, performance, plotly, httr, corrplot, gifski, patchwork)
```

::: {.callout-note collapse="true" title="Packages Explanations"}
-   [tmap](https://cran.r-project.org/web/packages/tmap/): Used for creating thematic maps in R, both static and interactive, with extensive mapping capabilities.

-   [sf](https://r-spatial.github.io/sf/): Handles and manipulates geospatial data, enabling operations like reading, writing, transforming, and visualizing spatial data.

-   [tidyverse](https://www.tidyverse.org/): A suite of R packages designed for data science tasks, including data manipulation, exploration, and visualization.

-   [sfdep](https://cran.r-project.org/web/packages/sfdep/index.html): This package provides methods for measuring and diagnosing spatial dependence in linear regression models, particularly when working with spatial econometrics. It is tailored to work with 'sf' objects, which are used to handle spatial data in R.

-   [knitr](https://yihui.org/knitr/): Allows for dynamic report generation with R, making it easy to integrate R code into reports and weave together narrative text and code output.

-   [Hmisc](https://cran.r-project.org/web/packages/Hmisc/index.html): Contains many functions useful for data analysis, high-level graphics, utility operations, and functions for dealing with missing values.

-   [mapview](https://cran.r-project.org/web/packages/mapview/index.html): Facilitates the interactive viewing of spatial data in R, built on top of Leaflet.js.

-   [DT](https://rstudio.github.io/DT/): Provides an R interface to the JavaScript library DataTables, useful for creating interactive tables in R markdown documents and Shiny apps.

-   [sp](https://cran.r-project.org/web/packages/sp/index.html): Provides classes and methods for spatial data, and has been superseded by `sf` but is still widely used for compatibility reasons.

-   [stplanr](https://cran.r-project.org/web/packages/stplanr/index.html): Offers sustainable transport planning tools for spatial lines, networks, and movement data.

-   [reshape2](https://cran.r-project.org/web/packages/reshape2/index.html): An R package that allows you to flexibly reshape data, such as melting and casting data frames.

-   [skimr](https://cran.r-project.org/web/packages/skimr/index.html): Summarizes data in a frictionless way and produces a report with useful summary statistics.

-   [performance](https://cran.r-project.org/web/packages/performance/index.html): Assesses the quality and performance of statistical models, including checks for assumptions.

-   [plotly](https://plotly.com/r/): An R package that creates interactive web graphics using the plotly.js library.

-   [httr](https://cran.r-project.org/web/packages/httr/index.html): Simplifies the process of working with HTTP requests, such as API calls.

-   [gifski](https://cran.r-project.org/web/packages/gifski/index.html): Converts images, plots, or animations into high-quality GIFs using the gifski library.
:::

## Data Import and Minor Wrangling

This section will import the required aspatial and geospatial dataset. The process also **involves minor data change** like **setting the correct reference system**, and **removing duplicates** before going to more complex data wrangling in the next section. 

### Geospatial

the following panel will show how each geospatial dataset is imported, modify the CRS Code to 3414 to standardize, check on the data in general, check for duplicates, and display how the data looks like in Singapore map.

::: {.callout-note collapse="true" title="What is CRS Code?"}
A Coordinate Reference System (CRS) Code is a standardized method for locating and describing positions on the Earth's surface. It helps define how geographic data is represented in maps and digital systems.
:::

::: {.callout-note collapse="true" title="Why 3414?"}
The CRS code 3414 specifically refers to the coordinate reference system used for geospatial data in Singapore. It's a unique identifier assigned to this specific system, allowing geographers and mapping software to accurately interpret and display location-based information for Singapore.
:::

::: panel-tabset
#### MPSZ
the following code will import the masterplan subzone 2019 dataset and assign the correct coordinate reference.
```{r}
# Read spatial data for MPSZ-2019 and transform CRS
mpsz <- st_read(dsn='../data/geospatial',  # Specify data source directory
                layer='MPSZ-2019') %>%    # Specify layer to read
  st_transform(crs=3414)                  # Transform CRS to 3414

# View the structure and contents of the mpsz data frame
glimpse(mpsz)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package imports spatial vector data into R. It specifies the data source (`dsn`) and layer (`layer`). Here, it reads the 'MPSZ-2019' layer from the specified directory.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of spatial data. In this case, it transforms the CRS of `mpsz` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

next, check for possible duplicates using this code.
```{r}
# check for duplicates based on unique id
if_else(n_distinct(mpsz$SUBZONE_N) == nrow(mpsz), "no duplicates detected", "possible duplicates detected")
```
::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://dplyr.tidyverse.org/reference/n_distinct.html) from **dplyr** package calculates the number of unique values in a vector. In this code, it's used to count the unique values in `mpsz$SUBZONE_N`.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R returns the number of rows in a data frame. Here, it's used to get the total number of rows in `mpsz`.
-   [if_else](https://dplyr.tidyverse.org/reference/if_else.html) from **dplyr** package performs a vectorized conditional operation. In this context, it checks if the number of unique `SUBZONE_N` values equals the total number of rows in `mpsz`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.
```{r}
# set tmap mode (plot for lighter rendering, view for analysis)
tmap_mode('plot')

# display the data in a map
tm_shape(mpsz)+
  tm_polygons(alpha = 0.3) +
  tm_layout(main.title = 'Singapore Planning Zone', main.title.position = "center")
```
::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_mode) from **tmap** package sets the mode for creating maps. The mode `'plot'` is chosen here for static map plotting, which is typically lighter and faster for rendering compared to the interactive `'view'` mode.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data (`mpsz` in this case) for plotting.
-   [tm_polygons](https://rdrr.io/cran/tmap/man/tm_polygons.html) from **tmap** package adds a layer of polygons to the map, with an alpha parameter to adjust the transparency of these polygons.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) from **tmap** package customizes the layout of the map, including the title and its position.
-   This code snippet creates a thematic map displaying the Singapore Planning Zone with polygonal areas, each represented with a certain level of transparency.
:::

#### Bus Stop Location
the following code will import the bus stop location dataset and assign the correct coordinate reference.
```{r}
busstop <- st_read(dsn = "../data/geospatial",
                   layer = "BusStop") %>%
  st_transform(crs = 3414)

# check the data
glimpse(busstop)
```


::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "BusStop" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `busstop` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

Next, we will check for duplicates using the following code.
*BUS_STOP_N is used as the basis for duplicate checking because it will be the reference in joining the data*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(busstop$BUS_STOP_N) == nrow(busstop), "no duplicates detected", "possible duplicates detected")
```

Since duplicates is found, we will try to check what are the duplicated value using the following code. 


::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "BusStop" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `busstop` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::


```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- busstop[duplicated(busstop$BUS_STOP_N) | duplicated(busstop$BUS_STOP_N, fromLast = TRUE), ] %>%
  arrange(BUS_STOP_N)

# show the number of duplicates
nrow(duplicates)

# Display the sorted rows with duplicate BUS_STOP_N
kable(head(duplicates, n = 32))
```
::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R identifies duplicate elements in a vector or rows in a data frame. Here, it's used to find duplicate values in the `BUS_STOP_N` column of the `busstop` data frame.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from **dplyr** package sorts a data frame by one or more columns. In this code, `arrange` is used to sort the `duplicates` data frame by `BUS_STOP_N`.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) returns the number of rows in a data frame. It's used to count the number of duplicate rows.
-   [kable](https://www.rdocumentation.org/packages/knitr/versions/1.28/topics/kable) from the **knitr** package creates a simple table from a data frame or matrix. This function is used to display the first 32 rows of the `duplicates` data frame in a markdown table format.
:::

Based on the table, we can see that the duplicates does comes from the same bus stop code. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 5161 to 5145.

```{r}
# Keep one row of the duplicates in the original dataset
busstop <- busstop[!duplicated(busstop$BUS_STOP_N) | duplicated(busstop$BUS_STOP_N, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(busstop)
```
::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `BUS_STOP_N` column of the `busstop` data frame to find duplicates.
-   The subset operation (`busstop[...]`) is used to keep only one row for each duplicate in `busstop`. The logical condition `!duplicated(busstop$BUS_STOP_N)` keeps the first occurrence of each duplicate, and `duplicated(busstop$BUS_STOP_N, fromLast = TRUE)` keeps the last occurrence.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the updated `busstop` data frame after removing duplicates.
:::

next, we try to visualize how does the bus stop distribution looks like in the map using the following code.
```{r}
# set tmap mode (plot for lighter rendering, view for analysis)
tmap_mode('plot')

# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.3)+
  tm_shape(busstop)+
  tm_dots() +
  tm_layout(main.title = 'Bus Stop Distribution Map', main.title.position = "center")
```
::: {.callout-note collapse="true" title="Functions"}
-   [tmap_mode](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tmap_mode) from **tmap** package sets the mode for creating maps. The mode `'plot'` is chosen for static map plotting, which is typically faster and lighter for rendering than the interactive `'view'` mode.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used twice in this code: first for the `mpsz` dataset and then for `busstop`.
-   [tm_polygons](https://rdrr.io/cran/tmap/man/tm_polygons.html) from **tmap** package adds a layer of polygons to the map, in this case, for the `mpsz` data.
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) from **tmap** package adds a layer of dots to the map, representing the `busstop` data.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) from **tmap** package customizes the layout of the map, including the main title and its position.
-   This code snippet creates a thematic map showing the distribution of bus stops (`busstop`) over a polygonal map of planning zones (`mpsz`).
:::

#### Train Station
the following code will import the train station dataset and assign the correct coordinate reference.
```{r}
station <- st_read(dsn = '../data/geospatial',
                        layer = 'RapidTransitSystemStation') %>%
  st_transform(crs = 3414)

# the data contain non closed ring, use st_is_valid to fix
station <- station %>%
  filter(st_is_valid(.))
         
# check the data
glimpse(station)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_read) from the **sf** package is used to read spatial data from a file, database, or web service. In this code, it's used to read the 'RapidTransitSystemStation' layer from the geospatial data located at '../data/geospatial'.
-   [st_transform](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_transform) from the **sf** package is used to transform or convert coordinates of simple feature. Here, it's used to transform the coordinates of the `station` data to the coordinate reference system (CRS) with the EPSG code 3414.
-   [st_is_valid](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_is_valid) from the **sf** package checks if the geometry, in this case `station`, is valid. The `filter` function from the **dplyr** package is then used to keep only the valid geometries in `station`.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `station` data frame after the transformations.
:::

next, check for possible duplicates using this code.
*the duplicate consider geometry as the unique value as different MRT station should have different location*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(station$geometry) == nrow(station), "no duplicates detected", "possible duplicates detected")
```
since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.
```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(station) +
  tm_fill(col = 'green',
          id = 'STN_NAM_DE') +
  tm_layout(main.title = 'MRT Station Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_shape) from the **tmap** package is used to specify the spatial object that you want to visualize. In this code, it's used twice to specify two different spatial objects: `mpsz` and `station`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_polygons) from the **tmap** package is used to create a layer of polygons. Here, it's used to create a layer of polygons from `mpsz` with white color and 0.01 alpha (transparency).
-   [tm_fill](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_fill) from the **tmap** package is used to fill the polygons of `station` with green color. The `id` argument is set to 'STN_NAM_DE', which means the polygons are grouped by the 'STN_NAM_DE' attribute of `station`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_layout) from the **tmap** package is used to set the layout of the map. Here, it's used to set the main title of the map to 'MRT Station Distribution Map' and position it in the center.
:::

#### Train Exit
the following code will import the train exit location dataset and assign the correct coordinate reference.
```{r}
trainexit <- st_read(dsn = '../data/geospatial',
                     layer = 'Train_Station_Exit_Layer') %>%
  st_transform(crs = 3414)

glimpse(trainexit)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_read) from the **sf** package is used to read spatial data from a file, database, or web service. In this code, it's used to read the 'Train_Station_Exit_Layer' layer from the geospatial data located at '../data/geospatial'.
-   [st_transform](https://www.rdocumentation.org/packages/sf/versions/0.9-8/topics/st_transform) from the **sf** package is used to transform or convert coordinates of simple feature. Here, it's used to transform the coordinates of the `trainexit` data to the coordinate reference system (CRS) with the EPSG code 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `trainexit` data frame after the transformations.
:::

next, check for possible duplicates using this code.
*the duplicate consider geometry as the unique value as different MRT exit should have different location*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(trainexit$geometry) == nrow(trainexit), "no duplicates detected", "possible duplicates detected")
```
::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/n_distinct) from the **dplyr** package is used to count the number of unique values in a vector. In this code, it's used to count the number of unique geometries in the `trainexit` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R is used to get the number of rows in a data frame. Here, it's used to get the number of rows in the `trainexit` data frame.
-   [if_else](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/if_else) from the **dplyr** package is a vectorized conditional function that returns a value based on whether a condition is true or false. In this code, it's used to check if the number of unique geometries is equal to the number of rows in `trainexit`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since no duplicate is found, we can immediately try to visualize how does the master plan looks like in the map using the following code.
```{r}
# visualize the data
tm_shape(mpsz)+
  tm_polygons(alpha = 0.5)+
  tm_shape(trainexit)+
  tm_dots(col = 'blue',
          id = 'exit_id') +
  tm_layout(main.title = 'Train Station Exit Distribution Map', main.title.position = "center")
```


#### Business
the following code will import the business point of interest dataset and assign the correct coordinate reference.
```{r}
biz <- st_read(dsn = "../data/geospatial",
                   layer = "Business") %>%
  st_transform(crs = 3414)

glimpse(biz)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_read](https://r-spatial.github.io/sf/reference/st_read.html) from **sf** package is used for reading spatial vector data into R. It imports data from a specified data source (`dsn`) and layer (`layer`). In this code, it reads the "Business" layer from the data source located at `"../data/geospatial"`.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of a spatial object. Here, it transforms the CRS of `biz` to 3414.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, offering a quick look at its structure, including the types of columns and the first few entries in each column.
:::

next, check for possible duplicates using this code.
*considering some business might have branches, the duplicate condition here is only if all column value is the same*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(biz) == nrow(biz), "no duplicates detected", "possible duplicates detected")
```
::: {.callout-note collapse="true" title="Functions"}
-   [n_distinct](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/n_distinct) from the **dplyr** package is used to count the number of unique values in a vector. In this code, it's used to count the number of unique geometries in the `trainexit` data frame.
-   [nrow](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/nrow) from **base** R is used to get the number of rows in a data frame. Here, it's used to get the number of rows in the `trainexit` data frame.
-   [if_else](https://www.rdocumentation.org/packages/dplyr/versions/0.8.5/topics/if_else) from the **dplyr** package is a vectorized conditional function that returns a value based on whether a condition is true or false. In this code, it's used to check if the number of unique geometries is equal to the number of rows in `trainexit`. If they are equal, it returns "no duplicates detected"; otherwise, it returns "possible duplicates detected".
:::

since duplicates is found, we will try to check what are the duplicated value using the following code. 
```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- biz[duplicated(biz) | duplicated(biz, fromLast = TRUE), ] %>%
  arrange(POI_NAME)

# Display the sorted rows with duplicate BUS_STOP_N
head(duplicates)
```

::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `biz` data frame to find duplicates. The logical condition `duplicated(biz) | duplicated(biz, fromLast = TRUE)` keeps both the first and last occurrence of each duplicate.
-   The subset operation (`biz[...]`) is used to keep only the rows for each duplicate in `biz`.
-   [arrange](https://dplyr.tidyverse.org/reference/arrange.html) from the **dplyr** package is used to arrange rows by column values. Here, it's used to arrange the rows of `duplicates` by 'POI_NAME'.
-   [head](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/head) from **base** R is used to return the first parts of a vector, matrix, table, data frame or function. In this code, it's used to display the first few rows of the `duplicates` data frame after removing duplicates and arranging by 'POI_NAME'.
:::

Based on the table, we can see that the duplicates does comes from the same business point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 6550 to 6549

```{r}
# Keep one row of the duplicates in the original dataset
biz <- biz[!duplicated(biz) | duplicated(biz, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(biz)
```
::: {.callout-note collapse="true" title="Functions"}
-   [duplicated](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/duplicated) from **base** R is used to identify duplicate elements in a vector or rows in a data frame. In this code, it's applied to the `biz` data frame to find duplicates. The logical condition `!duplicated(biz) | duplicated(biz, fromLast = TRUE)` keeps both the first and last occurrence of each duplicate.
-   The subset operation (`biz[...]`) is used to keep only one row for each duplicate in `biz`.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from the **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column. It's used to display the structure of the `biz` data frame after removing duplicates.
:::


```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(biz)+
  tm_dots(col = 'red') +
  tm_layout(main.title = 'Business Distribution Map', main.title.position = "center")
```

::: {.callout-note collapse="true" title="Functions"}
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_shape) from the **tmap** package is used to specify the spatial object that you want to visualize. In this code, it's used twice to specify two different spatial objects: `mpsz` and `trainexit`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_polygons) from the **tmap** package is used to create a layer of polygons. Here, it's used to create a layer of polygons from `mpsz` with 0.5 alpha (transparency).
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_dots) from the **tmap** package is used to create a layer of dots. Here, it's used to create a layer of dots from `trainexit` with blue color. The `id` argument is set to 'exit_id', which means the dots are grouped by the 'exit_id' attribute of `trainexit`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/3.3-1/topics/tm_layout) from the **tmap** package is used to set the layout of the map. Here, it's used to set the main title of the map to 'Train Station Exit Distribution Map' and position it in the center.
:::

#### Entertainment
the following code will import the entertainment point of interest dataset and assign the correct coordinate reference.
```{r}
entertn <- st_read(dsn = '../data/geospatial',
                     layer = 'entertn') %>%
  st_transform(crs = 3414)

glimpse(entertn)
```
next, check for possible duplicates using this code.
*considering some entertainment might have branches, the duplicate condition here is only if all column value is the same*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(entertn) == nrow(entertn), "no duplicates detected", "possible duplicates detected")
```
since duplicates is found, we will try to check what are the duplicated value using the following code. 
```{r}
# Subset rows where BUS_STOP_N has duplicates and arrange by BUS_STOP_N
duplicates <- entertn[duplicated(entertn) | duplicated(entertn, fromLast = TRUE), ] %>%
  arrange(POI_NAME)

# Display the sorted rows with duplicateW
head(duplicates)
```
Based on the table, we can see that the duplicates does comes from the same entertainment point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 114 to 113
```{r}
# Keep one row of the duplicates in the original dataset
entertn <- entertn[!duplicated(entertn) | duplicated(entertn, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(entertn)
```
next, we try to visualize how does the entertainment point of interest distribution looks like in the map using the following code.
```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(entertn)+
  tm_dots(col = 'cyan',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Entertainment Distribution Map', main.title.position = "center")
```

#### Food & Beverage
the following code will import the F&B point of interest dataset and assign the correct coordinate reference.
```{r}
fnb <- st_read(dsn = '../data/geospatial',
                     layer = 'F&B') %>%
  st_transform(crs = 3414)

glimpse(fnb)
```
next, check for possible duplicates using this code.
*considering some F & B might have branches, the duplicate condition here is only if all column value is the same*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(fnb) == nrow(fnb), "no duplicates detected", "possible duplicates detected")
```
since no duplicate is found, we can immediately try to visualize how does the F&B distribution looks like in the map using the following code.
```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(fnb)+
  tm_dots(col = 'magenta',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Food & Beverage Distribution Map', main.title.position = "center")
```

#### Financial Services
the following code will import the financial services point of interest dataset and assign the correct coordinate reference.
```{r}
finance <- st_read(dsn = '../data/geospatial',
                   layer = 'FinServ') %>%
  st_transform(crs = 3414)

glimpse(finance)
```
next, check for possible duplicates using this code.
*considering some financial services might have branches, the duplicate condition here is only if all column value is the same*

```{r}
# check for duplicates based on unique id
if_else(n_distinct(finance) == nrow(finance), "no duplicates detected", "possible duplicates detected")
```

since duplicates is found, we will try to check what are the duplicated value using the following code. 
```{r}
# Subset rows for duplicates and arrange by POI_NAME and POI_ST_NAM
duplicates <- finance[duplicated(finance) | duplicated(finance, fromLast = TRUE), ] %>%
  arrange(POI_NAME, POI_ST_NAM)

# Display the sorted rows with duplicatew
kable(head(duplicates, n=20))
```

Based on the table, we can see that the duplicates does comes from the same financial service point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 3320 to 3058
```{r}
# Keep one row of the duplicates in the original dataset
finance <- finance[!duplicated(finance) | duplicated(finance, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(finance)
```
next, we try to visualize how does the financial services point of interest distribution looks like in the map using the following code.
```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(finance)+
  tm_dots(col = 'black',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Financial Service Distribution Map', main.title.position = "center")
```

#### Leisure & Recreation
the following code will import the leisure & recreation point of interest dataset and assign the correct coordinate reference.
```{r}
lnr <- st_read(dsn = '../data/geospatial',
                   layer = 'Liesure&Recreation') %>%
  st_transform(crs = 3414)

glimpse(lnr)
```
next, check for possible duplicates using this code.
*considering some leisure & recreation might have branches, the duplicate condition here is only if all column value is the same*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(lnr) == nrow(lnr), "no duplicates detected", "possible duplicates detected")
```
since no duplicate is found, we can immediately try to visualize how does the leisure & entertainment point of interest distribution looks like in the map using the following code.
```{r}
# visualize the output
tm_shape(mpsz)+
  tm_polygons(alpha = 0.01)+
  tm_shape(lnr)+
  tm_dots(col = 'coral',
          id = 'POI_NAME') +
  tm_layout(main.title = 'Leisure & Recreation Distribution Map', main.title.position = "center")
```

#### Retails
the following code will import the retails point of interest dataset and assign the correct coordinate reference.
```{r}
retails <- st_read(dsn = '../data/geospatial',
                   layer = 'Retails') %>%
  st_transform(crs = 3414)

glimpse(retails)
```

next, check for possible duplicates using this code.
*considering some financial services might have branches, the duplicate condition here is only if all column value is the same*
```{r}
# check for duplicates based on unique id
if_else(n_distinct(retails) == nrow(retails), "no duplicates detected", "possible duplicates detected")
```
since duplicates is found, we will try to check what are the duplicated value using the following code. 
```{r}
# Subset rows for duplicates and arrange by POI_NAME and POI_ST_NAM
duplicates <- retails[duplicated(retails) | duplicated(retails, fromLast = TRUE), ] %>%
  arrange(POI_NAME, POI_ST_NAM)

# Display the sorted rows with duplicatew
kable(head(duplicates, n=20))
```

Based on the table, we can see that the duplicates does comes from the same retail point of interest. Therefore, the following code chunk will execute the duplicate removal and show the result where number of rows have reduced from 37635 to 37463
```{r}
# Keep one row of the duplicates in the original dataset
retails <- retails[!duplicated(retails) | duplicated(retails, fromLast = TRUE), ]

# Display the resulting dataset
glimpse(retails)
```
next, we try to visualize how does the retail point of interest distribution looks like in the map using the following code.
```{r}
# visualize the output
tm_shape(mpsz) +
  tm_polygons(alpha = 0.01) +
  tm_shape(retails) +
  tm_dots(col = 'lightcoral', id = 'POI_NAME') +
  tm_layout(main.title="Retail Distribution Map", main.title.position = "center")
```

:::

### Aspatial

This subsection will import the aspatial data used in this project and check it. The data includes Passenger Volume by Origin Destination Bus Stops, HDB data which explanations can be found in [Overview] section.

#### Passenger Volume
Firstly, the following code will import the Passenger Volume by Origin Destination Bus Stops dataset. At the same time, it will also set the reference bus stop code data type to `factor` for easing compatibility issue and more efficient processing. As previously mentioned, this project will focus on weekday morning peak, so the code will also filter the data by that criteria, grouping it using the reference column, while summing the total trip for each unique grouped reference value. Finally, the code will generate output of summary statistics of the resulting dataset.

```{r}
# Load csv file
odb10 <- read_csv("../data/aspatial/origin_destination_bus_202310.csv.gz")

# change georeference data type into factors
odb10 <- odb10 %>%
  mutate(
    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)
  )

# filter and group the data
odb10 <- odb10 %>%
  filter(DAY_TYPE == "WEEKDAY", 
         TIME_PER_HOUR >= 6 & TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS))

# check the summary statistics of resulting dataframe
describe(odb10)
```

::: {.callout-note collapse="true" title="Functions"}
-   [read_csv](https://readr.tidyverse.org/reference/read_delim.html) from **readr** package reads a CSV file into R, converting it to a data frame.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new variables to a data frame or modify existing ones. Here, it converts `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE` to factors.
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to subset rows based on specified conditions. In this code, it filters data for weekdays during morning peak hours (6 to 9 AM).
-   [group_by](https://dplyr.tidyverse.org/reference/group_by.html) from **dplyr** package groups the data by specified columns, here by `ORIGIN_PT_CODE` and `DESTINATION_PT_CODE`.
-   [summarise](https://dplyr.tidyverse.org/reference/summarise.html) from **dplyr** package calculates summary statistics for each group, in this case summing up `TOTAL_TRIPS`.
-   [describe](https://www.rdocumentation.org/packages/Hmisc/versions/4.4-0/topics/describe) from **Hmisc** package provides a detailed summary of an object's contents, typically offering statistics like mean, standard deviation, frequency, and others.
:::

::: {.callout-note collapse="true" title="How to Read the Output?"}
The data provides details about the origin and destination points, along with the corresponding number of total trips, time information, and categorization based on day types. The variable summaries indicate the data distribution, with details such as the unique values, frequency, and descriptive statistics for each column.
:::

#### HDB

the following code will import the HDB dataset and check the data frame.
```{r}
# Load csv file
hdb <- read_csv("../data/aspatial/hdb.csv")

# check the data
glimpse(hdb)
```

To standardize the geospatial reference, we need to set the CRS code to 3414 based on the latitude and langitude values. The following code will execute it. Simultaneously, the code will also visualize the distribution of HDB across Singapore. 

```{r}
hdb_sf <- st_as_sf(hdb,
                   coords = c("lng", "lat"),
                   crs = 4326) %>%
  st_transform(crs = 3414)

# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(hdb_sf) +
  tm_dots(col = 'lightblue',
          id = 'building') +
  tm_layout(main.title = 'HDB Distribution Map', main.title.position = "center")
```

#### Schools

Geocoding involves converting an address or postal code into geographic coordinates like latitude and longitude. The [OneMap API](https://www.onemap.gov.sg/apidocs/), particularly the [Search](https://www.onemap.gov.sg/apidocs/apidocs) API by the Singapore Land Authority, facilitates this process, retrieving latitude, longitude, and x,y coordinates based on a given address or postal code.

In R, the provided code utilizes the *read_csv* function from the **readr** package to read input data from a CSV file. The geocoding is executed using the [SLA OneMap API](https://www.onemap.gov.sg/docs/#onemap-rest-apis). The **httr** package's HTTP call functions then send individual records to the OneMap geocoding server.

The geocoding operation results in two data frames: `found` for successfully geocoded records and `not_found` for unsuccessful ones. The `found` data table is merged with the initial CSV data table using a unique identifier (POSTAL) and saved as a new CSV file named `found`.

```{r}
url <- "https://www.onemap.gov.sg/api/common/elastic/search"

csv <- read_csv("../data/aspatial/Generalinformationofschools.csv")
postcodes <- csv$postal_code

found <- data.frame()
not_found <- data.frame()

for(postcode in postcodes){
  query <-list('searchVal' = postcode, 'returnGeom'='Y', 'getAddrDetails'='Y', 'pageNum' = '1')
  res  <- GET(url, query=query)
  
  if((content(res)$found)!=0)
    found<-rbind(found, data.frame(content(res))[4:13])
  else {
  not_found = data.frame(postcode)
  }
}

glimpse(found)
```

From previous in-class exercise, we already know that the not found data consist of Zhenghua Secondary School. Therefore, the following code will merge the data from previous process, then manually input the latitude and longitude data of the school.
```{r}
schools = merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE)

# manually add the Zhenghua Secondary School data
schools[schools$school_name == "ZHENGHUA SECONDARY SCHOOL", "results.LATITUDE"] <- 1.3887
schools[schools$school_name == "ZHENGHUA SECONDARY SCHOOL", "results.LONGITUDE"] <- 103.7652

# check the output
glimpse(schools)
```

To standardize the geospatial reference, we need to set the CRS code to 3414 based on the latitude and langitude values. The following code will execute it. Simultaneously, the code will also visualize the distribution of Schools across Singapore. 
```{r}
schools_sf <- schools %>%
  rename(
    latitude = "results.LATITUDE",
    longitude = "results.LONGITUDE"
  ) %>%
  select(
    postal_code, 
    school_name, 
    latitude, 
    longitude
  ) %>%
  st_as_sf(
    coords = c("longitude", "latitude"),
    crs=4326
  ) %>%
  st_transform(
    crs = 3414
  )

# visualize the output
tm_shape(mpsz)+
  tm_polygons(col = 'white', alpha = 0.01) +
  tm_shape(schools_sf) +
  tm_dots(col = 'lightgreen',
          id = 'building') +
  tm_layout(main.title = 'Schools Distribution Map', main.title.position = "center")
```
::: {.callout-note collapse="true" title="Functions"}
-   [rename](https://dplyr.tidyverse.org/reference/rename.html) from **dplyr** package changes the names of specific columns in a data frame for clarity. Here, it renames columns in `schools` to `latitude` and `longitude`.
-   [select](https://dplyr.tidyverse.org/reference/select.html) from **dplyr** package subsets specific columns in a data frame, retaining only `postal_code`, `school_name`, `latitude`, and `longitude`.
-   [st_as_sf](https://r-spatial.github.io/sf/reference/st_as_sf.html) from **sf** package converts a data frame to a simple features (sf) object, specifying the coordinates for spatial data.
-   [st_transform](https://r-spatial.github.io/sf/reference/st_transform.html) from **sf** package transforms the coordinate reference system (CRS) of the sf object.
-   [tm_shape](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_shape) from **tmap** package prepares spatial data for plotting. It's used twice: for `mpsz` and `schools_sf`.
-   [tm_polygons](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_polygons) adds a layer of polygons to the map, representing `mpsz`.
-   [tm_dots](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_dots) adds a layer of dots to the map, representing `schools_sf`.
-   [tm_layout](https://www.rdocumentation.org/packages/tmap/versions/2.2/topics/tm_layout) customizes the layout of the map, including the main title and its position.
-   The code snippet creates a thematic map showing the distribution of schools over a map with minimal polygon representation.
:::


# Data Wrangling
The following section will do various data wrangling from creating the hexagon layer, combining the data, duplicate checking, constructing Origin-Destination matrix, creating distance variable, and removing intra-zonal flows.

## Create Hexagon Layer
In this part, the following code will create honeycomb grid with hexagons with a distance of 375m from the center to the midpoint of the edge as the traffic analysis zone (TAZ). These hexagons will be the zones for the analysis. Simultaneously, the code will also intersect the grid with the bus stop data, and filter out grid that does not have any bus stop.
```{r}
# Create hexagonal grid based on bus stop locations
hexagonal_grid <- st_make_grid(busstop,
                               cellsize = 750,
                               what = "polygons",
                               square = FALSE)

# Convert hexagonal grid to spatial dataframe
hex_grid_sf <- st_sf(hex_grid = hexagonal_grid) %>%
  mutate(hexagon_id = 1:length(lengths(hexagonal_grid)))

# Count the number of bus stops within each hexagon
hex_grid_sf$num_bus_stops = lengths(st_intersects(hex_grid_sf, busstop))

# Filter hexagons with at least one bus stop
hexagons_with_bus_stops <- filter(hex_grid_sf, num_bus_stops > 0)

# check the output
glimpse(hexagons_with_bus_stops)
```

Next, we will visualize the distribution of bus stop across Singapore.
```{r}
# Plot the number of bus stops per hexagon with a color gradient and legend
tm_shape(hexagons_with_bus_stops) +
  tm_borders() +
  tm_fill("num_bus_stops",
          title = "Bus Stop Density",
          style = "jenks",
          palette = "YlOrRd",
          legend.show = TRUE) +
  tm_layout(main.title = 'Bus Stop Density in Hexagonal Grid',
            main.title.position = "center",
            legend.position = c("right", "top"))
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_make_grid](https://r-spatial.github.io/sf/reference/st_make_grid.html) from **sf** package is used to create a grid over a spatial object (`busstop`). Here, it generates a hexagonal grid with a specified cell size.
-   [st_sf](https://r-spatial.github.io/sf/reference/st_sf.html) from **sf** package converts the hexagonal grid into an `sf` (simple features) object.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new columns to a data frame or modify existing ones. Here, it's used to create a `hexagon_id` column.
-   [lengths](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lengths) from **base** R calculates the lengths of the elements in a list. It's used here to assign IDs to each hexagon.
-   [st_intersects](https://r-spatial.github.io/sf/reference/st_intersects.html) from **sf** package finds the intersection between two spatial objects, in this case, between each hexagon and the bus stops.
-   [filter](https://dplyr.tidyverse.org/reference/filter.html) from **dplyr** package is used to retain rows based on a specified condition. Here, it filters hexagons that contain at least one bus stop.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a quick overview of the data frame's structure, including column types and the first few entries in each column.
:::

## Combine Bus Stop and Hexagon
In this step, the following code will join the bus stop and hexagon data, and filter out only the required columns for following sections.
```{r}
# Perform a spatial join
busstop_with_hex_info <- st_join(busstop, hexagons_with_bus_stops, by = NULL, join = st_within)

# Select the relevant columns
busstop_with_hex_info <- busstop_with_hex_info %>%
  select(BUS_STOP_N, BUS_ROOF_N, LOC_DESC, hexagon_id, geometry) %>%
  mutate(
    BUS_STOP_N = as.factor(BUS_STOP_N)
  )

# Display the resulting data
glimpse(busstop_with_hex_info)
```
::: {.callout-note collapse="true" title="Functions"}
-   [st_join](https://r-spatial.github.io/sf/reference/st_join.html) from **sf** package performs a spatial join between two spatial objects. In this case, it joins `busstop` with `hexagons_with_bus_stops`. The `join = st_within` argument specifies that bus stops are joined with hexagons in which they are located.
-   [select](https://dplyr.tidyverse.org/reference/select.html) from **dplyr** package subsets specific columns from the joined spatial data frame. This code retains columns like `BUS_STOP_N`, `BUS_ROOF_N`, `LOC_DESC`, `hexagon_id`, and `geometry`.
-   [mutate](https://dplyr.tidyverse.org/reference/mutate.html) from **dplyr** package is used to add new variables or modify existing ones. Here, it converts `BUS_STOP_N` to a factor.
-   [glimpse](https://dplyr.tidyverse.org/reference/glimpse.html) from **dplyr** package provides a transposed summary of the data frame, giving a quick look at its structure, including the types of columns and the first few entries in each column.
:::

## Combine Trips and Hexagon
now, we will finally join the passenger trip data with the hex and bus stop data. considering that the data have origin and destination bus stop, we will also have hex for both.
```{r}
# Add origin_hex to odb10
odb_hex <- odb10 %>%
  left_join(busstop_with_hex_info %>% select(BUS_STOP_N, hexagon_id), 
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(origin_hex = hexagon_id)

# Add destination_hex to odb10
odb_hex <- odb_hex %>%
  left_join(busstop_with_hex_info %>% select(BUS_STOP_N, hexagon_id), 
            by = c("DESTINATION_PT_CODE" = "BUS_STOP_N")) %>%
  rename(destination_hex = hexagon_id)

# Remove additional geometry columns
odb_hex <- odb_hex %>%
  select(-contains("geometry."))

# Convert hex id to factor
odb_hex <- odb_hex %>%
  mutate(
    origin_hex = as.factor(origin_hex),
    destination_hex = as.factor(destination_hex)
  )

# check the output
glimpse(odb_hex)
```

## Final Duplicate Checking
Before going into the next step, the following code will firstly make sure that there are no duplicates in the dataset.
```{r}
if_else(n_distinct(odb_hex) == nrow(odb_hex), "no duplicates detected", "possible duplicates detected")
```

## Construct O-D Matrix
Next, we will build the O-D matrix which represent various which aggregate the trips on a one on one origin vs destination basis.
```{r}
# Remove rows with missing values
clean_odb <- odb_hex[complete.cases(odb_hex), ]

# Aggregate data based on origin_hex and destination_hex
odb_flow <- clean_odb %>%
  group_by(origin_hex, destination_hex) %>%
  summarise(trips = sum(TRIPS),
            origin_hex = unique(origin_hex),
            destination_hex = unique(destination_hex)) %>%
  ungroup()

# check the output
glimpse(odb_flow)
```

## Create Distance Variable

```{r}
# set busstop to spatial
busstop_sp <- as(busstop_with_hex_info, "Spatial")

# calculate the distance
tp_dist <- sp::spDists(busstop_sp, longlat = FALSE)

# add column names to the variable
hex_id_names <- busstop_sp$hexagon_id
colnames(tp_dist) <- paste0(hex_id_names)
rownames(tp_dist) <- paste0(hex_id_names)

# melt the table
dist <- reshape2::melt(tp_dist) %>%
  rename(origin_hex = Var1,
         destination_hex = Var2,
         distance = value) %>%
  mutate(origin_hex = as.factor(origin_hex),
         destination_hex = as.factor(destination_hex))

# remove duplicates
dist <- distinct(dist, origin_hex, destination_hex, .keep_all = TRUE)

# Perform left join to odb_flow with factor conversion
odb_flow <- odb_flow %>%
  left_join(
    dist %>% select(origin_hex, destination_hex, distance) %>%
      mutate(across(c(origin_hex, destination_hex), as.character)),
    by = c("origin_hex", "destination_hex")
  ) %>%
  mutate(across(c(origin_hex, destination_hex), as.factor))

# check the output
glimpse(odb_flow)
```

## Remove Intra-Zonal Flows

the following code chunk will remove intra-zonal flows

```{r}
odb_flow <- odb_flow %>%
  filter(as.character(origin_hex) != as.character(destination_hex))

# check the output
glimpse(odb_flow)
```

::: {.callout-note collapse="true" title="Why intra-zonal flows is removed?"}
The exclusion of intra-zonal flows from the Spatial Interaction Model calibration process is likely driven by the desire to focus on modeling and understanding interactions between different zones, without interference from flows within the same zone.

Intra-zonal flows, representing movements within a single zone, may not contribute significantly to the overall spatial interaction patterns that the model aims to capture. By removing intra-zonal flows, the analysis can concentrate on the dynamics and factors influencing interactions between distinct zones, providing a clearer and more interpretable representation of spatial relationships.

Including intra-zonal flows in the model might introduce unnecessary complexity or noise, potentially affecting the accuracy and interpretability of the model's results. Therefore, excluding intra-zonal flows is a methodological choice to streamline the analysis and enhance the model's ability to capture meaningful inter-zonal interactions.
:::

# Visualize the Desire Lines

firstly, create the line

There are important arguments in od2line:

flow: A data frame representing the origin-destination data. The first column should be the same as the first column of the data in the zones dataframe.

zones: The origin and destination of the travels should be passed here.

zone_code: Name of the variable in zones containing the ids of the zones.

```{r}
flowline <- od2line(flow = odb_flow, 
        zones = hexagons_with_bus_stops,
        zone_code = 'hexagon_id')

glimpse(flowline)
```

## 3rd Quantile Trips (175)

```{r}
summary(flowline$trips)
```

since trips without filter will be too messy, first trial will use the third quantile

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(trips >= 175) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7),
           alpha = 0.2)+
  tm_layout(main.title = 'Weekday Morning Peak Flow for 3rd Quantile Trips', main.title.position = 'center')
```

busier in central and east

## 99th Quantile Trips (6082)

```{r}
quantile(flowline$trips, probs = seq(0, 1, 0.01), na.rm = TRUE)
```

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(trips >= 6082) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7))+
  tm_layout(main.title = 'Weekday Morning Peak Flow for 99th Percentile Trips', main.title.position = 'center',
            main.title.size = 1.2)
```

north-downtown, west-downton, north-east, and short trip becomes more visible

## 3rd Quantile Distance (8923)

```{r}
summary(flowline$distance)
```

since trips without filter will be too messy, first trial will use the third quantile

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(distance >= 8923) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7),
           alpha = 0.2)+
  tm_layout(main.title = 'Weekday Morning Peak Flowline for 3rd Quantile Distance', main.title.position = 'center',
            main.title.size = 1.2)
```

too messy

## 99th Quantile Distance (18587)

```{r}
quantile(flowline$distance, probs = seq(0, 1, 0.01), na.rm = TRUE)
```

```{r}
tmap_mode('plot')
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz) +
  tm_polygons(col = 'white') +
tm_shape(hexagons_with_bus_stops) +
  tm_polygons(alpha = 0.4,
              col = 'gray') +
flowline %>%
  filter(distance >= 18587) %>%
  tm_shape() +
  tm_lines(col = 'coral',
           lwd = "trips",
           scale = c(0.1, 1, 2, 3, 4, 5, 6, 7))+
  tm_layout(main.title = 'Weekday Morning Peak Flowline for 99th Percentile Distance', main.title.position = 'center',
            main.title.size = 1.2)
```

more dense from all directions to downtown, and also slightly dense between north and east

## Spread Across Decile of Trips

```{r}
quantile(flowline$trips, probs = seq(0, 1, 0.1), na.rm = TRUE)
```

```{r}
flowline <- flowline %>%
  mutate(
    decile = cut(trips, 
                         breaks = c(0, 2, 5, 11, 21, 38, 67, 125, 254, 661, 5000, Inf),
                         labels = c("<2", "2-5", "5-11", "11-21",
                                    "21-38", "38-67", "67-125", "125-254",
                                    "254-661", "661-5000", ">5000"),
                         ordered_result=TRUE
))

# check the output
glimpse(flowline)
```

```{r}
#| eval: false
trips_spread <- tm_shape(mpsz) +
  tm_fill(col = "white") +
  tm_borders(col = "black", lwd = .5) +
tm_shape(flowline) +
  tm_lines(lwd = "trips", scale = 1.5, col = "coral", alpha = .7) +
  tm_layout(title = "Weekday Morning Peak Traffic Flow", title.position = c("right", "top")) +
  tm_facets(along = "decile", free.coords = FALSE)

# Save animation as gif
tmap_animation(trips_spread, "../images/trips_spread.gif", loop = TRUE, delay = 300,
               outer.margins = NA, restart.delay=500)
```

![](../images/trips_spread.gif) for the first 8 decile, the trips slightly change but still more dense on central. on the extra category of \>5000, north-downtown, west-downtown, north-east, and short trip becomes more visible

# Assembling Variables

## Push Factors

HDB, station, trainexit, proxy_pop, busstop

station and trainexit that can function as transit are special case where they might be a push and pull factor. people might come from train station to take a bus or take a bus to go to a train station.

```{r}
factors_holder <- hexagons_with_bus_stops %>%
  rename(push_num_bus_stops = num_bus_stops)

# define function to add push poi counts columns
add_push_poi_counts <- function(factors_holder, poi_datasets) {
  # Loop through each POI dataset
  for (poi_name in poi_datasets) {
    # Add a new column with the count using st_intersects and lengths
    factors_holder[[paste0("push_", poi_name, "_count")]] <- 
      ifelse(
        lengths(st_intersects(factors_holder, get(poi_name))) == 0,
        0.99,
        lengths(st_intersects(factors_holder, get(poi_name)))
      )
  }
  
  # Return the updated factors_holder dataframe
  return(factors_holder)
}

# List of POI dataset names
push_poi_datasets <- c("station", "trainexit", "hdb_sf")

# Call the function
factors_holder <- add_push_poi_counts(factors_holder, push_poi_datasets)

# check the output
glimpse(factors_holder)
```

add approximate population of the location, based on estimated hdb capacity

```{r}
hdb_capacity <- hdb_sf %>%
  mutate(capacity = `1room_sold` *1 +
           `2room_sold` * 2+
           `3room_sold` * 3+
           `4room_sold` * 4+
           `5room_sold` * 5+
           exec_sold * 4+
           multigen_sold * 6+
           studio_apartment_sold *1+
           `1room_rental` * 1+
           `2room_rental` * 2+
           `3room_rental` * 3+
           other_room_rental * 2) %>%
  select(blk_no, geometry, capacity)

# get hexagon_id for hdb, then sum the capacity
hdb_capacity <- st_join(hdb_capacity, hexagons_with_bus_stops, by = NULL, join = st_within) %>%
  group_by(hexagon_id) %>%
  summarise(push_est_pop = sum(capacity)) %>%
  st_drop_geometry()

# Convert hexagon_id to character in both dataframes
factors_holder <- factors_holder %>% mutate(hexagon_id = as.character(hexagon_id))
hdb_capacity <- hdb_capacity %>% mutate(hexagon_id = as.character(hexagon_id))

# Left join factors_holder and hdb_capacity by hexagon_id
factors_holder <- left_join(factors_holder, hdb_capacity, by = "hexagon_id") %>%
  mutate(hexagon_id = as.factor(hexagon_id)) %>%  # Convert back to factor
  mutate_at(vars(contains("push_est_pop")), list(~coalesce(., 0.99)))  # Fill missing values with 0.99 

glimpse(factors_holder)
```

## Pull Factors

station, trainexit, biz, entertn, fnb, finance, lnr, retails, school

```{r}
# define function to add pull poi counts columns
add_pull_poi_counts <- function(factors_holder, poi_datasets) {
  # Loop through each POI dataset
  for (poi_name in poi_datasets) {
    # Add a new column with the count using st_intersects and lengths
    factors_holder[[paste0("pull_", poi_name, "_count")]] <- 
      ifelse(
        lengths(st_intersects(factors_holder, get(poi_name))) == 0,
        0.99,
        lengths(st_intersects(factors_holder, get(poi_name)))
      )
  }
  
  # Return the updated factors_holder dataframe
  return(factors_holder)
}

# List of POI dataset names
pull_poi_datasets <- c("station", "trainexit", "biz", "entertn", "fnb", "finance", "lnr", "retails", "schools_sf")

# Call the function
factors_holder <- add_pull_poi_counts(factors_holder, pull_poi_datasets)

glimpse(factors_holder)
```

## Final Dataset

```{r}
glimpse(odb_flow)
```

```{r}
final_df <- left_join(odb_flow, factors_holder %>% select(starts_with("push_"), hexagon_id), by = c("origin_hex" = "hexagon_id"))

final_df <- left_join(final_df, factors_holder %>% select(starts_with("pull_"), hexagon_id), by = c("destination_hex" = "hexagon_id"))

final_df <- final_df %>%
  select(-hex_grid.x, -hex_grid.y)

# Assuming "final_df" is your data frame
final_df <- final_df %>%
  mutate(push_est_pop = ifelse(push_est_pop == 0, 0.99, push_est_pop))

# Check the output
glimpse(final_df)
```

```{r}
#| eval: false
write_rds(final_df, "../data/rds/final_df_the2.rds")
```

# Spatial Interaction Model

first, re-import the data

```{r}
final_df <- read_rds("../data/rds/final_df_the2.rds")
glimpse(final_df)
```

GLM model does not have a built in R2 value, therefore we build a function to calculate the R2. 
```{r}
CalcRSquared <- function(observed, estimated){
  r <- cor(observed, estimated)
  R2 <- r^2
  R2
}
```

As explained in [Overview] section, the Spatial Interaction Model used here are various type of the Gravity Model.

## Correlation Analysis

Before running the model, its a good practice to check collinearity between the variables to prevent bumping into multicollinearity problem in the model.

```{r}
# Calculate correlation
correlation_matrix <- cor(final_df[,3:18])

# Generate correlation plot
corrplot.mixed(correlation_matrix,
               lower = "color",      # Use ellipse for lower part
               upper = "number",     # Display correlation numbers in the upper part
               tl.pos = "lt",        # Place variable names to the left and top
               tl.col = "black",     # Set variable names color to black
               tl.cex = 0.7,         # Set variable names font size
               number.cex = 0.4      # Set correlation numbers font size
               )
```

Upon examining the correlation matrix, using a conservative threshold of correlation value above 0.7, the highly correlated variables are as follows:

1.  all station count and train exit count for each push and pull respectively have a strong positive correlation, suggesting that areas with a higher number of stations also tend to have a higher count of train exits. This result is quite expected.

2.  push_hdb_sf_count and push_est_pop also show a strong positive correlation, which may imply that regions with a higher number of HDB (Housing Development Board) flats tend to have a larger estimated population, a logical relationship given that the data is derived from the same source. Nevertheless, this result might also be caused by many of the hexagonal grid actually do not have HDB in the vicinity.

3.  pull_finance_count and pull_train_exit_count.

4.  pull_fnb_count and pull_finance_count.

5.  pull_fnb_count and pull_lnr_count.

6.  pull_fnr_count and pull_retails_count

::: {.notebox .lightbulb data-latex="lightbulb"}
this finding will be considered as basis for eliminating some variables in the model. Nevertheless, for experimental and comparison purpose, models without variables elimination will also be run in the next section.
:::

## The 4 Models Without Elimination

::: panel-tabset

### Unconstrained
```{r}
uncSIM <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                log(push_station_count) +
                log(push_trainexit_count) +
                log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                log(pull_finance_count) +
                log(pull_lnr_count) +
                log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(uncSIM)
CalcRSquared(uncSIM$data$trips, uncSIM$fitted.values)
```
**Key Coefficients:**
The estimated coefficients reveal the impact of various predictor variables on the logarithm of trip counts. Notably, a positive coefficient (e.g., log(push_num_bus_stops)) suggests that an increase in the corresponding variable is associated with a higher number of trips, while a negative coefficient (e.g., log(push_station_count)) implies a negative influence. The large z-values and highly significant p-values (<2e-16) underscore the robustness and statistical significance of these associations.

- **Intercept:** 13.24 (p < 2e-16)
- **log(push_num_bus_stops):** 0.302 (p < 2e-16)
- **log(push_station_count):** -0.600 (p < 2e-16)
- **log(push_trainexit_count):** 0.389 (p < 2e-16)
- **log(push_hdb_sf_count):** 0.454 (p < 2e-16)
- **log(push_est_pop):** -0.051 (p < 2e-16)
- **log(pull_station_count):** -0.469 (p < 2e-16)
- **log(pull_trainexit_count):** 0.670 (p < 2e-16)
- **log(pull_biz_count):** 0.089 (p < 2e-16)
- **log(pull_entertn_count):** -0.054 (p < 2e-16)
- **log(pull_fnb_count):** -0.166 (p < 2e-16)
- **log(pull_finance_count):** 0.267 (p < 2e-16)
- **log(pull_lnr_count):** -0.228 (p < 2e-16)
- **log(pull_retails_count):** 0.039 (p < 2e-16)
- **log(pull_schools_sf_count):** 0.258 (p < 2e-16)
- **log(distance):** -1.180 (p < 2e-16)

**Model Fit:**
The coefficient of determination (R-squared) for the model is 0.188, indicating that approximately 18.8% of the variability in trip counts can be explained by the selected predictor variables. While this suggests a moderate level of explanatory power, it also highlights the existence of unaccounted factors influencing trip counts. The significance of the individual coefficients supports the model's ability to capture meaningful relationships within the dataset, despite the overall modest R-squared value.

### Origin Constrained
```{r}
orcSIM <- glm(formula = trips ~
                log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                log(pull_finance_count) +
                log(pull_lnr_count) +
                log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance) +
                origin_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(orcSIM)
CalcRSquared(orcSIM$data$trips, orcSIM$fitted.values)
```
**Key Coefficients:**
The estimated coefficients from the Poisson regression model provide insights into the relationship between predictor variables and the logarithm of trip counts. Each coefficient represents the change in the log count of trips associated with a one-unit change in the corresponding predictor, holding other variables constant. Highly significant p-values (< 2e-16) emphasize the robustness of these associations.

- **Intercept:** 13.44 (p < 2e-16)
- **log(pull_station_count):** -0.403 (p < 2e-16)
- **log(pull_trainexit_count):** 0.662 (p < 2e-16)
- **log(pull_biz_count):** 0.108 (p < 2e-16)
- **log(pull_entertn_count):** -0.089 (p < 2e-16)
- **log(pull_fnb_count):** -0.134 (p < 2e-16)
- **log(pull_finance_count):** 0.265 (p < 2e-16)
- **log(pull_lnr_count):** -0.166 (p < 2e-16)
- **log(pull_retails_count):** 0.060 (p < 2e-16)
- **log(pull_schools_sf_count):** 0.230 (p < 2e-16)
- **log(distance):** -1.239 (p < 2e-16)
- **origin_hex1003:** 2.486 (p < 2e-16)
- **origin_hex1004:** 3.300 (p < 2e-16)
- **origin_hex1011:** -0.784 (p < 2e-16)
- **origin_hex1012:** -0.841 (p < 2e-16)
- and various other dummy origin variables in which many are statistically significant.

**Model Fit:**
The coefficient of determination (R-squared) for the model is 0.283, indicating that approximately 28.3% of the variability in trip counts can be explained by the selected predictor variables. The significance of individual coefficients and the model's goodness-of-fit metrics, such as the residual deviance and AIC, support its effectiveness in capturing patterns within the dataset. The p-values associated with each coefficient highlight their statistical significance in influencing trip counts.

### Destination Constrained
```{r}
decSIM <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                log(push_station_count) +
                log(push_trainexit_count) +
                log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(distance) +
                destination_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(decSIM)
CalcRSquared(decSIM$data$trips, decSIM$fitted.values)
```
**Key Coefficients:**
The Poisson regression model provides estimates of the coefficients, shedding light on the relationship between predictor variables and the logarithm of trip counts. Significant p-values (< 2e-16) highlight the importance of each predictor in influencing trip counts.

- **Intercept:** 13.90 (p < 2e-16)
- **log(push_num_bus_stops):** 0.320 (p < 2e-16)
- **log(push_station_count):** -0.548 (p < 2e-16)
- **log(push_trainexit_count):** 0.455 (p < 2e-16)
- **log(push_hdb_sf_count):** 0.398 (p < 2e-16)
- **log(push_est_pop):** -0.019 (p < 2e-16)
- **log(distance):** -1.260 (p < 2e-16)
- **destination_hex1003:** 1.793 (p < 2e-16)
- **destination_hex1004:** 0.856 (p < 2e-16)
- **destination_hex1011:** -0.046 (p = 0.0165)
- **destination_hex1012:** 1.545 (p < 2e-16)
- **destination_hex1013:** 0.134 (p = 3.71e-15)
- **destination_hex1014:** -1.525 (p < 2e-16)
- **destination_hex1015:** 1.265 (p < 2e-16)
- **destination_hex1016:** 1.288 (p < 2e-16)
- **destination_hex1018:** -0.200 (p < 2e-16)
- **destination_hex1019:** 0.308 (p < 2e-16)
- **destination_hex1023:** -0.778 (p < 2e-16)
- **destination_hex1024:** 0.413 (p < 2e-16)
- **destination_hex1025:** -3.483 (p < 2e-16)
- **destination_hex1033:** 0.481 (p < 2e-16)
- and various other dummy destination variables in which many are statistically significant.

**Model Fit:**
The R-squared value for the model is approximately 0.367, indicating that around 36.7% of the variability in trip counts can be explained by the selected predictor variables. The model's goodness-of-fit metrics, including the residual deviance and AIC, support its efficacy in capturing patterns within the dataset. Each predictor's p-value underscores its significance in explaining the observed variation in trip counts.

### Doubly Constrained
```{r}
docSIM <- glm(formula = trips ~ 
                origin_hex +
                destination_hex +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(docSIM)
CalcRSquared(docSIM$data$trips, docSIM$fitted.values)
```
**Key Coefficients:**
This Poisson regression model explores the relationship between trip counts and predictor variables, including origin, destination, and the logarithm of distance. The estimated coefficients, accompanied by their p-values, reveal the significance of each predictor in influencing trip counts.

- **Intercept:** 14.34 (p < 2e-16)
- **origin_hex1003:** 2.59 (p < 2e-16)
- **origin_hex1004:** 3.41 (p < 2e-16)
- **origin_hex1011:** -1.06 (p < 2e-16)
- **origin_hex1012:** -0.91 (p < 2e-16)
- **origin_hex1013:** -0.27 (p < 2e-16)
- **origin_hex1014:** 0.36 (p < 2e-16)
- **origin_hex1015:** -0.44 (p < 2e-16)
- **origin_hex1016:** 1.34 (p < 2e-16)
- **origin_hex1018:** 1.91 (p < 2e-16)
- and various other dummy origin and destination variables in which many are statistically significant.

**Model Fit:**
The model's R-squared value is approximately 0.534, indicating that around 53.4% of the variability in trip counts is explained by the selected predictor variables. The goodness-of-fit metrics, including the residual deviance and AIC, support the model's effectiveness in capturing patterns within the dataset. The p-values for each predictor underscore their significance in explaining the observed variation in trip counts.
:::

::: {.notebox .lightbulb data-latex="lightbulb"}
Note that `push_est_pop` which represent proxy for population in the origin location counterintuitively have a negative coefficient. Nevertheless,it might simply be a symptomps of **multicollinearity**. As we can see, the `push_hdb_sf_count` which previously was identified to be highly correlated with `push_hdb_sf_count`, have positive coefficient. This in a way suggest that **the impact of the two offset each other to some extent, and the net effect most likely be positive**. We can see on the later whether this hunch is correct in the next section, where highly correlated variables are eliminated from the model. 
:::

## The 3 Models With Elimination

::: panel-tabset

### Unconstrained
```{r}
uncSIM2 <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                #log(push_station_count) +
                log(push_trainexit_count) +
                #log(push_hdb_sf_count) +
                log(push_est_pop) +
                #log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                #log(pull_finance_count) +
                #log(pull_lnr_count) +
                #log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance),
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(uncSIM2)
CalcRSquared(uncSIM2$data$trips, uncSIM2$fitted.values)
```
::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** R package is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution. In this code, it's applied to the `trips` as the response variable and a set of predictor variables in the `final_df` data frame.
-   The formula `trips ~ log(pull_trainexit_count) + log(pull_biz_count) + log(pull_entertn_count) + log(pull_fnb_count) + log(pull_schools_sf_count) + log(distance) + origin_hex` specifies that `trips` is modeled as a function of `pull_trainexit_count`, `pull_biz_count`, `pull_entertn_count`, `pull_fnb_count`, `pull_schools_sf_count`, `distance`, and `origin_hex`. The `log` function is applied to transform the predictor variables.
-   The argument `family = poisson(link = "log")` specifies that a Poisson regression model is fitted with a log link function.
-   The argument `na.action = na.exclude` specifies that any missing values should be excluded from the model fitting process.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R package is used to produce result summaries of the results of various model fitting functions. In this code, it's applied to the fitted model `orcSIM2` to obtain a summary of the model fit.
-   `CalcRSquared` is user-defined function , which is used to calculate the R-squared value of the fitted model. It takes the observed values `orcSIM2$data$trips` and the fitted values `orcSIM2$fitted.values` as inputs.
:::

**Key Coefficients:**
The Poisson regression model reveals crucial coefficients that help interpret the impact of predictor variables on the logarithm of trip counts. With highly significant p-values (< 2e-16), each coefficient contributes significantly to explaining the variability in trip counts.

- **Intercept:** 13.22 (p < 2e-16)
- **log(push_num_bus_stops):** 0.383 (p < 2e-16)
- **log(push_trainexit_count):** 0.164 (p < 2e-16)
- **log(push_est_pop):** 0.130 (p < 2e-16)
- **log(pull_trainexit_count):** 0.780 (p < 2e-16)
- **log(pull_biz_count):** 0.073 (p < 2e-16)
- **log(pull_entertn_count):** -0.240 (p < 2e-16)
- **log(pull_fnb_count):** -0.193 (p < 2e-16)
- **log(pull_schools_sf_count):** 0.303 (p < 2e-16)
- **log(distance):** -1.169 (p < 2e-16)

**Model Fit:**
The R-squared value for the model is approximately 0.131, suggesting that the selected predictor variables collectively explain around 13.1% of the variability in trip counts. The goodness-of-fit metrics, including the residual deviance and AIC, demonstrate the model's efficacy in capturing patterns within the data. Notably, all predictor variables exhibit highly significant p-values, emphasizing their importance in explaining the observed variation in trip counts.


### Origin Constrained
```{r}
orcSIM2 <- glm(formula = trips ~
                #log(pull_station_count) +
                log(pull_trainexit_count) +
                log(pull_biz_count) +
                log(pull_entertn_count) +
                log(pull_fnb_count) +
                #log(pull_finance_count) +
                #log(pull_lnr_count) +
                #log(pull_retails_count) +
                log(pull_schools_sf_count) +
                log(distance) +
                origin_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(orcSIM2)
CalcRSquared(orcSIM2$data$trips, orcSIM2$fitted.values)
```
::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** R package is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution. In this code, it's applied to the `trips` as the response variable and a set of predictor variables in the `final_df` data frame.
-   The formula `trips ~ log(push_num_bus_stops) + log(push_trainexit_count) + log(push_est_pop) + log(distance) + destination_hex` specifies that `trips` is modeled as a function of `push_num_bus_stops`, `push_trainexit_count`, `push_est_pop`, `distance`, and `destination_hex`. The `log` function is applied to transform the predictor variables.
-   The argument `family = poisson(link = "log")` specifies that a Poisson regression model is fitted with a log link function.
-   The argument `na.action = na.exclude` specifies that any missing values should be excluded from the model fitting process.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R package is used to produce result summaries of the results of various model fitting functions. In this code, it's applied to the fitted model `decSIM2` to obtain a summary of the model fit.
-   `CalcRSquared` is a user-defined function which is used to calculate the R-squared value of the fitted model. It takes the observed values `decSIM2$data$trips` and the fitted values `decSIM2$fitted.values` as inputs.
:::

**Key Coefficients:**
The Poisson regression model reveals insights into the relationship between predictor variables and the logarithm of trip counts. The following coefficients are statistically significant (p < 2e-16), indicating their importance in explaining variations in trip counts:

- **Intercept:** 13.56
- **log(pull_trainexit_count):** 0.831
- **log(pull_biz_count):** 0.0971
- **log(pull_entertn_count):** -0.1845
- **log(pull_fnb_count):** -0.102
- **log(pull_schools_sf_count):** 0.2595
- **log(distance):** -1.2396
- **origin_hex1003:** 2.632
- **origin_hex1004:** 3.491
- **origin_hex1011:** -0.5583
- **origin_hex1012:** -0.6222
- **origin_hex1013:** -0.0155
- **origin_hex1014:** 0.4643
- **origin_hex1015:** -0.1897
- and various other dummy origin variables in which many are statistically significant.

**Model Fit:**
The R-squared value for the model is approximately 0.250, indicating that around 25% of the variability in trip counts can be explained by the selected predictor variables. The model's goodness-of-fit metrics, including the residual deviance and AIC, support its efficacy in capturing patterns within the dataset. Each predictor's p-value underscores its significance in explaining the observed variation in trip counts. The non-significant p-value like **origin_hex1013** suggests that this variable may not be a strong predictor of trip counts in this context.

### Destination Constrained
```{r}
decSIM2 <- glm(formula = trips ~ 
                log(push_num_bus_stops) +
                #log(push_station_count) +
                log(push_trainexit_count) +
                #log(push_hdb_sf_count) +
                log(push_est_pop) +
                log(distance) +
                destination_hex,
              family = poisson(link = "log"),
              data = final_df,
              na.action = na.exclude)

summary(decSIM2)
CalcRSquared(decSIM2$data$trips, decSIM2$fitted.values)
```
::: {.callout-note collapse="true" title="Functions"}
-   [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm) from **stats** R package is used to fit generalized linear models, specified by giving a symbolic description of the linear predictor and a description of the error distribution. In this code, it's applied to the `trips` as the response variable and a set of predictor variables in the `final_df` data frame.
-   The formula `trips ~ log(push_num_bus_stops) + log(push_trainexit_count) + log(push_est_pop) + log(distance) + destination_hex` specifies that `trips` is modeled as a function of `push_num_bus_stops`, `push_trainexit_count`, `push_est_pop`, `distance`, and `destination_hex`. The `log` function is applied to transform the predictor variables.
-   The argument `family = poisson(link = "log")` specifies that a Poisson regression model is fitted with a log link function.
-   The argument `na.action = na.exclude` specifies that any missing values should be excluded from the model fitting process.
-   [summary](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary) from **base** R package is used to produce result summaries of the results of various model fitting functions. In this code, it's applied to the fitted model `decSIM2` to obtain a summary of the model fit.
-   `CalcRSquared` is a user-defined function which is used to calculate the R-squared value of the fitted model. It takes the observed values `decSIM2$data$trips` and the fitted values `decSIM2$fitted.values` as inputs.
:::

**Key Coefficients:**
The Poisson regression model reveals insightful coefficients, offering a glimpse into the relationships between predictor variables and the logarithm of trip counts. Each predictor's significance is denoted by the associated p-values, all indicating high statistical significance (< 2e-16).

- **Intercept:** 13.64 (p < 2e-16)
- **log(push_num_bus_stops):** 0.395 (p < 2e-16)
- **log(push_trainexit_count):** 0.291 (p < 2e-16)
- **log(push_est_pop):** 0.137 (p < 2e-16)
- **log(distance):** -1.254 (p < 2e-16)
- **destination_hex1003:** 1.820 (p < 2e-16)
- **destination_hex1004:** 0.917 (p < 2e-16)
- **destination_hex1011:** -0.072 (p = 0.000194)
- **destination_hex1012:** 1.443 (p < 2e-16)
- **destination_hex1013:** 0.059 (p = 0.000490)
- **destination_hex1014:** -1.564 (p < 2e-16)
- **destination_hex1015:** 1.247 (p < 2e-16)
- **destination_hex1016:** 1.292 (p < 2e-16)
- **destination_hex1018:** -0.196 (p < 2e-16)
- and various other dummy destination variables in which many are statistically significant.

**Model Fit:**
The model's R-squared value is approximately 0.346, indicating that around 34.6% of the variability in trip counts can be explained by the selected predictor variables. The goodness-of-fit metrics, such as the residual deviance and AIC, support the model's effectiveness in capturing patterns within the dataset. All predictor variables demonstrate statistical significance, emphasizing their roles in explaining the observed variation in trip counts.

:::

## Compare The Result
### Performance Table
```{r}
model_list <- list(
  Unconstrained = uncSIM,
  Unconstrained_with_Elimination = uncSIM2,
  Origin_Constrained = orcSIM,
  Origin_Constrained_with_Elimination = orcSIM2,
  Destination_Constrained = decSIM,
  Destination_Constrained_with_Elimination = decSIM2,
  Doubly_Constrained = docSIM
)

# Compare performance with multiple metrics
compare_performance(model_list, metrics = c("AIC", "BIC", "RMSE"))
```
::: {.callout-note collapse="true" title="Functions"}
-   The `list` function from **base** R is used to create a list named `model_list` that contains seven elements. Each element is a model simulation, and they are named as follows: 'Unconstrained', 'Unconstrained_with_Elimination', 'Origin_Constrained', 'Origin_Constrained_with_Elimination', 'Destination_Constrained', 'Destination_Constrained_with_Elimination', and 'Doubly_Constrained'. The values of these elements are the corresponding model simulations (`uncSIM`, `uncSIM2`, `orcSIM`, `orcSIM2`, `decSIM`, `decSIM2`, `docSIM`).
-   The `compare_performance` function is then used to compare the performance of the models in `model_list` using multiple metrics. The `metrics` argument is set to a vector containing "AIC", "BIC", and "RMSE". This means that the function will calculate the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and Root Mean Square Error (RMSE) for each model in `model_list`.
:::

In comparing the performance of various spatial interaction models, the model indices indicate that the Doubly Constrained model outperforms the others. It has the lowest values for the Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC), suggesting better model fit with lower complexity. Moreover, it boasts the smallest Root Mean Square Error (RMSE) of 1267.730, implying it has the highest predictive accuracy.

The Destination Constrained models follow, displaying improved performance over the Unconstrained models, as indicated by their lower AIC, BIC, and RMSE values. The addition of variable elimination to the Origin and Destination Constrained models shows slight increases in AIC and BIC values with corresponding minor increases in RMSE, which suggests that while the elimination process simplifies the models, it does so at the cost of some predictive performance.

Note that these results are inline with what the R2 suggest in previous finding.

::: {.callout-note collapse="true" title="What is AIC?"}
The **Akaike Information Criterion (AIC)** is a measure of the relative quality of a statistical model for a given set of data. It provides a means for model selection by comparing models and choosing the one that minimizes information loss.
:::

::: {.callout-note collapse="true" title="What is AIC Weights?"}
These weights are derived from the AIC values of each model and are used to compare each model's likelihood of being the best model among the set of models being considered. A lower AIC value indicates a better model fit, and the corresponding weight represents the model's probability relative to the others. A weight close to 1 suggests a high likelihood that it is the best model, while a weight close to 0 suggests a lower likelihood.
:::

::: {.callout-note collapse="true" title="What is BIC?"}
The **Bayesian Information Criterion (BIC)** is similar to AIC, but it introduces a penalty term for the number of parameters in the model to discourage overfitting.
:::

::: {.callout-note collapse="true" title="What is BIC Weights?"}
Similar to AIC weights, BIC weights are calculated from the BIC values, which include a penalty for the number of parameters in the model to avoid overfitting. The BIC is more stringent than the AIC, especially as the sample size increases. As with AIC, a BIC weight close to 1 indicates a stronger model relative to the others in terms of both fit and parsimony.
:::

::: {.callout-note collapse="true" title="What is RMSE?"}
The **Root Mean Square Error (RMSE)** is a frequently used measure of the differences between values predicted by a model and the values observed. It represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences.
:::

### Visualized Fitted Value
Next, we will visualize to compare the predicted value against the original value for each of the seven models that we trained previously using the following code chunk. 
```{r}
# Function to round fitted values and create a data frame
round_and_rename <- function(sim_data, sim_name) {
  as.data.frame(sim_data$fitted.values) %>%
    round(digits = 0) %>%
    setNames(paste0(sim_name, "_trips"))
}

# Round and rename fitted values for each simulation
uncSIM_fitted <- round_and_rename(orcSIM, "uncSIM")
orcSIM_fitted <- round_and_rename(orcSIM, "orcSIM")
decSIM_fitted <- round_and_rename(decSIM, "decSIM")
docSIM_fitted <- round_and_rename(docSIM, "docSIM")
uncSIM2_fitted <- round_and_rename(orcSIM2, "uncSIM2")
orcSIM2_fitted <- round_and_rename(orcSIM2, "orcSIM2")
decSIM2_fitted <- round_and_rename(decSIM2, "decSIM2")

# Combine the rounded and renamed fitted values
final_df_viz <- final_df %>%
  cbind(uncSIM_fitted, orcSIM_fitted, decSIM_fitted, docSIM_fitted, uncSIM2_fitted, orcSIM2_fitted, decSIM2_fitted)

# Create a function to generate a ggplot
generate_ggplot <- function(data, x_col, color, title) {
  ggplot(data = data, aes(x = !!sym(x_col), y = trips)) +
    geom_point(
      size = data$trips / 10000,
      alpha = .6,
      shape = 21  # Change point shape
    ) +
    xlim(0, 50000) +
    geom_smooth(
      method = lm,
      se = TRUE,
      color = "blue"  # Change smooth line color
    ) +
    labs(title = title) +
    theme(
      plot.title = element_text(size = 10),
      axis.text.x = element_blank(),
      axis.ticks.x = element_blank(),
      axis.text.y = element_blank(),
      axis.title.y = element_blank()
    )
}

# Generate ggplots for each simulation
p_unc <- generate_ggplot(final_df_viz, "uncSIM_trips", "black", "Unconstrained")
p_orc <- generate_ggplot(final_df_viz, "orcSIM_trips", "black", "Origin-constrained")
p_dec <- generate_ggplot(final_df_viz, "decSIM_trips", "black", "Destination-constrained")
p_doc <- generate_ggplot(final_df_viz, "docSIM_trips", "black", "Doubly-constrained")
p_unc2 <- generate_ggplot(final_df_viz, "uncSIM2_trips", "black", "Unconstrained with Elimination")
p_orc2 <- generate_ggplot(final_df_viz, "orcSIM2_trips", "black", "Origin-constrained with Elimination")
p_dec2 <- generate_ggplot(final_df_viz, "decSIM2_trips", "black", "Destination-constrained with Elimination")

# Combine the plots using patchwork
p_unc + p_unc2 + p_orc + p_orc2 + p_dec + p_dec2+ p_doc 
```
::: {.callout-note collapse="true" title="Functions"}
-   The `round_and_rename` function is a user-defined function that takes two arguments: `sim_data` and `sim_name`. It rounds the fitted values in `sim_data` to 0 decimal places, converts them to a data frame, and renames the column using `sim_name`. The new column name is the concatenation of `sim_name` and "_trips".
-   The `round_and_rename` function is then applied to each simulation data (`orcSIM`, `decSIM`, `docSIM`, `orcSIM2`, `decSIM2`) to round and rename the fitted values. The results are stored in new variables (`uncSIM_fitted`, `orcSIM_fitted`, `decSIM_fitted`, `docSIM_fitted`, `uncSIM2_fitted`, `orcSIM2_fitted`, `decSIM2_fitted`).
-   The `cbind` function from **base** R is used to combine the rounded and renamed fitted values into a new data frame `final_df_viz`.
-   The `generate_ggplot` function is another user-defined function that takes four arguments: `data`, `x_col`, `color`, and `title`. It generates a ggplot of `data` with `x_col` on the x-axis and `trips` on the y-axis. The points are sized according to `trips` and shaped as hollow circles (`shape = 21`). A linear model is fitted to the data (`method = lm`), and the smooth line color is set to blue. The title of the plot is set to `title`, and the text size, axis text, and axis title are customized using the `theme` function.
-   The `generate_ggplot` function is then applied to `final_df_viz` for each simulation to generate ggplots. The results are stored in new variables (`p_unc`, `p_orc`, `p_dec`, `p_doc`, `p_unc2`, `p_orc2`, `p_dec2`).
-   The `+` operator from the **patchwork** package is used to combine the plots into a single plot.
:::

Summary analysis of each model's fitted values compared to the original trips:

- **Unconstrained Model**: This model shows a broad dispersion of points, with many falling far from the line of best fit. This suggests the model may not be very accurate in predicting the original trips, as indicated by the spread of fitted values.

- **Unconstrained with Elimination**: A similar pattern to the unconstrained model, but with a slight shift in the points, indicating that variable elimination may not have significantly improved the model's predictive ability.

- **Origin-Constrained Model**: Points are more aligned with the trend line compared to the unconstrained model, suggesting a better fit for predicting the number of trips.

- **Origin-Constrained with Elimination**: The scatter is close to the trend line, much like the origin-constrained model, implying that variable elimination has little effect on improving this model's fit.

- **Destination-Constrained Model**: The concentration of points around the line of best fit is even tighter here, indicating an improved accuracy over the origin-constrained models.

- **Destination-Constrained with Elimination**: This shows a pattern similar to the destination-constrained model, with a close clustering of points around the trend line, suggesting a good model fit.

- **Doubly-Constrained Model**: This model exhibits the tightest clustering of points along the line, indicating the highest predictive accuracy and the best fit among all models presented.

The size of the points, representing the number of trips, varies across models, with the doubly-constrained model showing the most consistent scaling in relation to the trend line, suggesting this model's superior capability in capturing the trip distribution pattern in the data.

## Conclusion
In this study, various Spatial Interaction Models (SIMs) were constructed to understand and predict bus commuter flow during the Weekday Morning Peak in Singapore. The models included Unconstrained, Origin-Constrained, Destination-Constrained, and Doubly-Constrained versions, each incorporating multiple features such as bus stops, train stations, HDB units, and more. This project also tried to experiment with inclusion of highly correlated variables and without its inclusion.

Based on the goodness-of-fit and linearity were conducted, revealing consistent outperformance of the Doubly-Constrained SIM over Origin-Constrained, Destination-Constrained, and Unconstrained SIMs. Key observations and conclusions include:

- **Model Complexity:** The Doubly-Constrained SIM is consistently outperfoming the other models. Nevertheless, it might be ***simply be due to the complexity*** of the models since it contains thousands of classes from two categorical variables, which are the origin hex and the destination hex. In a way, this simply suggests that ***uniqueness of each origin and destination hex is the best predictor of the amount of trips***

- **Push and Pull Factor Significance:** All push and pull factor variables becomes significant explanatory variable influencing the number of trips during Weekday Morning Peak Periods. Those variables includes counts of bus stop, train station exits, business, schools, entertainment, food & beverages, and schools. This is ***inline with the gravity model theory***. Nevertheless, some of these variables unexpectedly have ***inconsistent coefficients*** across the models (it came as negative sometimes, which is counterintuitive). This indicates ***spurious regression*** and/or multicollinearity exists, even among those variables that were observed to be not highly correlated. 

- **Distance Significance:** Distance between zones emerged as the crucial explanatory variable influencing the number of trips during Weekday Morning Peak Periods. Shorter distances correlated with higher trip numbers, highlighting the dominant role of distance in commuter behavior. This is also ***inline with the gravity model theory***.

- **Push and Pull Factor Significance:** Distance between zones emerged as significant explanatory variable influencing the number of trips during Weekday Morning Peak Periods. Shorter distances correlated with higher trip numbers, highlighting the dominant role of distance in commuter behavior. This is ***inline with the gravity model theory***.

Despite the relatively good predictive performance of the best model (>50% explained variance), the better result of Doubly-Constrained SIM also might simply suggests that ***uniqueness of each origin and destination hex is the best predictor of the amount of trips***. This implied various things for future research, such as:
- The current best model (doubly-constrained) might have ***reduced performance in the future if the condition in some hexagon grid changed***, as it relies on it as the variable. Therefore, future modelling should consider ***relying more on the push and pull factors variables***.

- The current push and pull factors variables are ***currently not good enough to represent the uniqueness*** that each hexagon grid has in determining trips. This might be due to various factors such as ***point of interest that is not yet identified***, and each ***point of interest might also have different characteristic that affect the magnitude*** of it as a push or pull factors. For example, business and schools might have different capacities which in turn translated into different amount of commuters. Another important factor that might be improved is the estimated population of each hexagon. If the estimated population can be more accurate, it might yield better result.

- Lastly, future project can also consider to use ***Spatial Econometric Interaction Models*** where factors can be given spatial weight. This might in turn be better to represent the characteristic of various geospatial conditions.

::: {.callout-note collapse="true" title="What is Spurious Regression?"}
Spurious regression occurs when two unrelated variables show a strong statistical relationship in a regression analysis, even though there is no true cause-and-effect connection between them. In other words, the apparent relationship is misleading and arises by chance. This phenomenon can mislead researchers into thinking they have discovered a meaningful connection when, in fact, there is none. It's a cautionary concept, reminding us to carefully consider the theoretical basis and real-world relevance of variables before concluding that they are genuinely related based solely on statistical results.
:::

::: {.callout-note collapse="true" title="What is Spatial Econometric Interaction Model (SIEM)?"}
A Spatial Econometric Interaction Model is a statistical tool used to analyze how the interactions between different locations or units influence economic phenomena. In simpler terms, it helps us understand how economic variables in one area can affect or be affected by variables in neighboring areas. This model accounts for spatial dependencies, recognizing that nearby locations may share similarities or influence each other.

Imagine you want to study housing prices in different neighborhoods. A Spatial Econometric Interaction Model would not only consider local factors like the number of bedrooms and neighborhood amenities but also account for how prices in one neighborhood might be influenced by prices in nearby neighborhoods. It helps capture the idea that economic conditions in one area can spill over and impact neighboring areas, providing a more realistic and accurate analysis of regional economic relationships.
:::


# References

Cengel. [Introduction to spatial data in R](https://cengel.github.io/R-spatial/intro.html)

Co≈ükun, et al. (2020). [Performance Matters on Identification of Origin-Destination Matrix on Geospatial Big Data](https://isprs-archives.copernicus.org/articles/XLIII-B4-2020/449/2020/isprs-archives-XLIII-B4-2020-449-2020.pdf)

Daniels & Mulley. [Explaining walking distance to public transport: The dominance of public transport supply](https://www.jstor.org/stable/26202654)

Haynes & Fotheringham (1985). [Gravity and Spatial Interaction Models](https://researchrepository.wvu.edu/rri-web-book/16/)

Kam Tin Seong. [16 Calibrating Spatial Interaction Models with R](https://r4gdsa.netlify.app/chap16)

Miller (2021). [TRAFFIC ANALYSIS ZONE DEFINITION: ISSUES & GUIDANCE](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf)

R. [Spatial interaction models with R](https://cran.r-project.org/web/packages/simodels/vignettes/simodels.html.)

Sekste and Kazakov. ["H3 hexagonal grid: Why we use it for data analysis and visualization"](https://www.kontur.io/blog/h3-hexagonal-grid/).

Sid Dhuri (2020). ["Spatial Data Analysis With Hexagonal Grids"](https://medium.com/swlh/spatial-data-analysis-with-hexagonal-grids-961de90a220e)

Tao Ran (2021). [Big Spatial Flow Data Analytics. In: Werner, M., Chiang, YY. (eds) Handbook of Big Geospatial Data](https://doi.org/10.1007/978-3-030-55462-0_7)

Land Transport Authority. [Land Transport Data Mall](https://datamall.lta.gov.sg/content/datamall/en.html)
