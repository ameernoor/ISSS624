[
  {
    "objectID": "take-home/the1.html",
    "href": "take-home/the1.html",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "",
    "text": "The SceneThe Data\n\n\nAs cities upgrade to digital systems, like smart buses and GPS, a lot of data is created about how people move around. This data is super useful for understanding patterns in both space (where) and time (when). For example, on public buses, smart cards and GPS devices collect information about routes and how many people are riding. This data holds clues about how people behave in the city, and understanding these patterns can help manage the city better and give an edge to transportation providers.\nBut, often, this valuable data is only used for basic things like tracking and mapping using Geographic Information System (GIS) applications. This is because regular GIS tools struggle to fully analyze and model this kind of complex spatial and time-related data.\nIn this exercise, we’re going to focus on bus trips during specific peak hours:\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nTo better understand these movement patterns and behaviors, we’re going to use something called Exploratory Spatial Data Analysis (ESDA). It’s like a powerful tool that helps us tackle complex challenges in society. In this project, we’re going to use specific methods like Local Indicators of Spatial Association (LISA) and Emerging Hot Spot Analysis (EHSA) to reveal the interesting spatial and time-related mobility patterns of people who take public buses in Singapore. These methods will help us dig deep into the data and uncover valuable insights.\n\n\n\n\n\n\nWhat is LISA?\n\n\n\n\n\nLocal Indicators of Spatial Association (LISA) statistics serve two primary purposes. Firstly, they act as indicators of local pockets of nonstationarity, or hotspots. Secondly, they are used to assess the influence of individual locations on the magnitude of a global statistic. In simpler terms, LISA helps in identifying specific areas (like hotspots) that differ significantly from their surrounding areas and evaluates how these individual areas contribute to the overall pattern observed in a larger region.\n\n\n\n\n\n\n\n\n\nWhat is EHSA?\n\n\n\n\n\nEmerging Hot Spot Analysis (EHSA) is a type of spatial statistical analysis that examines data over space and time to identify trends and patterns, specifically focusing on the detection of statistical hot and cold spots within a study region. This method is applicable in various fields, including crime patterns, disease outbreaks, and environmental issues. EHSA allows users to set parameters and then determines whether identified hot or cold spots are persistent, increasing, or decreasing in a given area.\n\n\n\n\n\nthe content of the following panel explained what aspatial and geospatial data are used in this project.\n\nAspatialGeospatial\n\n\nMonthly Passenger Volume by Origin Destination Bus Stops:\n\nAugust, September and October 2023 Period\ndownloaded from LTA DataMall via API\ncsv format.\nColumns/Fields in the dataset includes YEAR_MONTH, DAY_TYPE, TIME_PER_HOUR, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE, and TOTAL_TRIPS.\n\n\n\nTwo geospatial data in shp format are used in this project, which includes:\n\nBus Stop Location from LTA DataMall. It provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.\nhexagon, a hexagon layer of 500m (perpendicular distance between the centre of the hexagon and its edges.) Each spatial unit is regular in shape and finer than the Master Plan 2019 Planning Sub-zone GIS data set of URA.\n\n\n\n\n\n\n\nwhy hexagon?\n\n\n\n\n\n\nUniform Distances Everywhere: Think of hexagons as honeycomb cells. Each cell (hexagon) touches its neighbors at the same distance from its center. It’s like standing in the middle of a room and being the same distance from every wall, making it easier to measure and compare things.\nOutlier-Free Shape: Hexagons are like well-rounded polygons without any pointy tips. Sharp corners can create odd spots in data, but hexagons smoothly cover space without sticking out anywhere. This helps prevent weird data spikes that don’t fit the pattern.\nConsistent Spatial Relationships: Imagine a beehive where every hexagon is surrounded by others in the same pattern. This regular pattern is great for analyzing data because you can expect the same relationships everywhere, making the data predictable and easier to work with.\nIdeal for Non-Perpendicular Features: Real-world features like rivers and roads twist and turn. Squares can be awkward for mapping these, but hexagons, which are more circular, can follow their flow better. This way, a hexagon-based map can mimic the real world more closely than a checkerboard of squares.\n\nSummarized from: Dhuri, and Sekste & Kazakov."
  },
  {
    "objectID": "take-home/the1.html#import-library",
    "href": "take-home/the1.html#import-library",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "2.1 Import Library",
    "text": "2.1 Import Library\n\n\nCode\npacman::p_load(tmap, sf, tidyverse, sfdep, knitr, Hmisc, mapview, DT)\n\n\npacman::p_load(kableExtra, DT, ggplot2, patchwork, ggrain)\nExplanations for the imported library:\n\ntmap for visualizing geospatial\nsf for handling geospatial data\ntidyverse for handling aspatial data\nsfdep for computing spatial weights, global and local spatial autocorrelation statistics, and\nknitr for creating html tables\nHmisc for summary statistics\nmapview for interactive map backgrouds\nDT library to create interactive html tables\n\nUnused Yet - kableExtra and DT for formatting of dataframes\n\nggplot2, patchwork and ggrain for visualising attributes\nurbnthemes for consistent plot theming\nggplot2 to plot graphs\nplotly to plot interactive graphs"
  },
  {
    "objectID": "take-home/the1.html#import-and-setup-the-data",
    "href": "take-home/the1.html#import-and-setup-the-data",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "2.2 Import and Setup the Data",
    "text": "2.2 Import and Setup the Data\n\nAspatial\nthe following code will import all the origin destination bus data and check a sample dataframe. The process also involved transforming georeference data type into factors for easing compatibility issue and more efficient processing.\n\nAugust 2023September 2023October 2023\n\n\n\n\nCode\n# Load each csv file\nodb8 &lt;- read_csv(\"../data/aspatial/origin_destination_bus_202308.csv.gz\")\n\n# change georeference data type into factors\nodb8 &lt;- odb8 %&gt;%\n  mutate(\n    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)\n  )\n\n# check the dataframe\ndescribe(odb8)\n\n\nodb8 \n\n 7  Variables      5709512  Observations\n--------------------------------------------------------------------------------\nYEAR_MONTH \n       n  missing distinct    value \n 5709512        0        1  2023-08 \n                  \nValue      2023-08\nFrequency  5709512\nProportion       1\n--------------------------------------------------------------------------------\nDAY_TYPE \n       n  missing distinct \n 5709512        0        2 \n                                            \nValue               WEEKDAY WEEKENDS/HOLIDAY\nFrequency           3279836          2429676\nProportion            0.574            0.426\n--------------------------------------------------------------------------------\nTIME_PER_HOUR \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n 5709512        0       24    0.997    14.07    5.949        6        7 \n     .25      .50      .75      .90      .95 \n      10       14       18       21       22 \n\nlowest :  0  1  2  3  4, highest: 19 20 21 22 23\n--------------------------------------------------------------------------------\nPT_TYPE \n       n  missing distinct    value \n 5709512        0        1      BUS \n                  \nValue          BUS\nFrequency  5709512\nProportion       1\n--------------------------------------------------------------------------------\nORIGIN_PT_CODE \n       n  missing distinct \n 5709512        0     5067 \n\nlowest : 01012 01013 01019 01029 01039, highest: 99139 99161 99171 99181 99189\n--------------------------------------------------------------------------------\nDESTINATION_PT_CODE \n       n  missing distinct \n 5709512        0     5071 \n\nlowest : 01012 01013 01019 01029 01039, highest: 99139 99161 99171 99181 99189\n--------------------------------------------------------------------------------\nTOTAL_TRIPS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n 5709512        0     3544    0.982    21.04    33.59        1        1 \n     .25      .50      .75      .90      .95 \n       2        4       13       38       74 \n\nlowest :     1     2     3     4     5, highest: 30799 31669 32508 33424 35049\n--------------------------------------------------------------------------------\n\n\n\n\n\n\nCode\n# Load each csv file\nodb9 &lt;- read_csv(\"../data/aspatial/origin_destination_bus_202309.csv.gz\")\n\n# change georeference data type into factors\nodb9 &lt;- odb9 %&gt;%\n  mutate(\n    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)\n  )\n\n\n# check the dataframe\ndescribe(odb9)\n\n\nodb9 \n\n 7  Variables      5714196  Observations\n--------------------------------------------------------------------------------\nYEAR_MONTH \n       n  missing distinct    value \n 5714196        0        1  2023-09 \n                  \nValue      2023-09\nFrequency  5714196\nProportion       1\n--------------------------------------------------------------------------------\nDAY_TYPE \n       n  missing distinct \n 5714196        0        2 \n                                            \nValue               WEEKDAY WEEKENDS/HOLIDAY\nFrequency           3183300          2530896\nProportion            0.557            0.443\n--------------------------------------------------------------------------------\nTIME_PER_HOUR \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n 5714196        0       23    0.997    14.06     5.94        6        7 \n     .25      .50      .75      .90      .95 \n      10       14       18       21       22 \n\nlowest :  0  1  2  4  5, highest: 19 20 21 22 23\n--------------------------------------------------------------------------------\nPT_TYPE \n       n  missing distinct    value \n 5714196        0        1      BUS \n                  \nValue          BUS\nFrequency  5714196\nProportion       1\n--------------------------------------------------------------------------------\nORIGIN_PT_CODE \n       n  missing distinct \n 5714196        0     5067 \n\nlowest : 01012 01013 01019 01029 01039, highest: 99139 99161 99171 99181 99189\n--------------------------------------------------------------------------------\nDESTINATION_PT_CODE \n       n  missing distinct \n 5714196        0     5072 \n\nlowest : 01012 01013 01019 01029 01039, highest: 99139 99161 99171 99181 99189\n--------------------------------------------------------------------------------\nTOTAL_TRIPS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n 5714196        0     3274    0.981    19.59    31.01        1        1 \n     .25      .50      .75      .90      .95 \n       2        4       12       35       69 \n\nlowest :     1     2     3     4     5, highest: 27356 28248 28510 30096 31674\n--------------------------------------------------------------------------------\n\n\n\n\n\n\nCode\n# Load each csv file\nodb10 &lt;- read_csv(\"../data/aspatial/origin_destination_bus_202310.csv.gz\")\n\n# change georeference data type into factors\nodb10 &lt;- odb10 %&gt;%\n  mutate(\n    ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n    DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE)\n  )\n\n# check the dataframe\ndescribe(odb10)\n\n\nodb10 \n\n 7  Variables      5694297  Observations\n--------------------------------------------------------------------------------\nYEAR_MONTH \n       n  missing distinct    value \n 5694297        0        1  2023-10 \n                  \nValue      2023-10\nFrequency  5694297\nProportion       1\n--------------------------------------------------------------------------------\nDAY_TYPE \n       n  missing distinct \n 5694297        0        2 \n                                            \nValue               WEEKDAY WEEKENDS/HOLIDAY\nFrequency           3259419          2434878\nProportion            0.572            0.428\n--------------------------------------------------------------------------------\nTIME_PER_HOUR \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n 5694297        0       23    0.997    14.04    5.933        6        7 \n     .25      .50      .75      .90      .95 \n      10       14       18       21       22 \n\nlowest :  0  1  2  4  5, highest: 19 20 21 22 23\n--------------------------------------------------------------------------------\nPT_TYPE \n       n  missing distinct    value \n 5694297        0        1      BUS \n                  \nValue          BUS\nFrequency  5694297\nProportion       1\n--------------------------------------------------------------------------------\nORIGIN_PT_CODE \n       n  missing distinct \n 5694297        0     5073 \n\nlowest : 01012 01013 01019 01029 01039, highest: 99139 99161 99171 99181 99189\n--------------------------------------------------------------------------------\nDESTINATION_PT_CODE \n       n  missing distinct \n 5694297        0     5077 \n\nlowest : 01012 01013 01019 01029 01039, highest: 99139 99161 99171 99181 99189\n--------------------------------------------------------------------------------\nTOTAL_TRIPS \n       n  missing distinct     Info     Mean      Gmd      .05      .10 \n 5694297        0     3495    0.982    20.76    33.13        1        1 \n     .25      .50      .75      .90      .95 \n       2        4       12       37       73 \n\nlowest :     1     2     3     4     5, highest: 30985 31349 32355 35931 36668\n--------------------------------------------------------------------------------\n\n\n\n\n\nExplanations for each field in the data can be found in the following metadata.\n\n\n\n\n\n\nmetadata\n\n\n\n\n\n\nYEAR_MONTH: Represent year and Month in which the data is collected. Since it is a monthly data frame, only one unique value exist in each data frame.\nDAY_TYPE: Represent type of the day which classified as weekdays or weekends/holidays.\nTIME_PER_HOUR: Hour which the passenger trip is based on, in intervals from 0 to 23 hours.\nPT_TYPE: Type of public transport, Since it is bus data sets, only one unique value exist in each data frame (i.e. bus)\nORIGIN_PT_CODE: ID of origin bus stop\nDESTINATION_PT_CODE: ID of destination bus stop\nTOTAL_TRIPS: Number of trips\n\n\n\n\n\n\nGeospatial\nthe following panel show the import process for the bus stop geospatial data. The geospatial layer of the data shows point location of bus stops across Singapore.\n\nImport and Check the DataSetup EPSG Code, CRS and Prepare the data for joining\n\n\nAs previously done, while reading the data, categorical data that can serve as reference are converted into factors for easing compatibility issue and more efficient processing.\n\n\nCode\nbusstop &lt;- st_read(\n    dsn = \"../data/geospatial\",\n    layer = \"BusStop\"\n  ) %&gt;%\n  mutate(\n    BUS_STOP_N = as.factor(BUS_STOP_N),\n    BUS_ROOF_N = as.factor(BUS_ROOF_N),\n    LOC_DESC = as.factor(LOC_DESC)\n  )\n\n\nReading layer `BusStop' from data source `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nCode\n# check the output\nglimpse(busstop)\n\n\nRows: 5,161\nColumns: 4\n$ BUS_STOP_N &lt;fct&gt; 22069, 32071, 44331, 96081, 11561, 66191, 23389, 54411, 285…\n$ BUS_ROOF_N &lt;fct&gt; B06, B23, B01, B05, B05, B03, B02A, B02, B09, B01, B16, B02…\n$ LOC_DESC   &lt;fct&gt; OPP CEVA LOGISTICS, AFT TRACK 13, BLK 239, GRACE INDEPENDEN…\n$ geometry   &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.38),…\n\n\n\nCheck the unique values\n\nBUS_STOP_NBUS_ROOF_NLOC_DESCgeometry\n\n\ncount of unique/distinct values are:\n\n\nCode\nn_distinct(busstop$BUS_STOP_N)\n\n\n[1] 5145\n\n\n\n\ncount of unique/distinct values are:\n\n\nCode\nn_distinct(busstop$BUS_ROOF_N)\n\n\n[1] 97\n\n\n\n\ncount of unique/distinct values are:\n\n\nCode\nn_distinct(busstop$LOC_DESC)\n\n\n[1] 4559\n\n\n\n\ncount of unique/distinct values are:\n\n\nCode\nn_distinct(busstop$geometry)\n\n\n[1] 5160\n\n\n\n\n\nExplanations for each field in the data can be found in the following metadata.\n\n\n\n\n\n\nmetadata\n\n\n\n\n\n\nBUS_STOP_N: The unique identifier for each bus stop.\nBUS_ROOF_N: The identifier for the bus route or roof associated with the bus stop.\nLOC_DESC: Location description providing additional information about the bus stop’s surroundings.\ngeometry: The spatial information representing the location of each bus stop as a point in the SVY21 projected coordinate reference system.\n\n\n\n\n\n\n\nTo ensure that geospatial data from different sources can be processed together, both have to be projected using similar coordinate systems and be assigned the correct EPSG code based on CRS. The st_crs function is used to check for ESPG Code and Coordinate System of both geospatial files. For Singapore, the coordinate system is SVY21 with EPSG 3414 (source: epsg.io). The following code chunk output shows how the CRS was initially not 3414, then corrected using st_set_crs. Simultaneously, the dataframe is also prepared for joining process.\n\n\nCode\n# Check the current Coordinate Reference System (CRS) of the busstop dataset\nprint(\"Current CRS of busstop dataset:\")\n\n\n[1] \"Current CRS of busstop dataset:\"\n\n\nCode\nprint(st_crs(busstop))\n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"WGS 84\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nCode\n# Assign a new EPSG code (Spatial Reference ID)\nbusstop &lt;- st_set_crs(\n   busstop, \n   3414\n) %&gt;%\n# Rename the bus stop origin column for easier joining with the main dataframe\nmutate(\n   ORIGIN_PT_CODE = as.factor(BUS_STOP_N)\n) %&gt;%\nselect(\n   ORIGIN_PT_CODE, \n   LOC_DESC,\n   geometry\n) %&gt;%\n# Change all column names to lowercase for consistency\nrename_with(\n   tolower, everything()\n)\n\n# Confirm the updated EPSG code after the assignment\nprint(\"Updated CRS of busstop dataset:\")\n\n\n[1] \"Updated CRS of busstop dataset:\"\n\n\nCode\nprint(st_crs(busstop))\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "take-home/the1.html#data-wrangling",
    "href": "take-home/the1.html#data-wrangling",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "2.3 Data Wrangling",
    "text": "2.3 Data Wrangling\n\ncheck for duplicates\nIn this step, we inspect the dataset for duplicate entries. This precautionary step is crucial to avoid the inadvertent repetition of records, which could lead to the overcounting of passenger trips. By identifying and addressing duplicates at this stage, we ensure the accuracy and reliability of our analysis, laying the groundwork for more precise and trustworthy results in the subsequent analytical processes. Checking for duplicates is a fundamental practice that contributes to the integrity of the data and the overall robustness of the geospatial analysis.\n\nAspatial - Origin Destination BusGeospatial - Bus Stop\n\n\nthe following code is used to check duplicates and show how many duplicates exist in each odb.\n\n\nCode\n# Count the number of rows in each dataframe with duplicates\ncount_duplicate_rows &lt;- function(df, df_name) {\n  df %&gt;%\n    group_by_all() %&gt;%\n    filter(n() &gt; 1) %&gt;%\n    ungroup() %&gt;%\n    summarise(df_name = df_name, row_count = n())\n}\n\n# Apply the function to each dataframe\nduplicate1 &lt;- count_duplicate_rows(odb8, \"odb8\")\nduplicate2 &lt;- count_duplicate_rows(odb9, \"odb9\")\nduplicate3 &lt;- count_duplicate_rows(odb10, \"odb10\")\n\n# Combine the results\nall_duplicates &lt;- bind_rows(duplicate1, duplicate2, duplicate3)\n\n# Print the counts\nprint(all_duplicates)\n\n\n# A tibble: 3 × 2\n  df_name row_count\n  &lt;chr&gt;       &lt;int&gt;\n1 odb8            0\n2 odb9            0\n3 odb10           0\n\n\n\nThe result shows that there are no duplicated data in the origin destination bus dataset. Note that the checking was based on a very loose criteria of duplicate, in which duplicated rows need to have the same value across all columns.\n\n\n\nThe following codes\n\n\nCode\n# Subset rows where origin_pt_code has duplicates\nduplicates &lt;- busstop[duplicated(busstop$origin_pt_code) | duplicated(busstop$origin_pt_code, fromLast = TRUE), ]\n\n# Display the rows with duplicate origin_pt_code\nkable(head(duplicates, n=32))\n\n\n\n\n\n\norigin_pt_code\nloc_desc\ngeometry\n\n\n\n\n149\n58031\nOPP CANBERRA DR\nPOINT (27089.69 47570.9)\n\n\n166\n62251\nBef Blk 471B\nPOINT (35500.54 39943.41)\n\n\n278\n47201\nNA\nPOINT (22616.75 47793.68)\n\n\n338\n58031\nOPP CANBERRA DR\nPOINT (27111.07 47517.77)\n\n\n501\n22501\nBlk 662A\nPOINT (13489.09 35536.4)\n\n\n751\n82221\nBLK 3A\nPOINT (35323.6 33257.05)\n\n\n1321\n68091\nAFT BAKER ST\nPOINT (32164.11 42695.98)\n\n\n1609\n43709\nBLK 644\nPOINT (18963.42 36762.8)\n\n\n1937\n97079\nOPP ST. JOHN’S CRES\nPOINT (44144.57 38980.25)\n\n\n2035\n82221\nBlk 3A\nPOINT (35308.74 33335.17)\n\n\n2038\n97079\nOPP ST. JOHN’S CRES\nPOINT (44055.75 38908.5)\n\n\n2092\n22501\nBLK 662A\nPOINT (13488.02 35537.88)\n\n\n2237\n62251\nBEF BLK 471B\nPOINT (35500.36 39943.34)\n\n\n2656\n68099\nBEF BAKER ST\nPOINT (32154.9 42742.82)\n\n\n2773\n52059\nOPP BLK 65\nPOINT (30770.3 34460.06)\n\n\n2970\n67421\nCHENG LIM STN EXIT B\nPOINT (34548.54 42052.15)\n\n\n3126\n11009\nGhim Moh Ter\nPOINT (23101.34 32594.17)\n\n\n3156\n53041\nUpp Thomson Road\nPOINT (28105.8 37246.76)\n\n\n3158\n53041\nUpp Thomson Road\nPOINT (27956.34 37379.29)\n\n\n3255\n77329\nBEF PASIR RIS ST 53\nPOINT (40765.35 39452.18)\n\n\n3261\n77329\nPasir Ris Central\nPOINT (40728.15 39438.15)\n\n\n3264\n96319\nYusen Logistics\nPOINT (42187.23 34995.78)\n\n\n3265\n96319\nYUSEN LOGISTICS\nPOINT (42187.23 34995.78)\n\n\n3303\n52059\nBLK 219\nPOINT (30565.45 36133.15)\n\n\n3411\n43709\nBLK 644\nPOINT (18952.02 36751.83)\n\n\n3470\n51071\nMACRITCHIE RESERVOIR\nPOINT (28311.27 36036.92)\n\n\n3472\n51071\nMACRITCHIE RESERVOIR\nPOINT (28282.54 36033.93)\n\n\n3665\n11009\nGHIM MOH TER\nPOINT (23100.58 32604.36)\n\n\n3752\n68091\nAFT BAKER ST\nPOINT (32038.84 43298.68)\n\n\n3753\n68099\nBEF BAKER ST\nPOINT (32004.05 43320.34)\n\n\n4624\n47201\nW’LANDS NTH STN\nPOINT (22632.92 47934)\n\n\n5095\n67421\nCHENG LIM STN EXIT B\nPOINT (34741.77 42004.21)\n\n\n\n\n\n\nThe result shows that there are some duplicated data in the geospatial bus stop dataset. This might interfere with the joining data process and generated double count on later on. Note that the checking was based on the origin_pt_code field only, because it will be the basis of reference for joining the dataset later on. At a glance, the table above also show that, the duplicated code are actually located near each other. Therefore, removing the duplicates can be considered safe as the remaining bus stop code can represent the deleted one. The folowing code chunk will execute the duplicate removal and show the result where number of rows have reduced.\n\n\n\nCode\n# Keep one row of the duplicates in the original dataset\nbusstop &lt;- busstop[!duplicated(busstop$origin_pt_code) | duplicated(busstop$origin_pt_code, fromLast = TRUE), ]\n\n# Display the resulting dataset\nglimpse(busstop)\n\n\nRows: 5,145\nColumns: 3\n$ origin_pt_code &lt;fct&gt; 22069, 32071, 44331, 96081, 11561, 66191, 23389, 54411,…\n$ loc_desc       &lt;fct&gt; OPP CEVA LOGISTICS, AFT TRACK 13, BLK 239, GRACE INDEPE…\n$ geometry       &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT (13228.59 44206.…\n\n\n\n\n\n\n\ncategorical peak time variable\non the interest of analyzing the peak time, the following code will assign new column that categorize peak time, filter data that is not on peak time, and show a sample of the output.\n\n\nCode\n# Function to categorize and aggregate trips\ncategorize_and_aggregate &lt;- function(odb) {\n  odb &lt;- odb %&gt;%\n    # Categorize trips under periods based on day and timeframe\n    mutate(period = case_when(\n      DAY_TYPE == \"WEEKDAY\" & TIME_PER_HOUR &gt;= 6 & TIME_PER_HOUR &lt;= 9 ~ \"Weekday morning peak\",\n      DAY_TYPE == \"WEEKDAY\" & TIME_PER_HOUR &gt;= 17 & TIME_PER_HOUR &lt;= 20 ~ \"Weekday evening peak\",\n      DAY_TYPE == \"WEEKENDS/HOLIDAY\" & TIME_PER_HOUR &gt;= 11 & TIME_PER_HOUR &lt;= 14 ~ \"Weekend/holiday morning peak\",\n      DAY_TYPE == \"WEEKENDS/HOLIDAY\" & TIME_PER_HOUR &gt;= 16 & TIME_PER_HOUR &lt;= 19 ~ \"Weekend/holiday evening peak\",\n      TRUE ~ \"non-peak\"\n    )) %&gt;%\n    # Only retain needed periods for analysis\n    filter(period != \"non-peak\") %&gt;%\n    # Compute the number of trips per origin bus stop per month for each period\n    group_by(YEAR_MONTH, period, ORIGIN_PT_CODE) %&gt;%\n    summarise(num_trips = sum(TOTAL_TRIPS)) %&gt;%\n    # Change all column names to lowercase\n    rename_with(tolower, everything()) %&gt;%\n    ungroup()\n\n  return(odb)\n}\n\n# Apply the function to each dataset\nodb8 &lt;- categorize_and_aggregate(odb8)\nodb9 &lt;- categorize_and_aggregate(odb9)\nodb10 &lt;- categorize_and_aggregate(odb10)\n\n# sample the result\nglimpse(odb10)\n\n\nRows: 20,072\nColumns: 4\n$ year_month     &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", …\n$ period         &lt;chr&gt; \"Weekday evening peak\", \"Weekday evening peak\", \"Weekda…\n$ origin_pt_code &lt;fct&gt; 01012, 01013, 01019, 01029, 01039, 01059, 01109, 01112,…\n$ num_trips      &lt;dbl&gt; 8000, 7038, 3398, 9089, 12095, 2212, 276, 43513, 25769,…\n\n\n\n\nchoosing the month\nIn order to decide which month is better to perform LISA or whether analyzing all month separately will yield interesting result, it is a good step to check the data distribution of each month. By comparing the data distribution for each month, we can make an educated guess whether the LISA result for each month would be starkly different or just similar.\n\n\nCode\n# Combine odb8, odb9, and odb10 into one dataframe\ncombined_data &lt;- bind_rows(\n  mutate(odb8, month = \"odb8\"),\n  mutate(odb9, month = \"odb9\"),\n  mutate(odb10, month = \"odb10\")\n)\n\n# Create a standard vertical box plot\nbus_boxplot &lt;- combined_data %&gt;%\n  ggplot(aes(x = period, y = num_trips, fill = period)) + # Assigning aesthetics for x and y axes, and fill color based on period\n  geom_boxplot() + # Adding the box plot layer\n  facet_wrap(~month, scales = 'free_x') + # Faceting by month, with free scales for x axis\n  labs(\n    title = \"Distribution of Trips across Peak Periods\",\n    subtitle = \"Comparison across different months\",\n    x = \"Period\",\n    y = \"Number of Trips\"\n  ) +\n  theme_minimal() + # Using a minimal theme for a cleaner look\n  theme(axis.text.x = element_blank(), axis.title.x = element_blank()) # Removing x-axis category labels and label  \nbus_boxplot\n\n\n\n\n\nThe box plot to show the data distribution was not too helpful as it shows that all peak time across months has extreme outliers. Therefore, the next code chunk modify the boxplot by log-transforming the number of trips.\n\n\nCode\n# Create a modified vertical box plot\nbus_boxplot &lt;- combined_data %&gt;%\n  ggplot(aes(x = period, y = log(num_trips), fill = period)) + # Modified aesthetics with log-transformed y-axis\n  geom_boxplot() + # Adding the box plot layer\n  facet_wrap(~month, scales = 'free_x') + # Faceting by month, with free scales for x axis\n  labs(\n    title = \"Distribution of Log-Transformed Trips\",\n    subtitle = \"Comparison across different months\",\n    y = \"Log(Number of Trips)\"\n  ) +\n  theme_minimal() + # Using a minimal theme for a cleaner look\n  theme(axis.text.x = element_blank(), axis.title.x = element_blank()) # Removing x-axis category labels and label\n\nbus_boxplot\n\n\n\n\n\n\nThe log-transformed box plot show that the distribution of each peak periods across months is quite similar. In general, number of trips in the weekday peaks are typically higher than weekend/holiday peak. The similarity also extend to extreme outliers. For example, the green box plot (Weekday morning peak) always have a single point extreme outlier on the top of the box plot. Based on this observation, it can be inferred that performing LISA on one of month is most likely enough to discover the pattern. The month October, as the latest data available, is chosen for the next processes.\n\n\n\nextract trip numbers into wide form of peak period categories\n\n\nCode\n# Extract data from odb10 and store as a separate dataframe\nodb10_wide &lt;- odb10 %&gt;%\n  # Pivot the data wider, treating each row as a bus stop code with peak period trips as columns\n  pivot_wider(\n    names_from = period,\n    values_from = num_trips\n  ) %&gt;%\n  select(2:6)\n\nDT::datatable(odb10_wide,\n              options = list(pageLength = 8),\n              rownames = FALSE)\n\n\n\n\n\n\n\nCode\n# check the output\nglimpse(odb10_wide)\n\n\nRows: 5,072\nColumns: 5\n$ origin_pt_code                 &lt;fct&gt; 01012, 01013, 01019, 01029, 01039, 0105…\n$ `Weekday evening peak`         &lt;dbl&gt; 8000, 7038, 3398, 9089, 12095, 2212, 27…\n$ `Weekday morning peak`         &lt;dbl&gt; 1770, 841, 1530, 2483, 2919, 1734, 200,…\n$ `Weekend/holiday evening peak` &lt;dbl&gt; 3061, 2770, 1685, 4063, 7263, 1118, 101…\n$ `Weekend/holiday morning peak` &lt;dbl&gt; 2177, 1818, 1536, 3217, 5408, 1159, 116…\n\n\n\n\njoin aspatial and geospatial data\nTo retain the geospatial property, use left_join by using busstop as the main dataframe and bus stop code as the reference.\n\n\nCode\nodb10_sf &lt;- left_join(busstop, odb10_wide, by = \"origin_pt_code\")\n\nglimpse(odb10_sf)\n\n\nRows: 5,145\nColumns: 7\n$ origin_pt_code                 &lt;fct&gt; 22069, 32071, 44331, 96081, 11561, 6619…\n$ loc_desc                       &lt;fct&gt; OPP CEVA LOGISTICS, AFT TRACK 13, BLK 2…\n$ `Weekday evening peak`         &lt;dbl&gt; 67, 5, 1740, 445, 199, 349, 432, 1285, …\n$ `Weekday morning peak`         &lt;dbl&gt; 50, NA, 2075, 411, 207, 405, 60, 3059, …\n$ `Weekend/holiday evening peak` &lt;dbl&gt; 10, NA, 589, 47, 77, 169, 82, 492, 2016…\n$ `Weekend/holiday morning peak` &lt;dbl&gt; 8, NA, 682, 110, 70, 177, 16, 1442, 204…\n$ geometry                       &lt;POINT [m]&gt; POINT (13576.31 32883.65), POINT …\n\n\n\n\nCreating Hexagon Spatial Grid\nodb10_sf represents a spatial point dataframe, which might not be optimal for spatial autocorrelation analysis where the definition of ‘areas’ requires the representation of spatial entities as polygons instead of individual points. To address this, the subsequent code sections transform these bus stops into a grid of hexagons.\n\n1. Generate Hexagon Grid2. Generate Attribute Dataframe using Hexagon Identifiers3. Join the grid and attribute dataframe\n\n\n\nUtilize the st_make_grid() function to create a hexagon grid.\nThe cellsize parameter, measured in the same units as the dataframe’s spatial reference system, determines the width of each hexagon. Given that odb10_sf is projected in ESPG code 3414 with meters as the unit, the hexagon width is set to 500m.\nEach hexagon is assigned a unique identifier, known as hex_id, which serves as the primary key for future reference.\n\nOutput: Spatial Polygons with Hexagonal Geometry and Hex_id Identification\n\n\nCode\nodb10_hex &lt;- st_make_grid(\n    odb10_sf,\n    cellsize = 500,\n    square = FALSE\n  ) %&gt;%\n  st_sf() %&gt;%\n  rowid_to_column(\"hex_id\")\n\n# check the output\nglimpse(odb10_hex)\n\n\nRows: 5,580\nColumns: 2\n$ hex_id   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ geometry &lt;POLYGON [m]&gt; POLYGON ((3720.122 26626.44..., POLYGON ((3720.122 27…\n\n\nThis code utilizes the st_make_grid function to create a hexagon grid based on the specified parameters. The resulting hexagon grid is then converted into a spatial dataframe (st_sf()) to maintain its geospatial properties. The rowid_to_column function is employed to assign a unique identifier (hex_id) to each hexagon, facilitating subsequent analyses or referencing.\n\n\nGiven that each hexagonal area may encompass multiple bus stops, the hex_id serves as the primary key for aggregation. The subsequent procedures outline how to organize attributes based on hex_id:\n\nUtilize the st_join() function with join = st_within to associate bus stop points with hexagon areas.\nThe st_set_geometry(NULL) argument eliminates the geospatial layer, focusing on attribute aggregation.\nEmploy group_by() to obtain a unique hex_id for each row.\nUtilize summarise() to calculate the aggregate count of bus stops and trips per peak period within each hexagon area.\nReplace all NA values with 0 using replace(is.na(.), 0).\n\nOutput: Attribute Dataframe, with Hex_id as the Primary Key\n\n\nCode\nodb10_stops &lt;- st_join(\n  odb10_sf, \n  odb10_hex, \n  join = st_within\n) %&gt;%\n  st_set_geometry(NULL) %&gt;%\n  group_by(hex_id) %&gt;%\n  summarise(\n    n_busstops = n(),\n    busstops_code = str_c(origin_pt_code, collapse = \",\"),\n    loc_desc = str_c(loc_desc, collapse = \",\"),\n    `Weekday morning peak` = sum(`Weekday morning peak`, na.rm = TRUE),\n    `Weekday evening peak` = sum(`Weekday evening peak`, na.rm = TRUE),\n    `Weekend/holiday morning peak` = sum(`Weekend/holiday morning peak`, na.rm = TRUE),\n    `Weekend/holiday evening peak` = sum(`Weekend/holiday evening peak`, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(across(where(is.numeric), ~ replace_na(., 0)),\n         across(where(is.character), ~ replace_na(., \"\")))\n\n# check the output\nglimpse(odb10_stops)\n\n\nRows: 1,524\nColumns: 8\n$ hex_id                         &lt;int&gt; 34, 65, 99, 127, 129, 130, 131, 159, 16…\n$ n_busstops                     &lt;int&gt; 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, …\n$ busstops_code                  &lt;chr&gt; \"25059\", \"25751\", \"26379\", \"25761\", \"25…\n$ loc_desc                       &lt;chr&gt; \"AFT TUAS STH BLVD\", \"BEF TUAS STH AVE …\n$ `Weekday morning peak`         &lt;dbl&gt; 103, 52, 78, 185, 1116, 53, 60, 64, 251…\n$ `Weekday evening peak`         &lt;dbl&gt; 390, 114, 291, 1905, 2899, 241, 368, 29…\n$ `Weekend/holiday morning peak` &lt;dbl&gt; 0, 26, 52, 187, 455, 76, 45, 21, 39, 69…\n$ `Weekend/holiday evening peak` &lt;dbl&gt; 56, 14, 100, 346, 634, 55, 49, 53, 132,…\n\n\n\n\n\nUse left_join to combine the new odb10_stops attribute dataframe with the existing odb10_hex hexagon geospatial layer, linking them by hex_id to integrate attributes into the spatial polygon dataframe.\nEmploy filter to exclude hexagons without bus stops.\n\nOutput: Spatial Polygon Dataframe\n\n\nCode\nodb10_complete &lt;- odb10_hex %&gt;%\n  left_join(odb10_stops,\n            by = \"hex_id\"\n  ) %&gt;%\n  replace(is.na(.), 0)\n\nodb10_final &lt;- filter(odb10_complete,\n                       n_busstops &gt; 0)\n\n# check the output\nglimpse(odb10_final)\n\n\nRows: 1,524\nColumns: 9\n$ hex_id                         &lt;int&gt; 34, 65, 99, 127, 129, 130, 131, 159, 16…\n$ n_busstops                     &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, …\n$ busstops_code                  &lt;chr&gt; \"25059\", \"25751\", \"26379\", \"25761\", \"25…\n$ loc_desc                       &lt;chr&gt; \"AFT TUAS STH BLVD\", \"BEF TUAS STH AVE …\n$ `Weekday morning peak`         &lt;dbl&gt; 103, 52, 78, 185, 1116, 53, 60, 64, 251…\n$ `Weekday evening peak`         &lt;dbl&gt; 390, 114, 291, 1905, 2899, 241, 368, 29…\n$ `Weekend/holiday morning peak` &lt;dbl&gt; 0, 26, 52, 187, 455, 76, 45, 21, 39, 69…\n$ `Weekend/holiday evening peak` &lt;dbl&gt; 56, 14, 100, 346, 634, 55, 49, 53, 132,…\n$ geometry                       &lt;POLYGON [m]&gt; POLYGON ((3970.122 27925.48...,…\n\n\n\n\n\nIn Step 2, the code outlines a process for associating bus stop attributes with hexagonal areas using st_join. This involves grouping and summarizing the attributes based on the unique hex_id. The resulting dataframe, odb10_stops, serves as an attribute-centric representation with the hex_id as the primary key.\nMoving to Step 3, the code then combines this attribute dataframe with the original hexagon geospatial layer, odb10_hex, utilizing left_join. The resulting spatial polygon dataframe, bustraffic10, integrates both geometric and attribute information. The filter operation ensures that only hexagons containing bus stops are retained in the final dataframe."
  },
  {
    "objectID": "take-home/the1.html#distribution-of-bus-stop",
    "href": "take-home/the1.html#distribution-of-bus-stop",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "3.1 Distribution of Bus Stop",
    "text": "3.1 Distribution of Bus Stop\n\n\nCode\n# Plot the hexagon grid based on n_busstops\ntmap_mode(\"view\")\nbusstop_map = tm_shape(odb10_final)+\n  tm_fill(\n    col = \"n_busstops\",\n    palette = \"Blues\",\n    style = \"cont\",\n    title = \"Number of Bus Stops\",\n    id = \"loc_desc\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.format = list(\n      grid_id = list(format = \"f\", digits = 0)\n    )\n  )+\n  tm_borders(col = \"grey40\", lwd = 0.7)\nbusstop_map\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\nFrom the map, we can infer that the central region, likely encompassing the Central Business District (CBD) and surrounding residential areas, which are known to be highly populated and active, has a higher number of bus stops. This correlates with the need for robust public transportation in densely populated areas to support the daily commute of residents and workers. Lighter shades are observed towards the periphery of the island, suggesting fewer bus stops, which could correspond to less urbanized or industrial areas, like the Western and North-Eastern regions where the population density is lower. The map reflects Singapore’s urban planning and transportation strategy, which aims to provide accessibility and connectivity, particularly in high-density regions where demand for public transit is greatest."
  },
  {
    "objectID": "take-home/the1.html#distribution-of-trips-across-peak-hours",
    "href": "take-home/the1.html#distribution-of-trips-across-peak-hours",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "3.2 Distribution of Trips Across Peak Hours",
    "text": "3.2 Distribution of Trips Across Peak Hours\nfind the ideal breaks for plotting the trips using summary\n\n\nCode\nsummary(odb10_final)\n\n\n     hex_id       n_busstops     busstops_code        loc_desc        \n Min.   :  34   Min.   : 1.000   Length:1524        Length:1524       \n 1st Qu.:1941   1st Qu.: 2.000   Class :character   Class :character  \n Median :2950   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :2813   Mean   : 3.376                                        \n 3rd Qu.:3734   3rd Qu.: 4.000                                        \n Max.   :5558   Max.   :11.000                                        \n Weekday morning peak Weekday evening peak Weekend/holiday morning peak\n Min.   :     0       Min.   :     0       Min.   :     0.0            \n 1st Qu.:   909       1st Qu.:  2139       1st Qu.:   384.8            \n Median :  7532       Median :  7246       Median :  2159.5            \n Mean   : 16838       Mean   : 16135       Mean   :  4986.7            \n 3rd Qu.: 23245       3rd Qu.: 16947       3rd Qu.:  6371.0            \n Max.   :386450       Max.   :542529       Max.   :109329.0            \n Weekend/holiday evening peak          geometry   \n Min.   :     0.0             POLYGON      :1524  \n 1st Qu.:   529.5             epsg:3414    :   0  \n Median :  2157.5             +proj=tmer...:   0  \n Mean   :  4999.7                                 \n 3rd Qu.:  5458.2                                 \n Max.   :150399.0                                 \n\n\n\nThe summary result confirmed that the trips data is right-skewed and contains extreme outliers. This knowledge is then used to customize the break in the comparison map.\n\nthe following code plot the comparison map\n\n\nCode\n# Define the columns for which we want to find the global min and max\nvalue_columns &lt;- c(\"Weekday morning peak\", \"Weekday evening peak\", \"Weekend/holiday morning peak\", \"Weekend/holiday evening peak\")\n\n# Set tmap to interactive viewing mode\ntmap_mode(\"view\")\n\n# Define a function to create each map with a customized legend\ncreate_map &lt;- function(value_column) {\n  tm_shape(odb10_final) +\n    tm_fill(\n      col = value_column,\n      palette = \"-RdBu\",\n      style = \"fixed\",\n      title = value_column,\n      id = \"loc_desc\",\n      showNA = FALSE,\n      alpha = 0.6,\n      breaks = c(0, 500, 1000, 2000, 3000, 5000, 10000, 50000, 100000, 300000, 550000)\n    ) +\n    tm_borders(col = \"grey40\", lwd = 0.7) +\n    tm_layout(\n      legend.position = c(\"right\", \"bottom\"), # Adjust legend position\n      legend.frame = TRUE,\n      legend.width = 0.15, # Adjust the width of the legend\n      legend.height = 0.5  # Adjust the height of the legend\n    )\n}\n\n# Apply the function to each value column and store the maps\nmap_list &lt;- lapply(value_columns, create_map)\n\n# Combine the maps into a 2x2 grid\ncombined_map &lt;- tmap_arrange(map_list[[1]], map_list[[2]], map_list[[3]], map_list[[4]], ncol = 1, nrow = 4)\n\n# Show the combined map\ncombined_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\nUsing the same classification scaling for all maps, it clearly shows that weekdays trips volume is higher in general than weekend/holiday. Nevertheless, at a glance, the crowded area are more or less the same across all days and time. This confirmed the previous finding on bus stops in which the area with most traffics are likely to encompassing the Central Business District (CBD) and surrounding residential areas, which are known to be highly populated and active the area of Singapore. At the same time, it also reflects Singapore’s urban planning and transportation strategy, in which the busiest area with potential high traffics are supported by more bus stops.\n\n\nDistribution of Average Trips per Bus Stop\nThe next distribution will see which bus stop is the busiest on average, in terms of number of trips per bus stop. To do that, firstly the following code will generate new columns of trip per bus stop for each hexagon.\n\n\nCode\n# Create a new dataframe with transformed columns\nodb10_final &lt;- odb10_final %&gt;%\n  mutate(across(all_of(value_columns), ~ .x / n_busstops, .names = \"avg_{.col}\"))\n# check the summary for determining break points\nsummary(odb10_final)\n\n\n     hex_id       n_busstops     busstops_code        loc_desc        \n Min.   :  34   Min.   : 1.000   Length:1524        Length:1524       \n 1st Qu.:1941   1st Qu.: 2.000   Class :character   Class :character  \n Median :2950   Median : 3.000   Mode  :character   Mode  :character  \n Mean   :2813   Mean   : 3.376                                        \n 3rd Qu.:3734   3rd Qu.: 4.000                                        \n Max.   :5558   Max.   :11.000                                        \n Weekday morning peak Weekday evening peak Weekend/holiday morning peak\n Min.   :     0       Min.   :     0       Min.   :     0.0            \n 1st Qu.:   909       1st Qu.:  2139       1st Qu.:   384.8            \n Median :  7532       Median :  7246       Median :  2159.5            \n Mean   : 16838       Mean   : 16135       Mean   :  4986.7            \n 3rd Qu.: 23245       3rd Qu.: 16947       3rd Qu.:  6371.0            \n Max.   :386450       Max.   :542529       Max.   :109329.0            \n Weekend/holiday evening peak          geometry    avg_Weekday morning peak\n Min.   :     0.0             POLYGON      :1524   Min.   :     0          \n 1st Qu.:   529.5             epsg:3414    :   0   1st Qu.:   391          \n Median :  2157.5             +proj=tmer...:   0   Median :  2598          \n Mean   :  4999.7                                  Mean   :  4513          \n 3rd Qu.:  5458.2                                  3rd Qu.:  6119          \n Max.   :150399.0                                  Max.   :119816          \n avg_Weekday evening peak avg_Weekend/holiday morning peak\n Min.   :     0.0         Min.   :    0.0                 \n 1st Qu.:   950.8         1st Qu.:  162.8                 \n Median :  2284.0         Median :  775.0                 \n Mean   :  4420.4         Mean   : 1349.9                 \n 3rd Qu.:  4575.0         3rd Qu.: 1659.1                 \n Max.   :136001.0         Max.   :43420.0                 \n avg_Weekend/holiday evening peak\n Min.   :    0.0                 \n 1st Qu.:  224.1                 \n Median :  711.1                 \n Mean   : 1380.3                 \n 3rd Qu.: 1468.9                 \n Max.   :39425.0                 \n\n\nthe following code plot the comparison map\n\n\nCode\n# Define the columns for which we want to find the global min and max\nvalue_columns &lt;- c(\"avg_Weekday morning peak\", \"avg_Weekday evening peak\", \"avg_Weekend/holiday morning peak\", \"avg_Weekend/holiday evening peak\")\n\n# Set tmap to interactive viewing mode\ntmap_mode(\"view\")\n\n# Define a function to create each map with a customized legend\ncreate_map &lt;- function(value_column) {\n  tm_shape(odb10_final) +\n    tm_fill(\n      col = value_column,\n      palette = \"-RdBu\",\n      style = \"fixed\",\n      title = value_column,\n      id = \"loc_desc\",\n      showNA = FALSE,\n      alpha = 0.6,\n      breaks = c(0, 100, 200, 300, 400, 500, 750, 1000, 1500, 5000, 10000, 50000, 140000)\n    ) +\n    tm_borders(col = \"grey40\", lwd = 0.7) +\n    tm_layout(\n      legend.position = c(\"right\", \"bottom\"), # Adjust legend position\n      legend.frame = TRUE,\n      legend.width = 0.15, # Adjust the width of the legend\n      legend.height = 0.5  # Adjust the height of the legend\n    )\n}\n\n# Apply the function to each value column and store the maps\nmap_list &lt;- lapply(value_columns, create_map)\n\n# Combine the maps into a 2x2 grid\ncombined_map &lt;- tmap_arrange(map_list[[1]], map_list[[2]], map_list[[3]], map_list[[4]], ncol = 1, nrow = 4)\n\n# Show the combined map\ncombined_map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntmap_mode(\"plot\")\n\n\n\nat a glance, using the average trips per bus stop shows slightly different insight. Comparatively to the total trips, average number of trips shows that the area around Jurong, Woodlands, and bus stops in Johor (part of Malaysia) is actually busier than what total trips suggest. In the context of transport policy, this might be the first lead to expand the number of bus stops in the particular area to cater for more commuters."
  },
  {
    "objectID": "take-home/the1.html#constructing-a-distance-based-spatial-weights-matrix",
    "href": "take-home/the1.html#constructing-a-distance-based-spatial-weights-matrix",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "4.1 Constructing a Distance-Based Spatial Weights Matrix",
    "text": "4.1 Constructing a Distance-Based Spatial Weights Matrix\nBefore delving into global/local spatial autocorrelation statistics, we need to create a spatial weights matrix for our study area. This matrix defines the neighbors of hexagonal spatial units based on their distances. Here are some considerations:\n\nEach feature should have at least one neighbor, and no feature should be a neighbor with all other features.\nFor skewed data, each feature should ideally have at least 8 neighbors (we’ll start with 6).\n\nGiven the sparse distribution of bus stops in certain regions (e.g., central catchment areas, military training areas, and airports), distance-based methods are favored over contiguity methods.\nAdaptive Distance-Based (Fixed Number of Neighbors) method is chosen over Fixed-Distance Threshold:. This method is chosen due to the right-skewed nature of our data. In this method, Each hexagon is guaranteed at least nneighbors, facilitating later statistical significance testing.\nWe will set each hexagon to have 12 neighbors using the provided R code. The process involves using st_knn to obtain a list of neighbors, and then st_weights to generate row-standardized spatial weights.\n\n\nCode\nwm_all &lt;- odb10_final %&gt;% \n  mutate(\n    nb = st_knn(geometry, k = 12),\n    wt = st_weights(nb, style = 'W'),\n    .before = 1\n  )\n\n# check the output\nkable(head(wm_all))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnb\nwt\nhex_id\nn_busstops\nbusstops_code\nloc_desc\nWeekday morning peak\nWeekday evening peak\nWeekend/holiday morning peak\nWeekend/holiday evening peak\ngeometry\navg_Weekday morning peak\navg_Weekday evening peak\navg_Weekend/holiday morning peak\navg_Weekend/holiday evening peak\n\n\n\n\n2, 4, 5, 8, 9, 12, 13, 16, 22, 23, 38, 39\n0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333\n34\n1\n25059\nAFT TUAS STH BLVD\n103\n390\n0\n56\nPOLYGON ((3970.122 27925.48…\n103\n390.0\n0.0\n56\n\n\n1, 3, 4, 5, 8, 9, 12, 13, 16, 22, 23, 38\n0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333\n65\n1\n25751\nBEF TUAS STH AVE 14\n52\n114\n26\n14\nPOLYGON ((4220.122 28358.49…\n52\n114.0\n26.0\n14\n\n\n5, 6, 7, 9, 10, 13, 14, 16, 17, 18, 24, 25\n0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333\n99\n1\n26379\nYONG NAM\n78\n291\n52\n100\nPOLYGON ((4470.122 30523.55…\n78\n291.0\n52.0\n100\n\n\n1, 2, 5, 8, 9, 12, 13, 16, 22, 23, 38, 39\n0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333\n127\n1\n25761\nOPP REC S’PORE\n185\n1905\n187\n346\nPOLYGON ((4720.122 28358.49…\n185\n1905.0\n187.0\n346\n\n\n3, 6, 9, 10, 12, 13, 14, 16, 17, 24, 30, 31\n0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333\n129\n2\n25719,26389\nTHE INDEX,BEF TUAS STH AVE 5\n1116\n2899\n455\n634\nPOLYGON ((4720.122 30090.54…\n558\n1449.5\n227.5\n317\n\n\n3, 5, 7, 9, 10, 11, 13, 14, 17, 18, 24, 25\n0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333\n130\n1\n26369\nSEE HUP SENG\n53\n241\n76\n55\nPOLYGON ((4720.122 30956.57…\n53\n241.0\n76.0\n55"
  },
  {
    "objectID": "take-home/the1.html#global-autocorrelation-of-spatial-association-global-morans-i-with-simulation",
    "href": "take-home/the1.html#global-autocorrelation-of-spatial-association-global-morans-i-with-simulation",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "4.2 Global Autocorrelation of Spatial Association (Global Moran’s I with Simulation)",
    "text": "4.2 Global Autocorrelation of Spatial Association (Global Moran’s I with Simulation)\nGlobal spatial association evaluates the overall spatial pattern of a variable, in this case, the total number of trips across the entire study area. It provides a single metric summarizing the extent to which similar values cluster together or disperse across the geographic space.\nCompute Global Moran’s I for each peak time trips generated at the hexagon level, utilizing simulated data to avoid assumptions of normality and randomness. The number of simulations is set to 99+1 = 100.\n\n\nCode\n# Set the seed to ensure reproducibility\nset.seed(1234)\n\n# define the value_columns\nvalue_columns &lt;- c(\"Weekday morning peak\", \"Weekday evening peak\", \"Weekend/holiday morning peak\", \"Weekend/holiday evening peak\")\n\n# Create a function to perform global Moran's I test\nperform_global_moran &lt;- function(data, value_column, k) {\n  # Compute spatial weights\n  nb &lt;- st_knn(data$geometry, k = k)\n  wt &lt;- st_weights(nb, style = 'W')\n\n  # Perform global Moran's I test\n  moran_result &lt;- global_moran_perm(data[[value_column]], nb, wt, nsim = 99)\n  \n  # Include the value_column in the result\n  result &lt;- list(\n    value_column = value_column,\n    moran_result = moran_result\n  )\n  \n  return(result)\n}\n\n# Apply the function for each time interval\nresults &lt;- lapply(value_columns, function(vc) perform_global_moran(wm_all, vc, k = 12))\n\n# Print the results\nprint(results)\n\n\n[[1]]\n[[1]]$value_column\n[1] \"Weekday morning peak\"\n\n[[1]]$moran_result\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.18614, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n[[2]]\n[[2]]$value_column\n[1] \"Weekday evening peak\"\n\n[[2]]$moran_result\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.044306, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n[[3]]\n[[3]]$value_column\n[1] \"Weekend/holiday morning peak\"\n\n[[3]]$moran_result\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.13804, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n[[4]]\n[[4]]$value_column\n[1] \"Weekend/holiday evening peak\"\n\n[[4]]$moran_result\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.084188, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nFor all four time intervals, since the p-value for global Moran’s I is smaller than 0.05, we can reject the null hypothesis that the spatial patterns are random. Moreover, as the Moran’s I statistics are greater than 0, we can infer that the spatial distribution exhibits signs of clustering for all four time intervals, consistent with the choropleth maps plotted earlier."
  },
  {
    "objectID": "take-home/the1.html#local-autocorrelation-of-spatial-association",
    "href": "take-home/the1.html#local-autocorrelation-of-spatial-association",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "4.3 Local Autocorrelation of Spatial Association",
    "text": "4.3 Local Autocorrelation of Spatial Association\nLocal spatial association provides a more detailed examination of spatial patterns at the local level, identifying specific areas with strong or weak spatial association. Local Moran’s I categorizes regions as high-high, low-low, high-low, or low-high, indicating clustering or outlier behavior.\nCompute Local Indicators of Spatial Association (LISA) for passenger trips generated by origin at the hex level. The function local_moran from the sfdep package is utilized, automatically computing neighbor lists and weights using simulated data. The results are then unnested for further analysis and displayed in an interactive datatable format.\n\n\nCode\n# Create a function to perform local Moran's I analysis\nget_lisa &lt;- function(wm, value_column, k) {\n  # Compute spatial weights\n  nb &lt;- st_knn(wm$geometry, k = k)\n  wt &lt;- st_weights(nb, style = 'W')\n\n  # Perform local Moran's I analysis and create a new data frame\n  result &lt;- wm %&gt;% \n    mutate(\n      local_moran = local_moran(.data[[value_column]], nb, wt, nsim = 99),\n      .before = 1\n    ) %&gt;%\n    unnest(local_moran)\n  \n  return(result)\n}\n\n# Initialize a list to store results for each value_column\nlisa_results &lt;- list()\n\n# Apply the function for each time interval and store results in the list\nfor (vc in value_columns) {\n  lisa_results[[vc]] &lt;- get_lisa(wm_all, vc, k = 6)\n}\n\n# Create a function to display a data table\ndisplay_datatable &lt;- function(lisa_result, value_column) {\n  datatable(\n    lisa_result,\n    class = 'cell-border stripe',\n    options = list(pageLength = 5),\n    caption = paste(\"Local Moran's I for\", value_column)\n  )\n}\n\n# Display data tables for each value column\nfor (vc in value_columns) {\n  display_datatable(lisa_results[[vc]], vc)\n}"
  },
  {
    "objectID": "take-home/the1.html#visualizing-significant-local-morans-i-at-95-confidence-level",
    "href": "take-home/the1.html#visualizing-significant-local-morans-i-at-95-confidence-level",
    "title": "Take-home 1 - Geospatial Analytics for Public Good",
    "section": "4.4 Visualizing Significant Local Moran’s I at 95% Confidence Level",
    "text": "4.4 Visualizing Significant Local Moran’s I at 95% Confidence Level\nUtilize tmap core functions to construct choropleth maps displaying the Local Moran’s I field and p-value field for all four time intervals. Only significant values of Local Moran’s I at the 95% confidence level are plotted.\n\n\nCode\n#restoring tmap mode for rendering\ntmap_mode(\"plot\")"
  },
  {
    "objectID": "in-class/ice2.html",
    "href": "in-class/ice2.html",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "",
    "text": "Illustration"
  },
  {
    "objectID": "in-class/ice2.html#calculating-spatial-weights",
    "href": "in-class/ice2.html#calculating-spatial-weights",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "2.1 Calculating Spatial Weights",
    "text": "2.1 Calculating Spatial Weights\n\nTwo types of Spatial Weights: 1) Contiguity Weights considers how neighboring areas are connected or share a common border, emphasizing spatial adjacency; 2) Distance-based Weights: This type takes into account the distance between locations, giving more weight to closer locations and less weight to those farther away, capturing spatial relationships based on proximity.\n\n\n2.1.1 Contiguity Weights\nThis part of exercise will use contiguity spatial weights using sfdep package. To derive the weights, the following steps is required:\n\nidentify contiguity neighbour list by using st_contiguity() from sfdep package.\nderive the spatial weights by using st_weights() from sfdep package\n\n\nThe advantage of sfdep over spdep is that its output is in the form of an sf tibble data frame. This is beneficial because sf tibble data frames are part of the tidyverse ecosystem, making it easier to work with and integrate into tidy data workflows in R.\n\n\nIdentifying Contiguity Neighbours\nthe following panel will show how to identify contiguity neighbours using various methods.\n\nQueen’s MethodRook’s MethodHigher Order Neighbors\n\n\n\n\nCode\n# Create neighbor dataframe using Queen Method from the original 'hunan_GDPPC' dataframe\nnb_queen &lt;- hunan_GDPPC %&gt;%\n\n  # Add a new column 'nb' (neighbors) representing contiguity relationships using spatial geometries\n  mutate(nb = st_contiguity(geometry),\n\n         # Insert the newly created columns at the beginning of the dataset\n         .before=1)\n\n# summarize the neighbors column\nsummary(nb_queen$nb)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n\n\nCode\n# Create neighbor dataframe using Rook Method from the original 'hunan_GDPPC' dataframe\nnb_rook &lt;- hunan_GDPPC %&gt;%\n\n  # Add a new column 'nb' (neighbors) representing contiguity relationships using spatial geometries\n  mutate(nb = st_contiguity(geometry, queen = FALSE),\n\n         # Add another column 'wt' calculating weights based on contiguity relationships\n         wt = st_weights(nb, style = 'W'),\n\n         # Insert the newly created columns at the beginning of the dataset\n         .before=1)\n\n# summarize the neighbors column\nsummary(nb_rook$nb)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\n\n\n\nSpatial relationships may extend beyond immediate neighbors when we’re dealing with complex geographical patterns or phenomena that involve interactions across multiple layers or scales. In such cases, high-order contiguity becomes relevant because it allows us to capture and analyze more distant spatial connections. This is particularly important when studying phenomena with a broader reach or influence that goes beyond the traditional notion of adjacent neighbors, providing a more comprehensive understanding of spatial dependencies in the data.\n\nThe following code chunk give example of using st_nb_lag_cumul() to derive contiguity neighbour list using lag 2 Queen’s method. It set the lag order to 2, so the result contains both 1st and 2nd order neighbors.\n\n\nCode\nnb2_queen &lt;-  hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n          # Add another new column 'nb2' calculating cumulative second-order contiguity relationships\n         nb2 = st_nb_lag_cumul(nb, 2),\n         .before = 1)\n\n# Check the output\nsummary(nb2_queen$nb2)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 1324 \nPercentage nonzero weights: 17.09711 \nAverage number of links: 15.04545 \nLink number distribution:\n\n 5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 26 28 33 \n 2  1  6  4  5  4  8  5 10  4  4  8  4  8  5  2  2  1  2  1  1  1 \n2 least connected regions:\n30 88 with 5 links\n1 most connected region:\n56 with 33 links\n\n\n\n\n\nthe following code check the whole output using the 2 orders contiguity as example\n\n\nCode\nkable(head(nb2_queen, n=10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnb\nnb2\nNAME_2\nID_3\nNAME_3\nENGTYPE_3\nCounty\nGDPPC\ngeometry\n\n\n\n\n2, 3, 4, 57, 85\n2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\nChangde\n21098\nAnxiang\nCounty\nAnxiang\n23667\nPOLYGON ((112.0625 29.75523…\n\n\n1, 57, 58, 78, 85\n1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\nChangde\n21100\nHanshou\nCounty\nHanshou\n20981\nPOLYGON ((112.2288 29.11684…\n\n\n1, 4, 5, 85\n1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\nChangde\n21101\nJinshi\nCounty City\nJinshi\n34592\nPOLYGON ((111.8927 29.6013,…\n\n\n1, 3, 5, 6\n1, 2, 3, 5, 6, 57, 69, 75, 85\nChangde\n21102\nLi\nCounty\nLi\n24473\nPOLYGON ((111.3731 29.94649…\n\n\n3, 4, 6, 85\n1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\nChangde\n21103\nLinli\nCounty\nLinli\n25554\nPOLYGON ((111.6324 29.76288…\n\n\n4, 5, 69, 75, 85\n1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\nChangde\n21104\nShimen\nCounty\nShimen\n27137\nPOLYGON ((110.8825 30.11675…\n\n\n67, 71, 74, 84\n9, 19, 66, 67, 71, 73, 74, 76, 84, 86\nChangsha\n21109\nLiuyang\nCounty City\nLiuyang\n63118\nPOLYGON ((113.9905 28.5682,…\n\n\n9, 46, 47, 56, 78, 80, 86\n2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\nChangsha\n21110\nNingxiang\nCounty\nNingxiang\n62202\nPOLYGON ((112.7181 28.38299…\n\n\n8, 66, 68, 78, 84, 86\n2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\nChangsha\n21111\nWangcheng\nCounty\nWangcheng\n70666\nPOLYGON ((112.7914 28.52688…\n\n\n16, 17, 19, 20, 22, 70, 72, 73\n11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\nChenzhou\n21112\nAnren\nCounty\nAnren\n12761\nPOLYGON ((113.1757 26.82734…\n\n\n\n\n\n\n\nDeriving contiguity weights\nThe following panel shows how to use st_weights() of sfdep package to derive contiguity weights. the function provides three arguments which includes: - nb: a neighbor list object as created by st_neighbors() - style: Default “W” for row standardized weights. The value can also be “B”, “C”, “U”, “minmax”, and “S”. B is the basic binary coding, W is row standardises (sums over all links to n), C is globally standardised(sums over all links to n). U is equal to C divided by number of neighbours (sums over all links to unity, while S is a variance-stabilizing coding scheme (sums over all links to n). - allow_zero: If TRUE, assigns zero as lagged value to zone without neighbors.\n\nQueen - WHigher Order Queen - W\n\n\nThe following code will use queen method to derive contiguity weights (it’s the default method when the argument is not specified)\n\n\nCode\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         # add the weight column\n         wt = st_weights(nb, style = \"W\"),\n         .before = 1) \n# check the output\nwm_q\n\n\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\nCode\nwm2_q &lt;-  hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, style = \"W\"),\n         nb2 = st_nb_lag_cumul(nb, 2),\n         wt2 = st_weights(nb2, style = \"W\"),\n                  .before = 1)\n\n# Check the output\nwm2_q\n\n\nSimple feature collection with 88 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                            wt\n1                                                      0.2, 0.2, 0.2, 0.2, 0.2\n2                                                      0.2, 0.2, 0.2, 0.2, 0.2\n3                                                       0.25, 0.25, 0.25, 0.25\n4                                                       0.25, 0.25, 0.25, 0.25\n5                                                       0.25, 0.25, 0.25, 0.25\n6                                                      0.2, 0.2, 0.2, 0.2, 0.2\n7                                                       0.25, 0.25, 0.25, 0.25\n8  0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571, 0.1428571\n9             0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667\n10                      0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125\n                                                                                        nb2\n1                                     2, 3, 4, 5, 6, 32, 56, 57, 58, 64, 69, 75, 76, 78, 85\n2                           1, 3, 4, 5, 6, 8, 9, 32, 56, 57, 58, 64, 68, 69, 75, 76, 78, 85\n3                                                 1, 2, 4, 5, 6, 32, 56, 57, 69, 75, 78, 85\n4                                                             1, 2, 3, 5, 6, 57, 69, 75, 85\n5                                                 1, 2, 3, 4, 6, 32, 56, 57, 69, 75, 78, 85\n6                                         1, 2, 3, 4, 5, 32, 53, 55, 56, 57, 69, 75, 78, 85\n7                                                     9, 19, 66, 67, 71, 73, 74, 76, 84, 86\n8  2, 9, 19, 21, 31, 32, 34, 35, 36, 41, 45, 46, 47, 56, 58, 66, 68, 74, 78, 80, 84, 85, 86\n9               2, 7, 8, 19, 21, 35, 46, 47, 56, 58, 66, 67, 68, 74, 76, 78, 80, 84, 85, 86\n10               11, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 70, 71, 72, 73, 74, 82, 83, 86\n                                                                                                                                                                                                                                                                                  wt2\n1                                                                                                  0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667\n2                                                              0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556, 0.05555556\n3                                                                                                                                      0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333\n4                                                                                                                                                                                   0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111, 0.1111111\n5                                                                                                                                      0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333, 0.08333333\n6                                                                                                              0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857, 0.07142857\n7                                                                                                                                                                                                                                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1\n8  0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826, 0.04347826\n9                                                                                                                                                              0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05\n10                                                 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734...\n\n\n\n\n\n\n\n\n2.1.2 Distance-based Weights\nThe following panel display examples of how to use various method of deriving distance-based spatial weights. Important functions used includes: - st_nb_dists() of sfdep to calculate the nearest neighbour distance, generating a list of distances for each feature’s neighbors. - unlist() of Base R to convert output into a vector form to enable summary statistics. - st_dists_band() of sfdep is used to identify neighbors based on a distance band, by specifiying upper and lower arguments. The output is a list of neighbours (i.e. nb). - st_weights() is used to calculate spatial weights of the nb list. - st_knn() of sfdep is used to identify specified number of neighbors (default is k=1, one nearest neighbour). - st_contiguity() of sfdep is used to identify the neighbours using contiguity criteria. - st_inverse_distance() is used to calculate inverse distance weights of neighbours on the nb list.\n\nfixed distance weightsadaptive distance weightsinverse distance weights\n\n\nfind maximum distance in knn with 1 neighbor\n\n\nCode\n# Extract the geometry (spatial information) from the 'hunan_GDPPC' dataset\ngeo &lt;- sf::st_geometry(hunan_GDPPC)\n\n# Create a spatial weights matrix using k-nearest neighbors (KNN) for the extracted geometry\nnb &lt;- st_knn(geo, longlat = TRUE)\n\n# Calculate the distances between each feature's centroid and its k-nearest neighbors\ndists &lt;- unlist(st_nb_dists(geo, nb))\n\n# show the result\nsummary(dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  21.56   29.11   36.89   37.34   43.21   65.80 \n\n\nuse the maximum distance to set threshold value in the following fixed distance weights calculation code:\n\n\nCode\n# Create a new variable wm_fd using the hunan_GDPPC data frame\nwm_fd &lt;- hunan_GDPPC %&gt;%\n  \n  # Add a new column 'nb' to the data frame\n  mutate(\n    nb = st_dist_band(geometry, upper = 66),  # Calculate a distance band based on geometry\n    \n    # Add a new column 'wt' to the data frame\n    wt = st_weights(nb),  # Calculate weights based on the distance band\n    \n    # Place the new columns 'nb' and 'wt' before the existing columns in the data frame\n    .before = 1\n  )\n\n# check the output\nprint(summary(wm_fd$nb))\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 400 \nPercentage nonzero weights: 5.165289 \nAverage number of links: 4.545455 \nLink number distribution:\n\n 1  2  3  4  5  6  7 \n 4  7 10 16 23 23  5 \n4 least connected regions:\n30 32 65 75 with 1 link\n5 most connected regions:\n41 52 58 63 80 with 7 links\nNULL\n\n\nCode\nprint(glimpse(wm_fd))\n\n\nRows: 88\nColumns: 9\n$ nb        &lt;nb&gt; &lt;2, 3, 4, 5, 57, 64&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 57&gt;, &lt;1,…\n$ wt        &lt;list&gt; &lt;0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1…\n$ NAME_2    &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Chan…\n$ ID_3      &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2111…\n$ NAME_3    &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Li…\n$ ENGTYPE_3 &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"Coun…\n$ County    &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Li…\n$ GDPPC     &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7066…\n$ geometry  &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.2288 2…\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                       nb\n1      2, 3, 4, 5, 57, 64\n2       1, 57, 58, 78, 85\n3             1, 4, 5, 57\n4              1, 3, 5, 6\n5          1, 3, 4, 6, 69\n6                4, 5, 69\n7              67, 71, 84\n8       9, 46, 47, 78, 80\n9   8, 46, 66, 68, 84, 86\n10 16, 20, 22, 70, 72, 73\n                                                                 wt   NAME_2\n1  0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667  Changde\n2                                           0.2, 0.2, 0.2, 0.2, 0.2  Changde\n3                                            0.25, 0.25, 0.25, 0.25  Changde\n4                                            0.25, 0.25, 0.25, 0.25  Changde\n5                                           0.2, 0.2, 0.2, 0.2, 0.2  Changde\n6                                   0.3333333, 0.3333333, 0.3333333  Changde\n7                                   0.3333333, 0.3333333, 0.3333333 Changsha\n8                                           0.2, 0.2, 0.2, 0.2, 0.2 Changsha\n9  0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667 Changsha\n10 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667, 0.1666667 Chenzhou\n    ID_3    NAME_3   ENGTYPE_3    County GDPPC                       geometry\n1  21098   Anxiang      County   Anxiang 23667 POLYGON ((112.0625 29.75523...\n2  21100   Hanshou      County   Hanshou 20981 POLYGON ((112.2288 29.11684...\n3  21101    Jinshi County City    Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4  21102        Li      County        Li 24473 POLYGON ((111.3731 29.94649...\n5  21103     Linli      County     Linli 25554 POLYGON ((111.6324 29.76288...\n6  21104    Shimen      County    Shimen 27137 POLYGON ((110.8825 30.11675...\n7  21109   Liuyang County City   Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  21110 Ningxiang      County Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  21111 Wangcheng      County Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10 21112     Anren      County     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\n\n\nusing st_knn with number of neighbors (k) fixed to 8, it will find 8 nearest neighbours for each feature, without being limited by maximum distance (adaptive).\n\n\nCode\nwm_ad &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_knn(geometry,\n                     k=8),\n         wt = st_weights(nb),\n               .before = 1)\n\n# check the output\nprint(summary(wm_ad$nb))\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\nNULL\n\n\nCode\nprint(glimpse(wm_ad))\n\n\nRows: 88\nColumns: 9\n$ nb        &lt;nb&gt; &lt;2, 3, 4, 5, 57, 58, 64, 76&gt;, &lt;1, 3, 8, 57, 58, 68, 78, 85&gt;, …\n$ wt        &lt;list&gt; &lt;0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125&gt;, &lt;…\n$ NAME_2    &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Chan…\n$ ID_3      &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2111…\n$ NAME_3    &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Li…\n$ ENGTYPE_3 &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"Coun…\n$ County    &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Li…\n$ GDPPC     &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7066…\n$ geometry  &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.2288 2…\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1      2, 3, 4, 5, 57, 58, 64, 76\n2     1, 3, 8, 57, 58, 68, 78, 85\n3       1, 2, 4, 5, 6, 57, 64, 85\n4       1, 2, 3, 5, 6, 57, 64, 69\n5       1, 2, 3, 4, 6, 57, 69, 85\n6       1, 2, 3, 4, 5, 69, 75, 85\n7   9, 66, 67, 68, 71, 74, 84, 86\n8    2, 9, 35, 46, 47, 78, 80, 86\n9   8, 46, 47, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                       wt   NAME_2  ID_3\n1  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125  Changde 21098\n2  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125  Changde 21100\n3  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125  Changde 21101\n4  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125  Changde 21102\n5  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125  Changde 21103\n6  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125  Changde 21104\n7  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125 Changsha 21109\n8  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125 Changsha 21110\n9  0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125 Changsha 21111\n10 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125, 0.125 Chenzhou 21112\n      NAME_3   ENGTYPE_3    County GDPPC                       geometry\n1    Anxiang      County   Anxiang 23667 POLYGON ((112.0625 29.75523...\n2    Hanshou      County   Hanshou 20981 POLYGON ((112.2288 29.11684...\n3     Jinshi County City    Jinshi 34592 POLYGON ((111.8927 29.6013,...\n4         Li      County        Li 24473 POLYGON ((111.3731 29.94649...\n5      Linli      County     Linli 25554 POLYGON ((111.6324 29.76288...\n6     Shimen      County    Shimen 27137 POLYGON ((110.8825 30.11675...\n7    Liuyang County City   Liuyang 63118 POLYGON ((113.9905 28.5682,...\n8  Ningxiang      County Ningxiang 62202 POLYGON ((112.7181 28.38299...\n9  Wangcheng      County Wangcheng 70666 POLYGON ((112.7914 28.52688...\n10     Anren      County     Anren 12761 POLYGON ((113.1757 26.82734...\n\n\n\n\ncalculate the weight based on inverse distance (the farther from the feature, the less weight).\n\n\nCode\nwm_idw &lt;- hunan_GDPPC %&gt;% \n  mutate(nb = st_contiguity(geometry), # set the neighbours based on contiguity\n         wts = st_inverse_distance(nb, geometry,  # Create a new variable 'wts' representing inverse distance weights by using the 'st_inverse_distance' function.\n                                   scale = 1,  # Set the scale parameter to 1, meaning distances will be used as they are.\n                                   alpha = 1),  # Set the alpha parameter to 1, indicating a linear decrease in influence with distance.\n         .before = 1)\n\n# check the output\nprint(summary(wm_idw$nb))\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\nNULL\n\n\nCode\nprint(glimpse(wm_idw))\n\n\nRows: 88\nColumns: 9\n$ nb        &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1, 3,…\n$ wts       &lt;list&gt; &lt;0.01526149, 0.03515537, 0.02176677, 0.02836978, 0.01029857…\n$ NAME_2    &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Chan…\n$ ID_3      &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2111…\n$ NAME_3    &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Li…\n$ ENGTYPE_3 &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"Coun…\n$ County    &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Li…\n$ GDPPC     &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7066…\n$ geometry  &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.2288 2…\nSimple feature collection with 88 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                               nb\n1                 2, 3, 4, 57, 85\n2               1, 57, 58, 78, 85\n3                     1, 4, 5, 85\n4                      1, 3, 5, 6\n5                     3, 4, 6, 85\n6                4, 5, 69, 75, 85\n7                  67, 71, 74, 84\n8       9, 46, 47, 56, 78, 80, 86\n9           8, 66, 68, 78, 84, 86\n10 16, 17, 19, 20, 22, 70, 72, 73\n                                                                                              wts\n1                                      0.01526149, 0.03515537, 0.02176677, 0.02836978, 0.01029857\n2                                      0.01526149, 0.01601100, 0.01911052, 0.02327058, 0.01591694\n3                                                  0.03515537, 0.04581089, 0.04116397, 0.01208437\n4                                                  0.02176677, 0.04581089, 0.04637578, 0.01585302\n5                                                  0.04116397, 0.04637578, 0.01896212, 0.01351099\n6                                      0.01585302, 0.01896212, 0.02710909, 0.01140718, 0.01080890\n7                                                  0.01621067, 0.01536702, 0.01133628, 0.01836488\n8              0.01930410, 0.02675555, 0.02151751, 0.01076895, 0.02608065, 0.01519804, 0.01337412\n9                          0.01930410, 0.01651371, 0.01798519, 0.01473155, 0.03015561, 0.01612293\n10 0.02737233, 0.01390810, 0.01458881, 0.02156771, 0.02419268, 0.02350470, 0.01784174, 0.01621545\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "in-class/ice2.html#global-measures-of-spatial-association",
    "href": "in-class/ice2.html#global-measures-of-spatial-association",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "3.1 Global Measures of Spatial Association",
    "text": "3.1 Global Measures of Spatial Association\nThe global spatial association here is measured using Moran’s I statistics in sfdep package. Specifically global_moran, and global_moran_test(), globel_moran_perm() and functions are used.\nThe following panel show step by step of how its done.\n\n1 Derive contiguity weights2 Computing Global Moran’s I3a Global Moran’s I test3b Global Moran’s I permutation test\n\n\n\n\nCode\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n# check the output\nglimpse(wm_q)\n\n\nRows: 88\nColumns: 9\n$ nb        &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1, 3,…\n$ wt        &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.25…\n$ NAME_2    &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Chan…\n$ ID_3      &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2111…\n$ NAME_3    &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Li…\n$ ENGTYPE_3 &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"Coun…\n$ County    &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Li…\n$ GDPPC     &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7066…\n$ geometry  &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.2288 2…\n\n\n\n\n\n\nCode\nmoranI &lt;- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n# check the output\nglimpse(moranI)\n\n\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n\n\n\nThe Moran’s I value of 0.301 indicates that there is a moderate positive spatial autocorrelation in the distribution of GDP per capita (GDPPC). Spatial autocorrelation means that similar values tend to be clustered together in geographic space. In this context, areas with similar economic conditions are somewhat grouped or clustered on the map. The value of K, which is 7.64 in this case, provides a reference point for what we might expect under the assumption of spatial randomness. If the observed Moran’s I value is significantly different from the expected value of K, it suggests that the spatial pattern is not random. In simpler terms, the Moran’s I value of 0.301 is telling us that the distribution of GDP per capita in the geographic areas being studied is not random – there is a discernible pattern where neighboring areas tend to have similar economic conditions.\n\n\n\n\n\nCode\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\n\n\n    Moran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nThe Global Moran’s I test was conducted to assess whether the spatial pattern of GDP per capita (GDPPC) is random or exhibits clustering. The Moran I statistic standard deviate, which is 4.7351, indicates a strong positive spatial autocorrelation, reaffirming our earlier finding of a moderate positive spatial autocorrelation (Moran’s I value of 0.301). The p-value, being very small (1.095e-06), suggests that the observed spatial pattern is highly unlikely to occur by random chance. In simpler terms, this indicates a significant spatial clustering of similar economic conditions in neighboring areas on the map.\n\n\n\n\n\nCode\nset.seed(1234) # set seed to ensure computation is reproducible\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99) # number of simulation is nsim + 1, which means in this current setting, 100 simulation will be performed.\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\np-value is smaller than alpha value of 0.05. Hence, reject the null hypothesis that the spatial patterns spatial independent. Because the Moran’s I statistics is greater than 0. We can infer the spatial distribution shows sign of clustering.\n\n\n\n\nThe use of Monte Carlo simulation global_moran_perm(), is preferred over the normal global Moran test global_moran_test() when assessing spatial patterns because it provides a more reliable statistical test. Monte Carlo simulation generates many random scenarios to estimate the distribution of Moran’s I under the assumption of spatial randomness, allowing for a non-parametric evaluation of statistical significance. This approach is robust, especially when the assumptions of normality may not hold, making it a more flexible and accurate method for detecting spatial patterns in real-world data.\n\n\nInterpreting Global Moran’s I Value: 1) Positive Value Indicates positive spatial autocorrelation, meaning similar values are clustered together; 2) Negative Value Indicates negative spatial autocorrelation, meaning dissimilar values are clustered together; 3) Magnitude, The closer the value is to 1 (positively) or -1 (negatively), the stronger the spatial autocorrelation.\n\n\nInterpreting P-value of GLobal Moran’s I test: P-Value &lt; α Suggests that the observed spatial pattern is unlikely to be due to random chance, and vice versa. Positive Moran’s I and significant P Indicates a significant spatial clustering of similar values in neighboring areas. Negative Moran’s I and significant P Suggests a significant spatial dispersion or segregation of dissimilar values."
  },
  {
    "objectID": "in-class/ice2.html#local-measure-of-spatial-autocorrelation",
    "href": "in-class/ice2.html#local-measure-of-spatial-autocorrelation",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "3.2 Local Measure of Spatial Autocorrelation",
    "text": "3.2 Local Measure of Spatial Autocorrelation\n\n3.2.1 Computing local Moran’s I\nThis section will compute Local Moran’s I of GDPPC at county level by using local_moran() of sfdep package. unnest() of tidyr package is used to expand a list-column containing data frames into rows and columns.\n\n\nCode\n# LISA (Local Indicator of Spatial Autocorrelation)\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim = 99),\n    .before = 1) %&gt;%\n  unnest(local_moran) # unnest is to add local_moran into individual columns. without it, it will be a list\n\n# check the output\nglimpse(lisa)\n\n\nRows: 88\nColumns: 21\n$ ii           &lt;dbl&gt; -1.468468e-03, 2.587817e-02, -1.198765e-02, 1.022468e-03,…\n$ eii          &lt;dbl&gt; 0.0017692414, 0.0064149158, -0.0374068734, -0.0000348833,…\n$ var_ii       &lt;dbl&gt; 4.179959e-04, 1.051040e-02, 1.020555e-01, 4.367565e-06, 1…\n$ z_ii         &lt;dbl&gt; -0.15836231, 0.18984794, 0.07956903, 0.50594053, 0.448752…\n$ p_ii         &lt;dbl&gt; 0.874171311, 0.849428289, 0.936580031, 0.612898396, 0.653…\n$ p_ii_sim     &lt;dbl&gt; 0.82, 0.96, 0.76, 0.64, 0.50, 0.82, 0.08, 0.08, 0.02, 0.2…\n$ p_folded_sim &lt;dbl&gt; 0.41, 0.48, 0.38, 0.32, 0.25, 0.41, 0.04, 0.04, 0.01, 0.1…\n$ skewness     &lt;dbl&gt; -0.8122108, -1.0905447, 0.8239085, 1.0401038, 1.6357304, …\n$ kurtosis     &lt;dbl&gt; 0.651875433, 1.889177462, 0.046095140, 1.613439800, 3.960…\n$ mean         &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ median       &lt;fct&gt; High-High, High-High, High-High, High-High, High-High, Hi…\n$ pysal        &lt;fct&gt; Low-High, Low-Low, High-Low, High-High, High-High, High-L…\n$ nb           &lt;nb&gt; &lt;2, 3, 4, 57, 85&gt;, &lt;1, 57, 58, 78, 85&gt;, &lt;1, 4, 5, 85&gt;, &lt;1,…\n$ wt           &lt;list&gt; &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0.2, 0.2, 0.2, 0.2, 0.2&gt;, &lt;0…\n$ NAME_2       &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"C…\n$ ID_3         &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 2…\n$ NAME_3       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ ENGTYPE_3    &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"C…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 23667, 20981, 34592, 24473, 25554, 27137, 63118, 62202, 7…\n$ geometry     &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.228…\n\n\n\nNote that there are 88 rows in the output which represent the number of County in Hunan. This implied that each county have its own statistical value, which highlight that this is a Local Moran for measuring Local Spatial Autocorrelation.\n\nOutput of local_moran: - ii: local moran statistic - eii: expectation of local moran statistic; for local_moran_perm, its the permutation sample means - var_ii: variance of local moran statistic; for local_moran_perm, its the permutation sample standard deviations - z_ii: standard deviation of local moran statistic; for local_moran_perm, its based on permutation sample means and standard deviations - p_ii: p-value of local moran statistic using pnorm(); for local_moran_perm, its using standard deviation based on permutation sample means and standard deviations - p_ii_sim: For local_moran_perm(), rank() and punif() of observed statistic rank for [0, 1] p-values using alternative= - p_folded_sim: the simulation folded [0, 0.5] range ranked p-value source - skewness: For local_moran_perm, the output of e1071::skewness() for the permutation samples underlying the standard deviates - kurtosis: For local_moran_perm, the output of e1071::kurtosis() for the permutation samples underlying the standard deviates.\n\n\n3.2.2 Visualizing The Moran’s I Result\n::: panel-tabset #### ii\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\n\n\n\n\n\np-value\nThe following plot customize the map using common statistically significant alpha value threshold.\n\n\nCode\ntmap_mode(\"plot\")\n\n# Specify breaks and labels for classification\nbreaks &lt;- c(0, 0.01, 0.05, 0.1, 1.01)\nlabels &lt;- c(\"Significant at 1%\", \"Significant at 5%\", \"Significant at 10%\", \"Not Significant\")\n\ntm_shape(lisa) +\n  tm_fill(\"p_ii_sim\", breaks = breaks, labels = labels) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8, legend.outside = TRUE,\n            legend.outside.position = 'right')\n\n\n\n\n\n\n\nii and p-value\n\n\nCode\ntmap_mode(\"plot\")\nmap1 &lt;- tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 0.8)\n\nmap2 &lt;- tm_shape(lisa) +\n  tm_fill(\"p_ii\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\nLISA map\nLocal Indicators of Spatial Association (LISA) map is a categorical map showing outliers and clusters. There are two types of outliers namely: High-Low and Low-High outliers. Likewise, there are two type of clusters namely: High-High and Low-Low clusters. In fact, LISA map is an interpreted map by combining local Moran’s I of geographical areas and their respective p-values.\nIn lisa sf data.frame, we can find three fields contain the LISA categories. They are mean, median and pysal. In general, classification in mean will be used as shown in the code chunk below.\n\n\n\nCode\n# Filter the 'lisa' data frame to include only observations where the p-value ('p_ii') is less than 0.05.\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii &lt; 0.05)\n# Set tmap mode to \"plot\" for plotting maps.\ntmap_mode(\"plot\") \n# Create a map using the 'lisa' data frame.\ntm_shape(lisa) +\n  tm_polygons() + # Add polygon (geographical shape) layer to the map.\n  tm_borders(alpha = 0.5) + # Add borders to the polygons with a specified level of transparency.\n# Create another layer on the map using the filtered 'lisa_sig' data frame.\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") +  # Fill the polygons with color based on the 'mean' variable.\n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "in-class/ice2.html#creating-a-time-series-cube",
    "href": "in-class/ice2.html#creating-a-time-series-cube",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "5.1 Creating a Time Series Cube",
    "text": "5.1 Creating a Time Series Cube\nA spacetime cube, in the context of geospatial analytics, is a data structure where each location has a value for every time index, essentially representing a regular time-series for each location. In ESRI’s terminology, the fundamental component of a spacetime cube is a “bin,” which is a unique combination of a location and time index. Collections of these locations for each time index are termed “time slices,” and the set of bins at each time index for a location form a “bin time-series”. For more details on using sfdep package to create spatio-temporal cube visit this link\nin the following code chunks, these function is used: - spacetime() of sfdep to create an spacetime cube. - is_spacetime_cube() of sfdep package to verify if GDPPC_st is indeed an space-time cube object.\n\n\nCode\n# Create a spacetime object named GDPPC_st using the spacetime function\nGDPPC_st &lt;- spacetime(GDPPC, hunan,\n                      .loc_col = \"County\",\n                      .time_col = \"Year\")\n\n# verify that it is space time cube\nprint(is_spacetime_cube(GDPPC_st))\n\n\n[1] TRUE\n\n\nCode\n# Display a summary of the spacetime object\nprint(glimpse(GDPPC_st))\n\n\nRows: 1,496\nColumns: 3\n$ Year   &lt;dbl&gt; 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 200…\n$ County &lt;chr&gt; \"Longshan\", \"Changsha\", \"Wangcheng\", \"Ningxiang\", \"Liuyang\", \"Z…\n$ GDPPC  &lt;dbl&gt; 3469, 24612, 14659, 11687, 13406, 8546, 10944, 8040, 7383, 1168…\n# A tibble: 1,496 × 3\n    Year County    GDPPC\n * &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1  2005 Longshan   3469\n 2  2005 Changsha  24612\n 3  2005 Wangcheng 14659\n 4  2005 Ningxiang 11687\n 5  2005 Liuyang   13406\n 6  2005 Zhuzhou    8546\n 7  2005 You       10944\n 8  2005 Chaling    8040\n 9  2005 Yanling    7383\n10  2005 Liling    11688\n# ℹ 1,486 more rows\n\n\nThe TRUE return confirms that GDPPC_st object is indeed an time-space cube."
  },
  {
    "objectID": "in-class/ice2.html#computing-gi",
    "href": "in-class/ice2.html#computing-gi",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "5.2 Computing Gi*",
    "text": "5.2 Computing Gi*\nNext, compute the local Gi* statistics. To do it, derive inverse distance weights first.\n\n\nCode\n# Create a neighbors and weights object named DPPC_nb using the GDPPC_st spacetime object\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  # Activate the spatial geometry component of the spacetime object, allowing spatial operations to be performed\n  activate(\"geometry\") %&gt;%\n  # Add a new variable 'nb' representing neighbors and 'wt' representing weights\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry,\n                                  scale = 1,\n                                  alpha = 1),\n         .before = 1) %&gt;%\n  # Set the neighbors using the 'nb' variable\n  set_nbs(\"nb\") %&gt;%\n  # Set the weights using the 'wt' variable\n  set_wts(\"wt\")\n\n# Display a summary of the neighbors and weights object\nglimpse(GDPPC_nb)\n\n\nRows: 1,496\nColumns: 5\n$ Year   &lt;dbl&gt; 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 200…\n$ County &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"Liuya…\n$ GDPPC  &lt;dbl&gt; 8184, 6560, 9956, 8394, 8850, 9244, 13406, 11687, 14659, 7423, …\n$ nb     &lt;list&gt; &lt;1, 2, 3, 4, 57, 85&gt;, &lt;1, 2, 57, 58, 78, 85&gt;, &lt;1, 3, 4, 5, 85&gt;…\n$ wt     &lt;list&gt; &lt;0.00000000, 0.01526149, 0.03515537, 0.02176677, 0.02836978, 0…\n\n\n\n\nCode\nGDPPC_st\n\n\n# A tibble: 1,496 × 3\n    Year County    GDPPC\n * &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1  2005 Longshan   3469\n 2  2005 Changsha  24612\n 3  2005 Wangcheng 14659\n 4  2005 Ningxiang 11687\n 5  2005 Liuyang   13406\n 6  2005 Zhuzhou    8546\n 7  2005 You       10944\n 8  2005 Chaling    8040\n 9  2005 Yanling    7383\n10  2005 Liling    11688\n# ℹ 1,486 more rows\n\n\nCompute Gi* using the following code\n\n\nCode\ngi_stars &lt;- GDPPC_nb %&gt;% \n  group_by(Year) %&gt;% \n  mutate(gi_star = local_gstar_perm(\n    GDPPC, nb, wt)) %&gt;% \n  tidyr::unnest(gi_star)\n\n# check the output\nglimpse(gi_stars)\n\n\nRows: 1,496\nColumns: 13\nGroups: Year [17]\n$ Year         &lt;dbl&gt; 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 2005, 200…\n$ County       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", …\n$ GDPPC        &lt;dbl&gt; 8184, 6560, 9956, 8394, 8850, 9244, 13406, 11687, 14659, …\n$ nb           &lt;list&gt; &lt;1, 2, 3, 4, 57, 85&gt;, &lt;1, 2, 57, 58, 78, 85&gt;, &lt;1, 3, 4, …\n$ wt           &lt;list&gt; &lt;0.00000000, 0.01526149, 0.03515537, 0.02176677, 0.02836…\n$ gi_star      &lt;dbl&gt; 0.39812392, -0.23690950, 1.05308649, 0.96565566, 1.047539…\n$ e_gi         &lt;dbl&gt; 0.011503828, 0.010904067, 0.012643127, 0.011729795, 0.011…\n$ var_gi       &lt;dbl&gt; 2.689913e-06, 2.640805e-06, 3.327364e-06, 3.235001e-06, 3…\n$ p_value      &lt;dbl&gt; 0.382095046, 0.001990885, 0.507080740, 0.920309942, 0.884…\n$ p_sim        &lt;dbl&gt; 0.7023908659, 0.9984115046, 0.6120981684, 0.3574108152, 0…\n$ p_folded_sim &lt;dbl&gt; 0.608, 0.892, 0.528, 0.308, 0.352, 0.920, 0.008, 0.396, 0…\n$ skewness     &lt;dbl&gt; 0.304, 0.446, 0.264, 0.154, 0.176, 0.460, 0.004, 0.198, 0…\n$ kurtosis     &lt;dbl&gt; 0.8925173, 0.8204179, 0.9285558, 1.1852446, 0.8742758, 0.…"
  },
  {
    "objectID": "in-class/ice2.html#mann-kendall-test",
    "href": "in-class/ice2.html#mann-kendall-test",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "5.3 Mann-Kendall Test",
    "text": "5.3 Mann-Kendall Test\nUsing Gi* measures, Mann-Kendall test can be run to valuate each location for a trend\n\n\nCode\ncbg &lt;- gi_stars %&gt;% \n  ungroup() %&gt;% \n  filter(County == \"Changsha\") |&gt; \n  select(County, Year, gi_star)\n\n# check the output\nglimpse(cbg)\n\n\nRows: 17\nColumns: 3\n$ County  &lt;chr&gt; \"Changsha\", \"Changsha\", \"Changsha\", \"Changsha\", \"Changsha\", \"C…\n$ Year    &lt;dbl&gt; 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 20…\n$ gi_star &lt;dbl&gt; 5.028300, 5.169201, 5.295889, 5.603954, 6.278886, 5.935746, 5.…\n\n\nplot the result\n\n\nCode\nggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\n\n\n\n\n\nThe graph shows a time series of the standardized Gi* statistic (gi_star) for Changsha county in Hunan, which is a measure of local spatial association for GDP per capita over time. The trend indicates fluctuations with peaks and troughs, suggesting periods of relatively high and low localized economic performance when compared to the overall spatial-temporal dataset.\n\nAlternatively, create an interactive plot using ggplotly() of plotly package.\n\n\nCode\np &lt;- ggplot(data = cbg, \n       aes(x = Year, \n           y = gi_star)) +\n  geom_line() +\n  theme_light()\n\nggplotly(p)\n\n\n\n\n\n\n\n\nCode\ncbg %&gt;%\n  # Summarize the data using the Mann-Kendall test and store the results in a list named 'mk'\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;% \n  # Unnest the 'mk' list and widen it to create separate columns for each element\n  tidyr::unnest_wider(mk)\n\n\n# A tibble: 1 × 5\n    tau      sl     S     D  varS\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.485 0.00742    66  136.  589.\n\n\nIn the above result, sl is the p-value. This result tells us that there is a slight upward but insignificant trend.\nWe can replicate this for each location by using group_by() of dplyr package.\n\n\nCode\nehsa &lt;- gi_stars %&gt;%\n  # Group the data by 'County'\n  group_by(County) %&gt;%\n  # Summarize the data within each group using the Mann-Kendall test and store the results in a list named 'mk'\n  summarise(mk = list(\n    unclass(\n      Kendall::MannKendall(gi_star)))) %&gt;%\n  # Unnest the 'mk' list and widen it to create separate columns for each element\n  tidyr::unnest_wider(mk)\n\n# check the output\nglimpse(ehsa)\n\n\nRows: 88\nColumns: 6\n$ County &lt;chr&gt; \"Anhua\", \"Anren\", \"Anxiang\", \"Baojing\", \"Chaling\", \"Changning\",…\n$ tau    &lt;dbl&gt; 0.19117649, -0.29411769, 0.00000000, -0.69117653, -0.08823530, …\n$ sl     &lt;dbl&gt; 3.030965e-01, 1.081613e-01, 1.000000e+00, 1.276678e-04, 6.50463…\n$ S      &lt;dbl&gt; 26, -40, 0, -94, -12, -102, 66, -112, -16, 14, -6, -112, -104, …\n$ D      &lt;dbl&gt; 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136, 136…\n$ varS   &lt;dbl&gt; 589.3333, 589.3333, 589.3333, 589.3333, 589.3333, 589.3333, 589…"
  },
  {
    "objectID": "in-class/ice2.html#arrange-to-show-significant-emerging-hotcold-spots",
    "href": "in-class/ice2.html#arrange-to-show-significant-emerging-hotcold-spots",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "5.4 Arrange to show significant emerging hot/cold spots",
    "text": "5.4 Arrange to show significant emerging hot/cold spots\n\n\nCode\nemerging &lt;- ehsa %&gt;% \n  # Arrange the data in ascending order based on the 'sl' column and the absolute value of 'tau' column\n  arrange(sl, abs(tau)) %&gt;%\n  # Extract the top 5 rows after sorting\n  slice(1:5)\n\n# check the output\nglimpse(emerging)\n\n\nRows: 5\nColumns: 6\n$ County &lt;chr&gt; \"Shuangfeng\", \"Xiangtan\", \"Xiangxiang\", \"Chengbu\", \"Dongan\"\n$ tau    &lt;dbl&gt; 0.8676472, 0.8676472, 0.8676472, -0.8235295, -0.8235295\n$ sl     &lt;dbl&gt; 1.430511e-06, 1.430511e-06, 1.430511e-06, 4.822108e-06, 4.82210…\n$ S      &lt;dbl&gt; 118, 118, 118, -112, -112\n$ D      &lt;dbl&gt; 136, 136, 136, 136, 136\n$ varS   &lt;dbl&gt; 589.3333, 589.3333, 589.3333, 589.3333, 589.3333"
  },
  {
    "objectID": "in-class/ice2.html#performing-emerging-hotspot-analysis",
    "href": "in-class/ice2.html#performing-emerging-hotspot-analysis",
    "title": "In-class Exercise 2 - sfdep for Spatial Weights, GLSA & EHSA",
    "section": "5.5 Performing Emerging Hotspot Analysis",
    "text": "5.5 Performing Emerging Hotspot Analysis\nLastly, we will perform EHSA analysis by using emerging_hotspot_analysis() of sfdep package. It takes a spacetime object x (i.e. GDPPC_st), and the quoted name of the variable of interest (i.e. GDPPC) for .var argument. The k argument is used to specify the number of time lags which is set to 1 by default. Lastly, nsim map numbers of simulation to be performed.\n\n\nCode\nehsa &lt;- emerging_hotspot_analysis(\n  x = GDPPC_st,     # Input data: Spacetime object representing data with spatial and temporal dimensions\n  .var = \"GDPPC\",   # Variable of interest for the analysis is \"GDPPC\"\n  k = 1,             # number of time lags to include in the neighborhood for calculating the local Gi*\n  nsim = 99         # determining the number of simulations to calculate simulated p-value for logal Gi*\n)\n\n# check the output\nglimpse(ehsa)\n\n\nRows: 88\nColumns: 4\n$ location       &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\"…\n$ tau            &lt;dbl&gt; 0.22058827, 0.14705884, 0.44117653, -0.82352948, 0.1176…\n$ p_value        &lt;dbl&gt; 2.322488e-01, 4.338268e-01, 1.508367e-02, 4.822108e-06,…\n$ classification &lt;chr&gt; \"sporadic coldspot\", \"sporadic hotspot\", \"oscilating ho…\n\n\nClassifications:\n\nConsecutive Hotspot: A consecutive hotspot in EHSA refers to a spatial and temporal pattern where a specific area consistently exhibits high values over consecutive time periods.\nNo Pattern Detected: When EHSA identifies “no pattern detected,” it indicates that there is no discernible consistent spatial or temporal trend in the analyzed data.\nOscillating Coldspot: An oscillating coldspot in EHSA describes a location that alternates between periods of exhibiting lower values and periods of relative inactivity.\nOscillating Hotspot: An oscillating hotspot in EHSA characterizes a location that alternates between periods of exhibiting higher values and periods of relative inactivity.\nSporadic Coldspot: A sporadic coldspot in EHSA refers to a location that irregularly experiences periods of lower values without a clear, sustained pattern.\nSporadic Hotspot: A sporadic hotspot in EHSA describes a location that irregularly experiences periods of higher values without a clear, sustained pattern.\n\nthe next panel will visualise the result:\n\nDistribution of EHSA classesGeographic EHSA classes distribution\n\n\n\n\nCode\nggplot(data = ehsa,\n       aes(x = classification)) +\n  geom_bar()\n\n\n\n\n\nFigure above shows that sporadic cold spots class has the highest numbers of county.\n\n\n\n\nCode\n# join the dataset\nhunan_ehsa &lt;- hunan %&gt;%\n  left_join(ehsa,\n            by = join_by(County == location))\n\n# plot the map\nehsa_sig &lt;- hunan_ehsa  %&gt;%\n  filter(p_value &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(hunan_ehsa) +\n  tm_polygons() +\n  tm_borders(alpha = 0.5) +\ntm_shape(ehsa_sig) +\n  tm_fill(\"classification\") + \n  tm_borders(alpha = 0.4)"
  },
  {
    "objectID": "hands-on/hoe3.html",
    "href": "hands-on/hoe3.html",
    "title": "Hands-on Exercise 3: Processing and Visualising Flow Data",
    "section": "",
    "text": "Spatial interaction encompasses the dynamics of movement including people, goods, or information between locations in geographical space. This broad concept includes diverse activities such as global trade, transportation schedules, and even pedestrian movements.\nEach spatial interaction involves a discrete origin/destination pair, represented as a cell in a matrix. The matrix, known as an origin/destination matrix or spatial interaction matrix, has rows corresponding to origin locations and columns to destination locations.\nIn this analysis, we’ll construct an OD matrix using the Passenger Volume by Origin Destination Bus Stops dataset obtained from LTA DataMall."
  },
  {
    "objectID": "hands-on/hoe3.html#overview",
    "href": "hands-on/hoe3.html#overview",
    "title": "Hands-on Exercise 3: Processing and Visualising Flow Data",
    "section": "",
    "text": "Spatial interaction encompasses the dynamics of movement including people, goods, or information between locations in geographical space. This broad concept includes diverse activities such as global trade, transportation schedules, and even pedestrian movements.\nEach spatial interaction involves a discrete origin/destination pair, represented as a cell in a matrix. The matrix, known as an origin/destination matrix or spatial interaction matrix, has rows corresponding to origin locations and columns to destination locations.\nIn this analysis, we’ll construct an OD matrix using the Passenger Volume by Origin Destination Bus Stops dataset obtained from LTA DataMall."
  },
  {
    "objectID": "hands-on/hoe3.html#getting-started",
    "href": "hands-on/hoe3.html#getting-started",
    "title": "Hands-on Exercise 3: Processing and Visualising Flow Data",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nWe’ll employ four R packages for this analysis:\n\nsf: For handling and transforming geospatial data.\ntidyverse: For data manipulation and visualization.\ntmap: For creating thematic maps.\n\n\n\nCode\npacman::p_load(tmap, sf, DT, stplanr, performance,\n               ggpubr, tidyverse)\n#pacman::p_load' ensures all specified packages are installed and loaded"
  },
  {
    "objectID": "hands-on/hoe3.html#preparing-the-flow-data",
    "href": "hands-on/hoe3.html#preparing-the-flow-data",
    "title": "Hands-on Exercise 3: Processing and Visualising Flow Data",
    "section": "3 Preparing the Flow Data",
    "text": "3 Preparing the Flow Data\n\nImporting the OD Data\nWe begin by importing the Passenger Volume by Origin Destination Bus Stops dataset using the read_csv() function from the readr package.\n\n\nCode\nodbus202308 &lt;- read_csv(\"../data/aspatial/origin_destination_bus_202308.csv.gz\")\nodbus202308 &lt;- data.frame(lapply(odbus202308, factor))\nodbus202308$TOTAL_TRIPS &lt;- as.numeric(odbus202308$TOTAL_TRIPS)\nodbus202308$TIME_PER_HOUR &lt;- as.numeric(odbus202308$TIME_PER_HOUR)\n# The dataset is converted to a dataframe with appropriate data types\n\n# display the summary of the dataset\nglimpse(odbus202308)\n\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;fct&gt; 2023-08, 2023-08, 2023-08, 2023-08, 2023-08, 2023-…\n$ DAY_TYPE            &lt;fct&gt; WEEKDAY, WEEKENDS/HOLIDAY, WEEKENDS/HOLIDAY, WEEKD…\n$ TIME_PER_HOUR       &lt;dbl&gt; 17, 17, 15, 15, 18, 18, 18, 18, 8, 18, 15, 11, 11,…\n$ PT_TYPE             &lt;fct&gt; BUS, BUS, BUS, BUS, BUS, BUS, BUS, BUS, BUS, BUS, …\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 44069, 20281, 2…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 17229, 20141, 2…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\n\nExtracting the Study Data\nOur focus is on weekday commuting flows between 6 and 9 AM.\n\n\nCode\nodbus6_9 &lt;- odbus202308 %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 & TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n# Filtering and aggregating data for specific time and day\n\n# View the extracted data\ndatatable(odbus6_9)\n\n\n\n\n\n\n\nCode\n# Displaying the data in an interactive table format"
  },
  {
    "objectID": "hands-on/hoe3.html#working-with-geospatial-data",
    "href": "hands-on/hoe3.html#working-with-geospatial-data",
    "title": "Hands-on Exercise 3: Processing and Visualising Flow Data",
    "section": "4 Working with Geospatial Data",
    "text": "4 Working with Geospatial Data\nWe’ll use two geospatial datasets:\n\nBusStop: Locations of bus stops from LTA DataMall as of July 2023.\nMPSZ-2019: URA Master Plan 2019 sub-zone boundaries.\n\n\nImporting Geospatial Data\nThe datasets will be imported as follows:\n\n\nCode\nbusstop &lt;- st_read(dsn = \"../data/geospatial\", layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `BusStop' from data source `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nCode\n# Importing and transforming the BusStop data\n\nmpsz &lt;- st_read(dsn = \"../data/geospatial\", layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\nCode\n# Importing and transforming the MPSZ-2019 data\nmpsz\n\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26...\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nst_read() imports shapefiles into R as sf data frames.\nst_transform() alters the projection to CRS 3414 for uniformity."
  },
  {
    "objectID": "hands-on/hoe3.html#geospatial-data-wrangling",
    "href": "hands-on/hoe3.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 3: Processing and Visualising Flow Data",
    "section": "5 Geospatial Data Wrangling",
    "text": "5 Geospatial Data Wrangling\n\nCombining Busstop and mpsz\nWe’ll merge the planning subzone codes from the mpsz dataset into the busstop dataset.\n\n\nCode\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n# Merging data and retaining essential columns\n\n# Note: Five bus stops outside Singapore's boundary are excluded.\n\n# check the output\ndatatable(busstop_mpsz)\n\n\n\n\n\n\n\nCode\n# Viewing the combined data\n\n\nNow, let’s append the planning subzone codes to the odbus6_9 dataset:\n\n\nCode\nod_data &lt;- left_join(odbus6_9, busstop_mpsz, by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE, ORIGIN_SZ = SUBZONE_C, DESTIN_BS = DESTINATION_PT_CODE)\n# Joining and renaming columns for clarity\n\n# check the data\nglimpse(od_data)\n\n\nRows: 227,177\nColumns: 4\nGroups: ORIGIN_BS [5,005]\n$ ORIGIN_BS &lt;chr&gt; \"01012\", \"01012\", \"01012\", \"01012\", \"01012\", \"01012\", \"01012…\n$ DESTIN_BS &lt;fct&gt; 01112, 01113, 01121, 01211, 01311, 07371, 60011, 60021, 6003…\n$ TRIPS     &lt;dbl&gt; 177, 111, 40, 87, 184, 18, 22, 16, 12, 21, 2, 3, 1, 1, 1, 1,…\n$ ORIGIN_SZ &lt;chr&gt; \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", …\n\n\nChecking for duplicate records is crucial, if duplicates exist, we keep only unique records:\n\n\nCode\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n# Identifying any duplicate records\n\nod_data &lt;- unique(od_data)\n# Retaining only unique records\n\nglimpse(od_data)\n\n\nRows: 226,610\nColumns: 4\nGroups: ORIGIN_BS [5,005]\n$ ORIGIN_BS &lt;chr&gt; \"01012\", \"01012\", \"01012\", \"01012\", \"01012\", \"01012\", \"01012…\n$ DESTIN_BS &lt;fct&gt; 01112, 01113, 01121, 01211, 01311, 07371, 60011, 60021, 6003…\n$ TRIPS     &lt;dbl&gt; 177, 111, 40, 87, 184, 18, 22, 16, 12, 21, 2, 3, 1, 1, 1, 1,…\n$ ORIGIN_SZ &lt;chr&gt; \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", …\n\n\nCode\n# check the data\n\n\nWe’ll now complete the dataset with destination subzone codes:\n\n\nCode\nod_data &lt;- left_join(od_data, busstop_mpsz, by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) %&gt;%\n   rename(DESTIN_SZ = SUBZONE_C)\n# Further enriching the dataset with destination subzone codes\n\nglimpse(od_data)\n\n\nRows: 227,523\nColumns: 5\nGroups: ORIGIN_BS [5,005]\n$ ORIGIN_BS &lt;chr&gt; \"01012\", \"01012\", \"01012\", \"01012\", \"01012\", \"01012\", \"01012…\n$ DESTIN_BS &lt;chr&gt; \"01112\", \"01113\", \"01121\", \"01211\", \"01311\", \"07371\", \"60011…\n$ TRIPS     &lt;dbl&gt; 177, 111, 40, 87, 184, 18, 22, 16, 12, 21, 2, 3, 1, 1, 1, 1,…\n$ ORIGIN_SZ &lt;chr&gt; \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", …\n$ DESTIN_SZ &lt;chr&gt; \"RCSZ10\", \"DTSZ01\", \"RCSZ04\", \"KLSZ09\", \"KLSZ06\", \"KLSZ06\", …\n\n\nCode\n# check the data\n\n\nChecking for and removing any remaining duplicates:\n\n\nCode\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n# Re-checking for duplicates\n\nod_data &lt;- unique(od_data)\n# Ensuring all records are unique\n\nglimpse(od_data)\n\n\nRows: 226,846\nColumns: 5\nGroups: ORIGIN_BS [5,005]\n$ ORIGIN_BS &lt;chr&gt; \"01012\", \"01012\", \"01012\", \"01012\", \"01012\", \"01012\", \"01012…\n$ DESTIN_BS &lt;chr&gt; \"01112\", \"01113\", \"01121\", \"01211\", \"01311\", \"07371\", \"60011…\n$ TRIPS     &lt;dbl&gt; 177, 111, 40, 87, 184, 18, 22, 16, 12, 21, 2, 3, 1, 1, 1, 1,…\n$ ORIGIN_SZ &lt;chr&gt; \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", \"RCSZ10\", …\n$ DESTIN_SZ &lt;chr&gt; \"RCSZ10\", \"DTSZ01\", \"RCSZ04\", \"KLSZ09\", \"KLSZ06\", \"KLSZ06\", …\n\n\nCode\n# Check the data\n\n\nFinally, we’ll prepare the data for visualisation:\n\n\nCode\nod_data &lt;- od_data %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n# Final data preparation step\n\nglimpse(od_data)\n\n\nRows: 20,600\nColumns: 3\nGroups: ORIGIN_SZ [310]\n$ ORIGIN_SZ    &lt;chr&gt; \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01…\n$ DESTIN_SZ    &lt;chr&gt; \"AMSZ01\", \"AMSZ02\", \"AMSZ03\", \"AMSZ04\", \"AMSZ05\", \"AMSZ06…\n$ MORNING_PEAK &lt;dbl&gt; 1866, 8726, 12598, 2098, 7718, 1631, 1308, 2261, 1526, 14…\n\n\nCode\n# check the final data"
  },
  {
    "objectID": "hands-on/hoe3.html#visualising-spatial-interaction",
    "href": "hands-on/hoe3.html#visualising-spatial-interaction",
    "title": "Hands-on Exercise 3: Processing and Visualising Flow Data",
    "section": "6 Visualising Spatial Interaction",
    "text": "6 Visualising Spatial Interaction\nWe’ll now create and visualize desire lines using the stplanr package.\n\nRemoving Intra-zonal Flows\nIntra-zonal flows are not required for our analysis:\n\n\nCode\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ != od_data$DESTIN_SZ,]\n# Removing intra-zonal flows for clarity in visualization\n\nglimpse(od_data1)\n\n\nRows: 20,309\nColumns: 3\nGroups: ORIGIN_SZ [310]\n$ ORIGIN_SZ    &lt;chr&gt; \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01…\n$ DESTIN_SZ    &lt;chr&gt; \"AMSZ02\", \"AMSZ03\", \"AMSZ04\", \"AMSZ05\", \"AMSZ06\", \"AMSZ07…\n$ MORNING_PEAK &lt;dbl&gt; 8726, 12598, 2098, 7718, 1631, 1308, 2261, 1526, 141, 655…\n\n\nCode\n# check the output\n\n\n\n\nCreating Desire Lines\nHere’s how to create desire lines using the od2line() function:\n\n\nCode\nflowLine &lt;- od2line(flow = od_data1, zones = mpsz, zone_code = \"SUBZONE_C\")\n# Generating desire lines between different zones\n\nglimpse(flowLine)\n\n\nRows: 20,309\nColumns: 4\n$ ORIGIN_SZ    &lt;chr&gt; \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01…\n$ DESTIN_SZ    &lt;chr&gt; \"AMSZ02\", \"AMSZ03\", \"AMSZ04\", \"AMSZ05\", \"AMSZ06\", \"AMSZ07…\n$ MORNING_PEAK &lt;dbl&gt; 8726, 12598, 2098, 7718, 1631, 1308, 2261, 1526, 141, 655…\n$ geometry     &lt;LINESTRING [m]&gt; LINESTRING (29501.77 39419...., LINESTRING (29…\n\n\nCode\n# check the output\n\n\n\n\nVisualising the Desire Lines\nTo visualize the lines:\n\n\nCode\n# Enable tmap to automatically check and fix invalid polygons\ntmap_options(check.and.fix = TRUE)\n\n# Now create your plot\ntm_shape(mpsz) +\n  tm_polygons() +\n  flowLine %&gt;%\n  tm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.3)\n\n\n\n\n\nCode\n# Thematic map showing the intensity of commuting flows\n\n\nFocusing on selected flows can be insightful, especially when data are skewed:\n\n\nCode\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\n\n\n\n\nCode\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\n  tm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\", col = \"orange\", style = \"quantile\", scale = c(0.1, 1, 3, 5, 7, 10), n = 6, alpha = 0.3)\n\n\n\n\n\n\n\nCode\n# Filtering and visualizing only significant flows"
  },
  {
    "objectID": "hands-on/hoe2-localautocor.html",
    "href": "hands-on/hoe2-localautocor.html",
    "title": "Hands-on Exercise 2: Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "Zoomed Clusters of Wealthy vs Poor Area - Local Spatial Autocorrelation Illustration"
  },
  {
    "objectID": "hands-on/hoe2-localautocor.html#overview",
    "href": "hands-on/hoe2-localautocor.html#overview",
    "title": "Hands-on Exercise 2: Local Measures of Spatial Autocorrelation",
    "section": "1 Overview",
    "text": "1 Overview\nThis exercise is a follow up of 2B exercise on exploring Global and Local Measures of Spatial Autocorrelation (GLSA) using the spdep package. Specifically, this 2C exercise will focus on Local Measures of Spatial Autocorrelation.\nThe new learning objectives includes: - Compute Local Indicator of Spatial Association (LISA) statistics to detect clusters and outliers. - Compute Getis-Ord’s Gi-statistics to identify hotspots or cold spots. - Visualize the analysis output using the tmap package.\n\nLocal measures of spatial autocorrelation, in contrast, focus on the relationships between each observation and its immediate surroundings. Instead of a single summary statistic, they provide specific scores for each observation, which help to understand more about the spatial structure in the data. These measures can tell us where specific clusters or outliers are located within the map. They are based on the same general principles as global measures but are more detailed, sometimes mathematically related to the global measures. Local Indicators of Spatial Association (LISAs) are a common example of this type of measure, offering a detailed view of spatial patterns and relationships at a local level. summarized from: Rey et al., 2020"
  },
  {
    "objectID": "hands-on/hoe2-localautocor.html#getting-started",
    "href": "hands-on/hoe2-localautocor.html#getting-started",
    "title": "Hands-on Exercise 2: Local Measures of Spatial Autocorrelation",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nload the packages\n\n\nCode\npacman::p_load(sf, spdep, tmap, tidyverse)\n\n\nimport, join, and visualize the data\n\n\nCode\n#import geospatial data\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nCode\n# import aspatial data\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\n\n# perform relational join\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n# visualize the data\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\", n = 5, style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\", main.title.size = 1,\n            main.title.position = \"center\", legend.outside = TRUE,\n            legend.outside.position = \"bottom\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\", n = 5, style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\", main.title.size = 1,\n            main.title.position = \"center\", legend.outside = TRUE,\n            legend.outside.position = \"bottom\")\n\ntmap_arrange(equal, quantile, asp=1, ncol=2)\n\n\n\n\n\ncarry-over important variables from previous exercise\n\n\nCode\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)"
  },
  {
    "objectID": "hands-on/hoe2-localautocor.html#cluster-and-outlier-analysis",
    "href": "hands-on/hoe2-localautocor.html#cluster-and-outlier-analysis",
    "title": "Hands-on Exercise 2: Local Measures of Spatial Autocorrelation",
    "section": "3 Cluster and Outlier Analysis",
    "text": "3 Cluster and Outlier Analysis\nLocal Indicators of Spatial Association (LISA) are statistical tools that help us identify clusters or outliers in the spatial distribution of a variable. Imagine studying cancer rates in different neighborhoods of a city; LISA would reveal whether there are areas with unusually high or low rates, indicating a spatial pattern beyond random chance. In this section, we’ll focus on applying LISA, particularly local Moran’s I, to detect clusters and outliers in the GDP per capita of Hunan Province, China, in 2012.\n\n3.1 Computing and Mapping local Moran’s I\nTo calculate local Moran’s I, we use the localmoran() function of spdep package. This function computes various statistics, such as Ii, E.Ii, Var.Ii, Z.Ii, and Pr(), indicating the local spatial autocorrelation for each region.\n\n\nCode\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\n\nlocalmoran() function returns a matrix of values whose columns are: - Ii: the local Moran’s I statistics - E.Ii: the expectation of local moran statistic under the randomisation hypothesis - Var.Ii: the variance of local moran statistic under the randomisation hypothesis - Z.Ii:the standard deviate of local moran statistic - Pr(): the p-value of local moran statistic\n\nThe following code displays the content of the local Moran matrix by using printCoefmat().\n\n\nCode\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\nlocal Moran’s Ilocal Moran’s I valueslocal Moran’s I p-valuesMapping both local Moran’s I values and p-values\n\n\nTo visualize the local Moran’s I, we append the local Moran’s I dataframe to the hunan SpatialPolygonDataFrame, creating hunan.localMI. We can then use tmap to create choropleth maps.\n\n\nCode\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n\nthe code below uses tmap to plot a choropleth map of local Moran’s I values.\n\n\nCode\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\n\n\n\n\n\nConsidering both Ii values and their p-values is essential. The code chunk below creates a choropleth map of Moran’s I p-values.\n\n\nCode\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"right\")\n\n\n\n\n\n\n\nFor a comprehensive interpretation, it’s beneficial to plot both the local Moran’s I values and their corresponding p-values side by side.\n\n\nCode\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "hands-on/hoe2-localautocor.html#creating-a-lisa-cluster-map",
    "href": "hands-on/hoe2-localautocor.html#creating-a-lisa-cluster-map",
    "title": "Hands-on Exercise 2: Local Measures of Spatial Autocorrelation",
    "section": "4 Creating a LISA Cluster Map",
    "text": "4 Creating a LISA Cluster Map\nThe LISA Cluster Map helps us identify significant spatial clusters by color-coding areas based on their type of spatial autocorrelation. To generate this map, we start by plotting the Moran scatterplot.\n\nA LISA (Local Indicators of Spatial Association) Cluster Map is a tool used in geography and spatial analysis to identify areas of significant spatial autocorrelation in a dataset. It visually represents regions where observed values (like population density, temperature, etc.) are either similar or dissimilar to their neighboring areas. The map highlights clusters of high values near high values (high-high), low values near low values (low-low), and areas where high values are near low values (high-low) and vice versa. This visualization helps to easily identify areas of similar or dissimilar values and understand spatial patterns and relationships within the data. In essence, it’s a way to see where certain types of values are grouped together on a map.\n\n\nMoran scatterplotMoran scatterplot with standardised variableLISA map\n\n\nThe Moran scatterplot illustrates how the values of a variable at each location relate to the average values at neighboring locations. The code below uses moran.plot() from the spdep package to create this plot for the GDPPC.\n\n\nCode\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\nThe plot is divided into four quadrants. The top right quadrant represents areas with high GDPPC surrounded by areas with average GDPPC – these are the high-high locations.\n\n\nTo enhance the analysis, we standardize the GDPPC variable using the scale() function. Then, we plot the Moran scatterplot again.\nThe as.vector() added to the end is to make sure that the data type we get is a vector that map neatly into the dataframe.\n\n\nCode\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\nThe following code prepares the LISA cluster map by categorizing areas into low-low, low-high, high-low, high-high, or insignificant clusters, and plot it into a map.\n\n\nCode\n# Create an empty vector to store the quadrant information for each area\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\n# Calculate the spatially lagged variable (DV) by subtracting the mean from the lagged GDPPC\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)\n\n# Calculate the local Moran's I values (LM_I) by subtracting the mean from the original values\nLM_I &lt;- localMI[,1] - mean(localMI[,1])\n\n# Set the significance level for the local Moran's I\nsignif &lt;- 0.05       \n\n# Categorize areas into different quadrants based on their DV and LM_I values\nquadrant[DV &lt; 0 & LM_I &gt; 0] &lt;- 1  # Low values surrounded by high values (low-high)\nquadrant[DV &gt; 0 & LM_I &lt; 0] &lt;- 2  # High values surrounded by low values (high-low)\nquadrant[DV &lt; 0 & LM_I &lt; 0] &lt;- 3  # Low values surrounded by low values (low-low)\nquadrant[DV &gt; 0 & LM_I &gt; 0] &lt;- 4  # High values surrounded by high values (high-high)\n\n# Identify areas with non-significant Moran's I values and label them as 0\nquadrant[localMI[,5] &gt; signif] &lt;- 0\n\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\n\nWe can also include the local Moran’s I map and p-value map as shown below for easy comparison.\n\n\n\n\n\n\nThe LISA maps show significant spatial clustering of economic activity. Counties with a high GDP per Capita geographically clustered together indicates positive spatial autocorrelation. Conversely, areas with low Moran’s I values (orange) indicate counties with lower GDP per Capita that may be surrounded by higher-income counties. But, previous data processing section shows that some of the negative area might actually be richer, but the technical calculation averaged it down."
  },
  {
    "objectID": "hands-on/hoe2-localautocor.html#hot-spot-and-cold-spot-area-analysis",
    "href": "hands-on/hoe2-localautocor.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands-on Exercise 2: Local Measures of Spatial Autocorrelation",
    "section": "5 Hot Spot and Cold Spot Area Analysis",
    "text": "5 Hot Spot and Cold Spot Area Analysis\nIn addition to finding clusters and outliers, localized spatial statistics can help identify hot spot and cold spot areas. The term ‘hot spot’ generally refers to a region or value that is higher relative to its surroundings. The Getis and Ord’s G-statistics is a spatial statistical method that detects spatial anomalies by examining neighbors within a specified proximity to identify where either high or low values cluster spatially. Statistically significant hot spots are areas with high values where neighboring areas also exhibit high values. The analysis involves three steps: deriving a spatial weight matrix, computing Gi statistics, and mapping Gi statistics.\n\n5.1 Deriving distance-based weight matrix\nInitially, we need to define new neighbors based on distance rather than shared borders, which was considered in spatial autocorrelation. Two types of distance-based proximity matrices are fixed distance weight matrix and adaptive distance weight matrix.\n\nDeriving the centroidDetermine the cut-off distanceComputing fixed distance weight matrix\n\n\nFirst, we need points associated with each polygon, and to achieve this, we use the st_centroid() function. Longitude and latitude values are extracted separately and combined into a coordinate matrix.\n\n\nCode\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n\nWe determine the upper limit for the distance band by finding the k nearest neighbors for each point and calculating the distance between them.\n\n\nCode\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary indicates the largest first nearest neighbor distance is 61.79 km, setting this as the upper threshold ensures all units have at least one neighbor.\n\n\nNow, we calculate the distance weight matrix bt using a fixed distance of 62 km and dnearneigh() function\n\n\nCode\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nThen, we convert the neighbor object into a spatial weights object named wm62_lw using nb2listw().\n\n\nCode\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\n\n\n\n5.2 Computing adaptive distance weight matrix\nFixed distance weight matrices tend to assign more neighbors to densely settled areas and fewer neighbors to less densely settled areas. To control the number of neighbors directly, we use k-nearest neighbors and specify k.\n\n\nCode\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nThen, we convert the neighbor object into a spatial weights object\n\n\nCode\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "hands-on/hoe2-localautocor.html#computing-gi-statistics",
    "href": "hands-on/hoe2-localautocor.html#computing-gi-statistics",
    "title": "Hands-on Exercise 2: Local Measures of Spatial Autocorrelation",
    "section": "6 Computing Gi statistics",
    "text": "6 Computing Gi statistics\nGi statistics help us find areas with unusually high or low values compared to their neighbors\n\n\n6.1 Gi statistics using fixed distance\nThe first code chunk computes Gi statistics using a fixed distance weight matrix (wm62_lw). The resulting Gi values are Z-scores, where greater values indicate stronger clustering intensity. We then join these Gi values to the original data frame for visualization.\n\n\nCode\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe Gi statistics, represented as Z-scores, are now linked to the original data frame. We visualize these values on a map using a choropleth map, where colors indicate the intensity and direction of clustering.\n\n\nCode\n# Combine the Gi statistics with the original data frame 'hunan'\n# Convert the Gi vector ('gi.fixed') to a matrix and bind it to the original data frame ('hunan')\n# Rename the newly added column to 'gstat_fixed'\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n# Create a quick thematic map (qtm) of the original data frame ('hunan') for GDP per capita ('GDPPC')\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\n# Create a thematic map (Gimap) for Gi statistics using the 'tm' package\n# Shape the map based on the combined data frame ('hunan.gi')\nGimap &lt;-tm_shape(hunan.gi) +\n  # Fill the map with colors based on 'gstat_fixed' column\n  tm_fill(col = \"gstat_fixed\", \n          # Use the 'pretty' style for color breaks\n          style = \"pretty\",\n          # Choose the RdBu color palette\n          palette=\"-RdBu\",\n          # Set the title for the legend\n          title = \"local Gi\") +\n  # Add semi-transparent borders to map features\n  tm_borders(alpha = 0.5)\n\n# Arrange the original GDP per capita map and the Gi map side by side\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\nThe Gi* map (Getis-Ord Gi* statistic) for the GDP per Capita of counties in Hunan, China, suggests spatial hotspots and cold spots of economic wealth. The map on the right indicates areas with high Gi* values (in red), revealing clusters of counties with significantly high GDP per Capita—these are the hotspots of economic activity. Conversely, the blue areas represent cold spots with low Gi* values, indicating clusters of counties with low GDP per Capita. Notably, the central to northeast region shows a strong concentration of wealth with a Gi* value between 5 to 6, indicating a very high degree of clustering of economic affluence. This pattern highlights the unequal distribution of economic wealth across the region, with certain areas being significantly more prosperous than others, which can inform targeted economic development and policy intervention strategies.\n\n\n\n6.2 Gi statistics using adaptive distance\nNow, we repeat the process using an adaptive distance weight matrix (knn_lw). This allows us to consider different distances for each location based on their neighbors, providing a more flexible approach.\n\n\nCode\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\nThe Gi values based on adaptive distance weights are joined to the original data frame. Again, we visualize these values on a choropleth map to observe patterns.\n\n\nCode\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\n\n\nThe Gi map with adaptive distance shows a clear delineation of economic clustering across the Hunan area. Compared to the previous fixed distance Gi map, the adaptive distance approach may provide a more nuanced view of spatial relationships, adjusting for varying densities of counties. This can highlight economic disparities more precisely, as the fixed distance may overemphasize or underrepresent clustering due to uniform application of distance across varied geographical spaces. The adaptive map might therefore offer a more accurate representation of spatial economic patterns, potentially revealing more localized clusters that are not as apparent with a fixed distance approach."
  },
  {
    "objectID": "hands-on/hoe1.html",
    "href": "hands-on/hoe1.html",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "",
    "text": "Data Wrangling Illustration"
  },
  {
    "objectID": "hands-on/hoe1.html#overview",
    "href": "hands-on/hoe1.html#overview",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "1 Overview",
    "text": "1 Overview\nThis hands-on exercise is about importing and wrangling geospatial data using appropriate R packages."
  },
  {
    "objectID": "hands-on/hoe1.html#getting-started",
    "href": "hands-on/hoe1.html#getting-started",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "2 Getting Started",
    "text": "2 Getting Started\nThe code chunk below install and load sf and tidyverse packages into R environment.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "hands-on/hoe1.html#importing-geospatial-data",
    "href": "hands-on/hoe1.html#importing-geospatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "3 Importing Geospatial Data",
    "text": "3 Importing Geospatial Data\nThe data import process uses a tool called st_read. It is a function to read different types of maps, in the format/extension such as .shp, .dbf, .prj, and .shx. The function use the following parameters:\n\nLocation Instruction (dsn Parameter): This part is specifying where to find the map files. In our case, the maps are in a folder called “../data/geospatial.”\nLayer Instruction (layer Parameter): This part is specifying focus on a specific aspect of the maps. Think of the maps as a big book, and a layer is like a section that talks about a particular topic. In our example, we’re interested in a section named “MP14_SUBZONE_WEB_PL,” which contains information about areas called subzones.\n\n\nPolygon Data in Shapefile FormatPolyline Data in Shapefile FormGIS data in KML format\n\n\n\nmpsz &lt;- st_read(dsn = \"../data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\nShapefiles are a common geospatial vector data format used to represent geographic features such as points, lines, and polygons. In this case, MP14_SUBZONE_WEB_PL is a layer within the shapefile containing polygon features, which could represent, for example, subzones in a geographic region. The Master Plan 2014 Subzone Boundary (Web) data is a forward looking guiding plan for Singapore’s development in the medium term over the next 10 to 15 years Development Master Plan 2014. Subzones are divisions within a planning area which are usually centred around a focal point such as neighbourhood centre or activity node. There can be more than 10 subzones within a Planning Area. The data is sourced from Singapore Government\n\n\n\n\ncyclingpath &lt;- st_read(dsn = \"../data/geospatial\", layer = \"CyclingPathGazette\")\n\nReading layer `CyclingPathGazette' from data source \n  `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\n\nThis code imports polyline feature data from a shapefile. Polylines are sequences of connected straight lines and are commonly used to represent linear features such as roads, rivers, or cycling paths. In this case, the data are line representations of an intra-town path around Singapore designated for cyclists, excluding park connectors. The data is sourced from Land Transport Authority\n\n\n\n\npreschool &lt;- st_read(dsn = \"../data/geospatial/PreSchoolsLocation.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\ameernoor\\ISSS624\\data\\geospatial\\PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\n\nThis code import GIS (Geographic Information System) in KML format. KML (Keyhole Markup Language) is an XML-based format often used for expressing geographic annotation and visualization within Internet-based, two-dimensional maps and three-dimensional Earth browsers. In this example, the code imports geospatial data representing the location of pre-schools (childcare centres and kindergartens) around Singapore from a KML file. The data is sourced from Singapore Government"
  },
  {
    "objectID": "hands-on/hoe1.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "hands-on/hoe1.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "4 Checking the Content of A Simple Feature Data Frame",
    "text": "4 Checking the Content of A Simple Feature Data Frame\nWhen working with a geospatial data frame like ‘mpsz’ (or any dataset in general), it’s essential to understand its structure and content. The following codes are for checking and understanding the data:\n\nExtracting Geometric Information with st_geometry.Overview of Data Structure with ‘glimpse’Previewing Data with ‘head’\n\n\nThe st_geometry function is used to extract the geometric information (shapes) from the mpsz (Master Plan Subzone Boundary 2014) feature data frame.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n\nFrom the output of the code, it can be summarized that:\n\nThe dataset contains 323 features, each representing a geographic entity. The geometry type used is MULTIPOLYGON, indicating that these features consist of multiple connected polygons.\nThe dimension is XY, implying that the geometry is represented in a two-dimensional space with X and Y coordinates.\nThe bounding box provides the spatial extent of the dataset which includes xmin (minimum X-coordinate), ymin (minimum Y-coordinate), xmax (maximum X-coordinate), and ymax (maximum Y-coordinate)\nProjection Information: The data is projected in the SVY21 coordinate reference system (CRS). SVY21 is a coordinate system used in Singapore for accurate spatial representation.\nFirst 5 Geometries: The output displays the geometries for the first 5 features in the dataset, each represented as a MULTIPOLYGON.\n\n\n\n\nThe glimpse function is employed to obtain a quick overview of the structure and content of the ‘mpsz’ data frame, offering insights into its shape, variables and data types.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\n\nFrom the output of the code, it can be summarized that the dataset contains 323 row and 16 columns, with various data type including integer (int), characters/string (chr), date, double-precision floating-point/64bit float (dbl), and multipolygon\n\n\n\nThe head function is utilized to display the initial 5 rows of the ‘mpsz’ data frame, providing a glimpse of its data values. The n=5 parameter specifies the number of rows to be shown (in this case, the first 5 rows).\n\nhead(mpsz, n=5)\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\nThe output describes the data as a Simple feature collection with 5 features and 15 fields. Note that the features will change in accordance with the numer in the ‘n’ parameter changed. Field represents the number of columns in the dataset. Note that it only count 15 columns in the dataset as opposed to 16 columns in ‘glimpse’ function. It is because of the ‘geometry’ column is not counted as a ‘geometry’ column is not counted as a ‘simple feature’."
  },
  {
    "objectID": "hands-on/hoe1.html#visualizing-the-geospatial-data-on-chartplot",
    "href": "hands-on/hoe1.html#visualizing-the-geospatial-data-on-chartplot",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "5 Visualizing the Geospatial Data on Chart/Plot",
    "text": "5 Visualizing the Geospatial Data on Chart/Plot\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. THe following visualization use plot function from sf library. Note that ‘plot()’ function is mean for plotting the geospatial object for quick look. For high cartographic quality plot, other R package such as tmap should be used.\n\nPlotting All FeaturesPlotting Only the Geometric ShapesPlotting Based on a Specific Attribute\n\n\nThe plot function is used to visualize all features in the ‘mpsz’ dataset. The max.plot parameter limits the display to a maximum of 15 features\n\nplot(mpsz, max.plot = 15)\n\n\n\n\n\n\nHere, the plot function is applied to display only the geometric shapes from the ‘mpsz’ dataset. The st_geometry function extracts the geometries, and the plot focuses solely on the spatial representation.\n\nplot(st_geometry(mpsz))\n\n\n\n\n\n\nThis code utilizes the plot function to visualize features from the ‘mpsz’ dataset based on the attribute “PLN_AREA_N.” The resulting plot highlights spatial distributions based on the specified attribute, providing insights into the geographic distribution of the selected feature.\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "hands-on/hoe1.html#working-with-projection",
    "href": "hands-on/hoe1.html#working-with-projection",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "6 Working with Projection",
    "text": "6 Working with Projection\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system. In this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n6.1 Assigning EPSG code to a simple feature data frame\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\n\nChecking default EPSG CodeCorrecting EPSG CodeChecking Correction Result\n\n\nThe st_crs is a function to retrieve coordinate reference system from sf or sfc object. In this case, it is used to obtain the current EPSG code of the ‘mpsz’ dataset, providing information about its current coordinate reference system. The EPSG code (European Petroleum Survey Group) is a standardized identifier used to uniquely reference a coordinate reference system.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414. This code assigns the EPSG code 3414 to the ‘mpsz’ dataset, ensuring that it adheres to the SVY21 coordinate reference system (EPSG 3414) for accurate spatial representation.\n\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\n\n\nThe following code is to re-run st_crs to verify that the correction was successful by displaying the updated EPSG code (EPSG 3414) of the ‘mpsz3414’ dataset.\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\n\n\n6.2 Transforming the projection of preschool from wgs84 to svy21\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements. This following code utilizes st_transform to convert the projection of the preschool dataset from the WGS84 coordinate system to the SVY21 coordinate system (EPSG 3414). This transformation ensures compatibility with other spatial data in the SVY21 projection.\n\npreschool3414 &lt;- st_transform(preschool, crs = 3414)"
  },
  {
    "objectID": "hands-on/hoe1.html#importing-and-converting-an-aspatial-data",
    "href": "hands-on/hoe1.html#importing-and-converting-an-aspatial-data",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "7 Importing and Converting an Aspatial Data",
    "text": "7 Importing and Converting an Aspatial Data\nIn practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points. In this section, you will learn how to import an aspatial data into R environment and save it as a tibble data frame. Next, you will convert it into a simple feature data frame.\n\nImporting the aspatial dataCreating a simple feature data frame from an aspatial data frame\n\n\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\nlistings &lt;- read_csv(\"../data/aspatial/listings.csv\")\n\nDisplaying the Aspatial Data This code displays the content of the ‘listings’ data frame, providing a preview of its structure and values.\n\nlist(listings)\n\n[[1]]\n# A tibble: 3,483 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,473 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n\nThis code converts the listings data frame into a simple feature data frame named listings_sf. It assigns coordinates and transforms the CRS to SVY21 (EPSG 3414) for accurate spatial representation\n\nlistings_sf &lt;- st_as_sf(listings, coords = c(\"longitude\", \"latitude\"), crs=4326) %&gt;% st_transform(crs = 3414)\n\n\nThings to learn from the arguments above: - coords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates. - crs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io. - %&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nexamine the content using glimpse function\n\nglimpse(listings_sf)\n\nRows: 3,483\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 55, 69, 220, 85, 75, 45, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 20, 24, 47, 22, 17, 12, 133, 18, 6, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.14, 0.16, 0.31, 0.17, 0.12, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52, 52, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 89, 89, 89, 275, 274, 89, 365, 365, 365…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 3, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "hands-on/hoe1.html#geoprocessing-with-sf-package",
    "href": "hands-on/hoe1.html#geoprocessing-with-sf-package",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "8 Geoprocessing with sf package",
    "text": "8 Geoprocessing with sf package\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions. In this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\nBufferingPoint-in-polygon count\n\n\nThe scenario: The authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to\nThe solution: determine the extend of the land need to be acquired and their total area. This code calculates 5-meter buffers around cycling paths and stores the result in the ‘buffer_cycling’ dataset.\n\nbuffer_cycling &lt;- st_buffer(cyclingpath, dist=5, nQuadSegs = 30)\n\ncalculate the area of buffers The code adds a new column, ‘AREA,’ to the ‘buffer_cycling’ dataset, containing the calculated area of each buffer.\n\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\nderive the total land involved\n\nsum(buffer_cycling$AREA)\n\n1774367 [m^2]\n\n\n\n\nThe scenario A pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\n** The solution: The code counts the number of pre-schools within each Planning Subzone using the st_intersects() function and updates the ‘PreSch Count’ column in the ‘mpsz3414’ dataset. Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\nmpsz3414$'PreSch Count'&lt;- lengths(st_intersects(mpsz3414, preschool3414)) \n\nchecking the summary statistics\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nchecking the subzone with most number of pre-school To list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\ncalculating the density of pre-school by planning subzone This code calculates the area of each Planning Subzone and adds a new column, ‘Area,’ to the ‘mpsz3414’ dataset.\n\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\ncompute pre-schoold density The code computes the density of pre-schools per square kilometer for each Planning Subzone, providing a measure of the concentration of educational facilities in different areas.\n\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "hands-on/hoe1.html#exploratory-data-analysis-eda",
    "href": "hands-on/hoe1.html#exploratory-data-analysis-eda",
    "title": "Hands-on Exercise 1: Geospatial Data Wrangling with R",
    "section": "9 Exploratory Data Analysis (EDA)",
    "text": "9 Exploratory Data Analysis (EDA)\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\n\nPlotting histogram for PreSch DensityPlot better histogram using GG PlotPlot a scatterplot using GGPlot\n\n\nThe histogram visualizes the distribution of pre-school density across different Planning Subzones, providing insights into the variation and concentration of pre-schools in Singapore.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\n\nThis GG Plot-generated histogram offers a more detailed view of pre-school density, allowing for a nuanced exploration of the distribution in Planning Subzones. The chart provides additional context on the prevalence of single pre-school areas versus those with higher concentrations.\n\n\nCode\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n\nThis scatterplot employs GG Plot to illustrate the relationship between pre-school density and count in different Planning Subzones. It helps identify patterns, clusters, or outliers, facilitating a comprehensive understanding of the distribution and concentration of pre-schools. The chart is limited to a specific range for clarity in visualization.\n\n\nCode\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "data/geospatial/MPSZ-2019.html",
    "href": "data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 - Applied Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "hands-on/hoe1-choropleth.html",
    "href": "hands-on/hoe1-choropleth.html",
    "title": "Hands-on Exercise 1: Choropleth Mapping",
    "section": "",
    "text": "Painting Choropleth Map Illustration"
  },
  {
    "objectID": "hands-on/hoe1-choropleth.html#overview",
    "href": "hands-on/hoe1-choropleth.html#overview",
    "title": "Hands-on Exercise 1: Choropleth Mapping",
    "section": "1 Overview",
    "text": "1 Overview\nIn this hands-on exercise, we will continue on Exploratory Data Analysis, specifically using Choropleth Mapping\nChoropleth mapping is a way to represent regions, like countries or states, by using patterns or colors to show different values. For instance, a social scientist might use a choropleth map to display where the older population is located in Singapore based on the Master Plan 2014 Subzone Boundary.\nIn this chapter, you’ll discover how to create accurate and meaningful choropleth maps using an R package called tmap."
  },
  {
    "objectID": "hands-on/hoe1-choropleth.html#import-the-libraries",
    "href": "hands-on/hoe1-choropleth.html#import-the-libraries",
    "title": "Hands-on Exercise 1: Choropleth Mapping",
    "section": "2 Import The Libraries",
    "text": "2 Import The Libraries\nThe code chunk below install and load sf, tidyverse and tmap packages into R environment.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "hands-on/hoe1-choropleth.html#importing-the-data",
    "href": "hands-on/hoe1-choropleth.html#importing-the-data",
    "title": "Hands-on Exercise 1: Choropleth Mapping",
    "section": "3 Importing The Data",
    "text": "3 Importing The Data\nWe’ll use two sets of information to make the choropleth map:\n\nMaster Plan 2014 Subzone Boundary (Web): This is a map file that shows the shape of different areas in Singapore, specifically at the planning subzone level. The data can be downloaded from Singapore Government\nSingapore Residents Data (June 2011-2020): This is a list of information about people living in Singapore, like how many people are in different age groups, their gender, and the type of homes they live in. This data is in a CSV file (respopagesextod2011to2020.csv). The data can be downloaded from the Department of Statistics, Singapore. Even though it doesn’t have actual location coordinates, it has fields called PA and SZ that can help match it to the shapes in the MP14_SUBZONE_WEB_PL file.\n\n\nGeospatial Data (Subzone Boundary)Attribute Data\n\n\nThe code below does the following 1. uses the st_read() function from the sf package to bring in the MP14_SUBZONE_WEB_PL shapefile into R, and import it as a simple feature data frame named mpsz. 2. display the data frame by calling mpsz\n\nmpsz &lt;- st_read(dsn = \"../data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\nNote that only the first ten records are displayed. By default, R shows a summary of only the first few rows to minimize resource usage and avoid overwhelming the user. To see more rows, you can use functions like head() and specify the n parameter, e.g. head(mpsz, n = 20) to display the first 20 rows.\n\n\n\nNext, we’re going to bring in the respopagsex2011to2020.csv file into RStudio and store it in a data table named popdata. We’ll do this using the read_csv() function from the readr package, as you can see in the code snippet below.\n\npopdata &lt;- read_csv(\"../data/aspatial/respopagesextod2011to2020.csv.gz\")"
  },
  {
    "objectID": "hands-on/hoe1-choropleth.html#data-preparation",
    "href": "hands-on/hoe1-choropleth.html#data-preparation",
    "title": "Hands-on Exercise 1: Choropleth Mapping",
    "section": "4 Data Preparation",
    "text": "4 Data Preparation\nBefore making a special map, you need to create a table with data for the year 2020. This table should have information about different areas (PA, SZ) and various age groups like YOUNG (0-4 to 20-24), ECONOMY ACTIVE (25-29 to 60-64), AGED (65 and above), TOTAL (all age groups), and DEPENDENCY (the ratio of young and aged people to the economy-active group).\n\nData WranglingJoining the attribute data and geospatial data\n\n\nWe’ll be using some functions to shape our data the way we want: - pivot_wider() from tidyr package - mutate(), filter(), group_by(), and select() from dplyr package\nThe code will do the following steps in order: - Filter the data: It only keeps the rows where the Time column is 2020. - Group the data: It groups the data by PA (Planning Area), SZ (Subzone), and AG (Age Group). - Summarize the data: It calculates the sum of the Pop column for each group. - Reshape the data: It spreads the data wide, turning the Age Group values into separate columns. - Create new columns: It calculates the YOUNG, ECONOMY ACTIVE, AGED, TOTAL, and DEPENDENCY values based on the grouped and summarized data. - Select the columns: It picks the specific columns to be kept in the final data table.\n\n\nCode\n# Filter the 'popdata' dataset for the year 2020\npopdata2020 &lt;- popdata %&gt;%\n\n  # Keep only records where 'Time' is equal to 2020\n  filter(Time == 2020) %&gt;%\n\n  # Group the data by 'PA', 'SZ', and 'AG'\n  group_by(PA, SZ, AG) %&gt;%\n\n  # Summarize the data by calculating the sum of 'Pop' for each group\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n\n  # Ungroup the data to remove grouping constraints\n  ungroup() %&gt;%\n\n  # Reshape the data by widening it using 'AG' as column names and 'POP' as values\n  pivot_wider(names_from = AG, values_from = POP) %&gt;%\n\n  # Create a new column 'YOUNG' by summing specific columns\n  mutate(YOUNG = rowSums(.[3:6]) + rowSums(.[12])) %&gt;%\n\n  # Create a new column 'ECONOMY ACTIVE' by summing specific columns\n  mutate(`ECONOMY ACTIVE` = rowSums(.[7:11]) + rowSums(.[13:15])) %&gt;%\n\n  # Create a new column 'AGED' by summing specific columns\n  mutate(`AGED` = rowSums(.[16:21])) %&gt;%\n\n  # Create a new column 'TOTAL' by summing specific columns\n  mutate(`TOTAL` = rowSums(.[3:21])) %&gt;%\n\n  # Create a new column 'DEPENDENCY' by calculating a ratio\n  mutate(`DEPENDENCY` = (`YOUNG` + `AGED`) / `ECONOMY ACTIVE`) %&gt;%\n\n  # Select specific columns for the final dataset\n  select(`PA`, `SZ`, `YOUNG`, `ECONOMY ACTIVE`, `AGED`, `TOTAL`, `DEPENDENCY`)\n\n# Display a summary of the 'popdata2020' dataset\nglimpse(popdata2020)\n\n\nRows: 332\nColumns: 7\n$ PA               &lt;chr&gt; \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio\", \"Ang Mo Kio…\n$ SZ               &lt;chr&gt; \"Ang Mo Kio Town Centre\", \"Cheng San\", \"Chong Boon\", …\n$ YOUNG            &lt;dbl&gt; 1440, 6640, 6150, 5540, 2100, 3960, 2220, 4690, 0, 12…\n$ `ECONOMY ACTIVE` &lt;dbl&gt; 2610, 15460, 13950, 12090, 3410, 8420, 4200, 11450, 0…\n$ AGED             &lt;dbl&gt; 760, 6050, 6470, 5120, 1310, 3610, 1530, 5100, 0, 750…\n$ TOTAL            &lt;dbl&gt; 4810, 28150, 26570, 22750, 6820, 15990, 7950, 21240, …\n$ DEPENDENCY       &lt;dbl&gt; 0.8429119, 0.8208279, 0.9046595, 0.8817204, 1.0000000…\n\n\nExport the Dataset\n\n\nCode\n# Install and load the 'writexl' package if not already installed\nif (!requireNamespace(\"writexl\", quietly = TRUE)) {\n  install.packages(\"writexl\")\n}\n\n# Load the 'writexl' package\nlibrary(writexl)\n\n# Export 'popdata2020' dataset to an Excel file named 'popdata2020.xlsx'\nwrite_xlsx(popdata2020, path = \"../data/aspatial/popdata2020.xlsx\")\n\n\n\n\nBefore we can combine our geographic and population data, we need to make sure the values in the PA and SZ fields are all in uppercase. This is because these values have a mix of upper- and lowercase, while SUBZONE_N and PLN_AREA_N are all in uppercase.\nthe following code will change the values in the PA and SZ columns to uppercase. After that, it will filters out rows where the ECONOMY ACTIVE column is greater than 0.\n\n\nCode\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\nglimpse(popdata2020)\n\n\nRows: 234\nColumns: 7\n$ PA               &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO…\n$ SZ               &lt;chr&gt; \"ANG MO KIO TOWN CENTRE\", \"CHENG SAN\", \"CHONG BOON\", …\n$ YOUNG            &lt;dbl&gt; 1440, 6640, 6150, 5540, 2100, 3960, 2220, 4690, 1220,…\n$ `ECONOMY ACTIVE` &lt;dbl&gt; 2610, 15460, 13950, 12090, 3410, 8420, 4200, 11450, 2…\n$ AGED             &lt;dbl&gt; 760, 6050, 6470, 5120, 1310, 3610, 1530, 5100, 750, 4…\n$ TOTAL            &lt;dbl&gt; 4810, 28150, 26570, 22750, 6820, 15990, 7950, 21240, …\n$ DEPENDENCY       &lt;dbl&gt; 0.8429119, 0.8208279, 0.9046595, 0.8817204, 1.0000000…\n\n\nNow, we’re using left_join() from the dplyr package to connect our geographical data and the population attribute table. This connection is made using planning subzone names, specifically SUBZONE_N in the geographical data and SZ in the attribute table, as the common identifier.\n\n\nCode\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\nglimpse(mpsz_pop2020)\n\n\nRows: 323\nColumns: 22\n$ OBJECTID         &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16…\n$ SUBZONE_NO       &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, …\n$ SUBZONE_N        &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERS…\n$ SUBZONE_C        &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BM…\n$ CA_IND           &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\"…\n$ PLN_AREA_N       &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT M…\n$ PLN_AREA_C       &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\",…\n$ REGION_N         &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\",…\n$ REGION_C         &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC          &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13…\n$ FMEL_UPD_D       &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014…\n$ X_ADDR           &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 253…\n$ Y_ADDR           &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 299…\n$ SHAPE_Leng       &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 442…\n$ SHAPE_Area       &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.4…\n$ PA               &lt;chr&gt; NA, \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\", \"BUKI…\n$ YOUNG            &lt;dbl&gt; NA, 1200, 0, 3150, 2900, 3340, 3130, 0, 1290, 50, NA,…\n$ `ECONOMY ACTIVE` &lt;dbl&gt; NA, 2860, 40, 6900, 6020, 6800, 7700, 50, 2600, 140, …\n$ AGED             &lt;dbl&gt; NA, 2120, 10, 3320, 1740, 3420, 3610, 10, 610, 60, NA…\n$ TOTAL            &lt;dbl&gt; NA, 6180, 50, 13370, 10660, 13560, 14440, 60, 4500, 2…\n$ DEPENDENCY       &lt;dbl&gt; NA, 1.1608392, 0.2500000, 0.9376812, 0.7707641, 0.994…\n$ geometry         &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOL…\n\n\n\nleft_join() is used with mpsz simple feature data frame as the left data table to ensure that the output will be a simple features data frame.\n\nLastly, the write_rds() function is used to save our combined data (stored in the mpsz_pop2020 data frame) into an RDS file.\n\nwrite_rds(mpsz_pop2020, \"../data/rds/mpszpop2020.rds\")\n\n\nAn RDS file is a binary file format used in R to store single R objects. It stands for R Data Store. This file format is efficient for saving and loading R objects because it preserves the object’s structure, including its data type, attributes, and metadata. Unlike other formats like CSV or Excel, RDS files are tailored for R-specific objects and are typically smaller in size. When you save an object as an RDS file, you can later load it back into R using the read_rds() function to retrieve the exact R object with all its properties intact. It’s a handy way to store and share R data without losing any of the specific characteristics of the objects."
  },
  {
    "objectID": "hands-on/hoe1-choropleth.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "hands-on/hoe1-choropleth.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands-on Exercise 1: Choropleth Mapping",
    "section": "5 Choropleth Mapping Geospatial Data using tmap",
    "text": "5 Choropleth Mapping Geospatial Data using tmap\nThere are two ways to make a thematic map using tmap: - Quick Approach: Use qtm() to swiftly draw a choropleth map. - Customizable Approach: Create a highly customizable thematic map by using tmap elements.\n\n5.1 Plotting choropleth map using qtm\nThe fastest way to draw a choropleth map using tmap is with qtm(). It’s straightforward and produces a solid default visualization in many cases.\nThe following code snippet will generate a standard choropleth map.\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, fill = \"DEPENDENCY\")\n\n\n\n\n\n\n5.2 Creating a choropleth map by using tmap’s elements\nDespite its quick and easy way of making a choropleth map, the limitation of using qtm() is that it makes it challenging to control the appearance of individual map layers. For a high-quality cartographic choropleth map, it’s better to use tmap’s drawing elements.\nThe next code will do the following steps: - tm_shape(): This sets the spatial object (mpsz_pop2020) to be used in the map.\n\ntm_fill(): It fills the polygons with colors based on the “DEPENDENCY” column, using the quantile method and a blue color palette.\ntm_layout(): Defines the layout elements, including the main title, legend settings, frame, and other stylistic elements.\ntm_borders(), tm_compass(), tm_scale_bar(), tm_grid(): These add map embellishments such as borders, compass, scale bar, and grid.\ntm_credits(): Adds a text credit at the bottom left of the map, mentioning the data sources.\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\neach of the tmap functions that are used to create the plot can be seen in the following panel.\n\nbase maptm_polygonstm_filltm_border\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\nThe default interval binning used to draw the choropleth map is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer.\nBy default, Missing value will be shaded in grey.\n\n\n\n\nwithout setting the border, the planning subzones will not have any boundary if the dependency value is the same\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\n\nParameters of tm_border(): - alpha = transparency. the default value is 1 (not transparent) - col = border colour, - lwd = border line width. The default is 1, and - lty = border line type. The default is “solid”.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n\n\n5.3 Data Classification methods of tmap\nChoropleth maps usually use different ways to group data, and the goal is to organize a bunch of observations into specific ranges or groups.\ntmap offers ten methods to classify data, including fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo pick a data classification method, you use the style argument in tm_fill() or tm_polygons().\n\n5.3.1 Plotting choropleth maps with built-in classification methods\nThe following panel will compare various choropleth maps with built-in classification methods and constant n = 5\n\njenksequalsdprettyquantilehclustfisherfisher\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nUsing equal range data classification, the map is not too informative as the data is skewed\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"pretty\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"fisher\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nVarious method of classification can give highly different result. One of the main contributor is due to skewness and presence of outliers in the data. Classification method which is insensitive to it will give monotonous map, where only a few region have different color, and vice versa. As an analyst, domain knowledge is required to decide which classification method is the most appropriate (i.e. whether small differences between the Dependency data matters). Ultimately, the method chosen should be able to support the best decision making.\n\nThe following panel will compare various choropleth maps with different number of classes\n\njenks 2 classesequal 6 classesequal 10 classesequal 20 classes\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 2,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 10,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 20,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nsimilar to classification method, number of classes could matter in showing differences between area. with n set as 2, even jenks method become monotonous, revealing the outlier area. on the other hand, with n set as high as 20, even the equal method start to show differences between region, albeit subtle (due to high degree of skewness/presence of extreme outliers)\n\n\n\n5.3.2 Plotting choropleth map with custom break\nthe automated break calculation in previous method can be overriden by explicitly set the break arguments.\nbefore starting, the following code will show descriptive statistics to be used for break reference.\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nwith reference to the summary statistics result, the break point is set at 0.60, 0.70, 0.80, and 0.90. The arguments also requires to include minimum and maximum value.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n5.4 Colour Scheme\nThe color scheme in tmap can be customized using user-defined or predefined color ramps from the RColorBrewer package.\nTo use a ColorBrewer palette, you assign the desired color to the palette argument of tm_fill(). If you want to change the color, you can do so by specifying the palette in the code.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nThe above choropleth map is shaded in green. To reverse the color shade use “-” prefix.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nThe colour scheme also changed with the new setting.\n\n\n5.5 Map Layouts\nMap layouts refer to combining all map elements into a cohesive map, including objects, title, scale bar, compass, margins, aspect ratios, color settings, and data classification methods. In tmap, various legend options are available to change the placement, format, and appearance of the legend. You can use tm_fill() along with tm_layout() to customize the legend based on your preferences. To change the style of layout, use tmap_style(). The following panel show how the various options are used.\n\nAdding legendClassic Map StyleCartographic Furniture Map Style\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n\n\nreset the tmap setting to default style.\n\ntmap_style(\"white\")\n\n\n\n5.6 Drawing Small Multiple Choropleth Maps\nSmall multiple choropleth maps, or facet maps, display many maps side-by-side or stacked vertically. tmap allows you to create small multiples in different ways, such as assigning multiple values to aesthetic arguments or using tm_facets().\nYou can also create small multiples by defining a group-by variable in tm_facets() or by creating multiple stand-alone maps with tmap_arrange(). Each method offers flexibility in visualizing spatial relationships.\n\nassigning multiple values to one aesthetic argumentsassigning multiple values to more than one aesthetic argumentsdefining a group-by variable in tm_facetscreating multiple stand-alone maps with tmap_arrange\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"), style = c(\"equal\", \"quantile\"), palette = list(\"Blues\",\"Greens\")) + tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nCode\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nCode\nyoungmap &lt;- tm_shape(mpsz_pop2020) + tm_polygons(\"YOUNG\", style = \"quantile\", palette = \"Blues\")\nagedmap &lt;- tm_shape(mpsz_pop2020) + tm_polygons(\"AGED\", style = \"quantile\", palette = \"Blues\")\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n\n\n5.7 Mapping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth maps, you can use selection functions to map spatial objects meeting specific criteria. This allows you to focus on specific regions or areas in the map based on your selection criterion. The following code choose Central Region as example\n\n\nCode\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "hands-on/hoe2-globalautocor.html",
    "href": "hands-on/hoe2-globalautocor.html",
    "title": "Hands-on Exercise 2: Global Measures of Spatial Autocorrelation",
    "section": "",
    "text": "Spaces are Clustered - Global Spatial Autocorrelation Illustration"
  },
  {
    "objectID": "hands-on/hoe2-globalautocor.html#overview",
    "href": "hands-on/hoe2-globalautocor.html#overview",
    "title": "Hands-on Exercise 2: Global Measures of Spatial Autocorrelation",
    "section": "1 Overview",
    "text": "1 Overview\nIn this part, we’ll explore Global and Local Measures of Spatial Autocorrelation (GLSA) using the spdep package. Specifically, this 2B exercise will focus on Global Measures of Spatial Autocorrelation while the Local Spatial Autocorrelation will be explored in 2C exercise. The new learning objectives includes: - Compute Global Spatial Autocorrelation (GSA) statistics using spdep. - Plot Moran scatterplot. - Compute and plot spatial correlogram with spdep. - Visualize the analysis output using the tmap package.\n\nGlobal measures of spatial autocorrelation provide a single summary statistic for an entire dataset, reflecting the overall degree of geographical clustering or dispersion. A commonly used global measure is Moran’s I, which helps to determine whether attribute values in a dataset are clustered together or spread apart. However, it doesn’t specify where these clusters or outliers are located on the map. Essentially, these measures give a general idea of spatial patterns across the whole study area but don’t provide detailed insights into local variations or specific areas summarized from: Wu and Kemp, 2019\n\n\n1.1 The analytical question\nIn spatial policy, local governments aim for an even distribution of development within a region. This exercise focuses on applying spatial statistical methods to investigate if development is evenly distributed in a province. If not, the next questions are: Is there evidence of spatial clustering? And if yes, where are these clusters?\nThe case study and data being explored is the same as previous exercise (2A), which are GDP per capita (GDPPR) in Hunan Province , People’s Republic of China.\n\n\n1.2 Setting Up the Analytical Tools\nEnsure that spdep, sf, tmap, and tidyverse packages in R are installed. These packages are used for importing and handling geospatial data, wrangling attribute data, computing spatial weights, and visualizing the results. The code below checks for and installs missing packages:\n\n\nCode\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "hands-on/hoe2-globalautocor.html#import-and-visualize-the-data",
    "href": "hands-on/hoe2-globalautocor.html#import-and-visualize-the-data",
    "title": "Hands-on Exercise 2: Global Measures of Spatial Autocorrelation",
    "section": "2 Import and Visualize the Data",
    "text": "2 Import and Visualize the Data\nSimilar to previous exercise, firstly the data needs to be imported, joined, and visualized to get the big picture.\n\n\nCode\n#import geospatial data\nhunan &lt;- st_read(dsn = \"../data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nCode\n# import aspatial data\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\n\n# perform relational join\nhunan &lt;- left_join(hunan, hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n# visualize the data\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\", n = 5, style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\", main.title.size = 1,\n            main.title.position = \"center\", legend.outside = TRUE,\n            legend.outside.position = \"bottom\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\", n = 5, style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\", main.title.size = 1,\n            main.title.position = \"center\", legend.outside = TRUE,\n            legend.outside.position = \"bottom\")\n\ntmap_arrange(equal, quantile, asp=1, ncol=2)"
  },
  {
    "objectID": "hands-on/hoe2-globalautocor.html#global-spatial-autocorrelation",
    "href": "hands-on/hoe2-globalautocor.html#global-spatial-autocorrelation",
    "title": "Hands-on Exercise 2: Global Measures of Spatial Autocorrelation",
    "section": "3 Global Spatial Autocorrelation",
    "text": "3 Global Spatial Autocorrelation\nIn this section, we’ll explore how to calculate global spatial autocorrelation statistics and conduct a test for spatial randomness across the entire study area.\n\n3.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area which use queen method in this example. After that weights needs to be assigned to each neighboring polygon, which use row-standardized weights matrix. Details on the concept was discussed in previous exercise.\n\n\nCode\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nCode\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n3.2 Global Spatial Autocorrelation: Moran’s I\n\nMoran’s I testMonte Carlo Moran’s IVisualising Monte Carlo Moran’s I\n\n\nNow, let’s perform a test for Moran’s I statistic, which assesses spatial autocorrelation. The test is performed using moran.test() of spdep.\n\n\nCode\nmoran.test(hunan$GDPPC, listw=rswm_q, zero.policy = TRUE, na.action=na.omit)\n\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\nThe statistical output indicates a Moran’s I statistic of 0.30, which is a measure of spatial autocorrelation. The standard deviate is 4.7351, leading to a very low p-value of 1.095e-06. The alternative hypothesis suggests a “greater” spatial autocorrelation. Based on the output, we can conclude that there is a strong and significant positive spatial autocorrelation in the GDPPC of Hunan county. This means that counties with similar GDPPC tend to be close to each other on the map. In other words, there is a spatial pattern of high-income and low-income counties in Hunan.\n\n\n\nNext, perform a permutation test for Moran’s I using the moran.mc() function of spdep. This involves running 1000 simulations.\n\n\nCode\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nThe associated p-value is 0.001, indicating statistical significance. The alternative hypothesis of “greater” spatial autocorrelation aligns with the initial Moran’s I test. This Monte Carlo simulation provides additional evidence supporting the presence of positive spatial autocorrelation in the GDP per capita values across the study area.\n\n\n\nTo gain more insights, we plot the distribution of simulated Moran’s I values using a histogram. the code use hist() and abline() of R Graphics are used.\n\n\nCode\nmean(bperm$res[1:999])\n\n\n[1] -0.01504572\n\n\nCode\nvar(bperm$res[1:999])\n\n\n[1] 0.004371574\n\n\nCode\nsummary(bperm$res[1:999])\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\nCode\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\n\nThe histogram indicates that the simulated Moran’s I values follow a normal distribution with it’s bell-shaped characteristic\n\nthe next code chunk perform similar task, but using ggplot package\n\n\nCode\n# Extract the res column and convert it to a data frame\nres_df &lt;- data.frame(res = bperm$res)\n\n# Plot the histogram using ggplot2\nggplot(res_df, aes(x = res)) +\n  geom_histogram(bins = 20) +\n  geom_vline(xintercept = 0, color = \"red\") +\n  xlab(\"Simulated Moran's I\")\n\n\n\n\n\n\n\n\n\n\n3.3 Global Spatial Autocorrelation: Geary’s\nIn this part, we explore Geary’s C statistics to understand spatial autocorrelation in our data. Geary’s C test helps us determine if there’s any pattern of similarity or dissimilarity between neighboring areas.\n\nGeary’s C testComputing Monte Carlo Geary’s CVisualising the Monte Carlo Geary’s C\n\n\nThe provided R code conducts Geary’s C test using the geary.test() function from the spdep package. This test essentially examines if the values in one region are significantly different from the values in neighboring regions.\n\n\nCode\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\nThe Geary’s C test results in a significant p-value (less than the typical significance level of 0.05), indicating that the spatial autocorrelation observed in the data is unlikely due to random chance. The Geary C statistic (0.6907) being less than the expectation (1.0000) suggests a pattern of dissimilarity between neighboring regions. Additionally, the variance (0.0073) provides information about the variability of this dissimilarity. Overall, these results suggest a non-random spatial pattern of dissimilarity in the distribution of the GDP per capita in the study area.\n\n\n\nThis code snippet uses Monte Carlo simulation to test Geary’s C statistic for spatial autocorrelation, by using geary.mc() of spdep. By comparing the observed statistic to a distribution of simulated values, it helps us assess whether the observed pattern is statistically significant.\n\n\nCode\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nThe observed Geary’s C statistic is 0.69072, and it ranks first among the simulated values, with a p-value of 0.001. This suggests that there is a strong spatial autocorrelation in the distribution of GDPPC values, indicating a pattern of either spatial similarity or dissimilarity among neighboring regions. The alternative hypothesis of greater spatial autocorrelation is supported by the low p-value, signifying that the observed spatial pattern is unlikely to have occurred by random chance alone..\n\n\n\nThe next step involves creating a histogram to understand the distribution of simulated values and to see where our observed value falls on the spectrum.\n\n\nCode\nmean(bperm$res[1:999])\n\n\n[1] 1.004402\n\n\nCode\nvar(bperm$res[1:999])\n\n\n[1] 0.007436493\n\n\nCode\nsummary(bperm$res[1:999])\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\nCode\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\n\n\nSimilar to previous result, this histogram also indicates that the simulated Geary c values follow a normal distribution with it’s bell-shaped characteristic\n\n\n4 Spatial Correlogram\nSpatial correlograms offer insights into spatial autocorrelation patterns by plotting autocorrelation indices against increasing distances. They are useful for exploratory analysis.\n\n4.1 Compute Moran’s I correlogram\nIn this part, the sp.correlogram() of spdep package computes a Moran’s I correlogram for GDPPC. The resulting plot illustrates how Moran’s I values change as the distance between regions increases. In the code chunk below,\n\n\nCode\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\nIt’s important to inspect the full analysis report, which can be printed using the following code:\n\n\nCode\nprint(MI_corr)\n\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe spatial correlogram for hunan$GDPPC using Moran’s I method reveals that as the distance between regions increases, there is a statistically significant spatial autocorrelation. The estimate of Moran’s I indicates positive autocorrelation at various lag distances (1 to 6). The p-values associated with each lag distance are highly significant (indicated by the ***), suggesting that the observed spatial pattern is not due to random chance. This implies that regions with similar GDPPC values tend to cluster together, providing evidence for a non-random spatial distribution of economic development in Hunan province.\n\n\n\n4.2 Compute Geary’s C correlogram and plot\nSimilarly, we calculate a Geary’s C correlogram to understand how the spatial autocorrelation, measured by Geary’s C, changes with increasing distances.\n\n\nCode\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\nPrint the analysis report for a more detailed understanding:\n\n\nCode\nprint(GC_corr)\n\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe Geary’s C correlogram for the GDPPC in Hunan Province indicates varying levels of spatial autocorrelation at different distance lags. The estimates at the first and second lags (distances) are significantly lower than the expected value, suggesting a pattern of positive spatial autocorrelation, meaning similar values tend to cluster together. However, as the distance increases, the autocorrelation becomes non-significant (lag 3), indicating a decrease in similarity between neighboring regions. Subsequently, at lags 4, 5, and 6, there is a mix of significant and non-significant autocorrelation, suggesting a complex spatial pattern with pockets of both similarity and dissimilarity at these distances."
  },
  {
    "objectID": "hands-on/hoe2-globalautocor.html#spatial-correlogram",
    "href": "hands-on/hoe2-globalautocor.html#spatial-correlogram",
    "title": "Hands-on Exercise 2: Global Measures of Spatial Autocorrelation",
    "section": "4 Spatial Correlogram",
    "text": "4 Spatial Correlogram\nSpatial correlograms offer insights into spatial autocorrelation patterns by plotting autocorrelation indices against increasing distances. They are useful for exploratory analysis.\n\n4.1 Compute Moran’s I correlogram\nIn this part, the sp.correlogram() of spdep package computes a Moran’s I correlogram for GDPPC. The resulting plot illustrates how Moran’s I values change as the distance between regions increases. In the code chunk below,\n\n\nCode\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\nIt’s important to inspect the full analysis report, which can be printed using the following code:\n\n\nCode\nprint(MI_corr)\n\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe spatial correlogram for hunan$GDPPC using Moran’s I method reveals that as the distance between regions increases, there is a statistically significant spatial autocorrelation. The estimate of Moran’s I indicates positive autocorrelation at various lag distances (1 to 6). The p-values associated with each lag distance are highly significant (indicated by the ***), suggesting that the observed spatial pattern is not due to random chance. This implies that regions with similar GDPPC values tend to cluster together, providing evidence for a non-random spatial distribution of economic development in Hunan province.\n\n\n\n4.2 Compute Geary’s C correlogram and plot\nSimilarly, we calculate a Geary’s C correlogram to understand how the spatial autocorrelation, measured by Geary’s C, changes with increasing distances.\n\n\nCode\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\nPrint the analysis report for a more detailed understanding:\n\n\nCode\nprint(GC_corr)\n\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe Geary’s C correlogram for the GDPPC in Hunan Province indicates varying levels of spatial autocorrelation at different distance lags. The estimates at the first and second lags (distances) are significantly lower than the expected value, suggesting a pattern of positive spatial autocorrelation, meaning similar values tend to cluster together. However, as the distance increases, the autocorrelation becomes non-significant (lag 3), indicating a decrease in similarity between neighboring regions. Subsequently, at lags 4, 5, and 6, there is a mix of significant and non-significant autocorrelation, suggesting a complex spatial pattern with pockets of both similarity and dissimilarity at these distances."
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html",
    "href": "hands-on/hoe2-spatialweight.html",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "",
    "text": "Weighing Space Illustration"
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html#overview",
    "href": "hands-on/hoe2-spatialweight.html#overview",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "1 Overview",
    "text": "1 Overview\n\nSpatial analysis is a method used to understand the significance of spatial relationships between different objects. It’s like figuring out how different pieces on a chessboard influence each other’s moves. Spatial weights are concepts that help us measure and analyze how different locations or regions are related to each other based on their proximity, similarity, or interaction. Spatial weights are numerical values that represent the strength or intensity of the connection between two spatial units, such as points, polygons, or pixels. Applications of spatial weights include detecting patterns, clusters, outliers, hot spots, or cold spots in spatial data, and testing hypotheses about spatial processes or phenomena. summarized from: Getis, 2010\n\nThe data used for practice in this exercise includes a map outlining the boundaries of Hunan county, presented as a geospatial dataset in ESRI shapefile format, and a CSV file named “Hunan_2012.csv,” which includes specific local development indicators for Hunan in the year 2012.\nThis exercise will help to get familiar with importing geospatial data using functions from the sf package, reading CSV files with functions from the readr package, conducting relational joins through functions from the dplyr package, computing spatial weights calculating spatially lagged variables using functions from the spdep package."
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html#preparing-the-library-and-data",
    "href": "hands-on/hoe2-spatialweight.html#preparing-the-library-and-data",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "2 Preparing the Library and Data",
    "text": "2 Preparing the Library and Data\nThe following code chunk will import the required library:\n\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\nThe following panel will show how the data is imported and joined\n\nImport ShapefileImport CSVJoin the Geospatial and Aspatial Data\n\n\nthe following code use st_read() from sf package to import Hunan shapefile into simple features Object\n\nhunan &lt;- st_read(dsn = \"../data/geospatial\", layer = \"Hunan\")\n\nReading layer `Hunan' from data source `C:\\ameernoor\\ISSS624\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\nglimpse(hunan)\n\nRows: 88\nColumns: 8\n$ NAME_2     &lt;chr&gt; \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Changde\", \"Cha…\n$ ID_3       &lt;int&gt; 21098, 21100, 21101, 21102, 21103, 21104, 21109, 21110, 211…\n$ NAME_3     &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"L…\n$ ENGTYPE_3  &lt;chr&gt; \"County\", \"County\", \"County City\", \"County\", \"County\", \"Cou…\n$ Shape_Leng &lt;dbl&gt; 1.869074, 2.360691, 1.425620, 3.474325, 2.289506, 4.171918,…\n$ Shape_Area &lt;dbl&gt; 0.10056190, 0.19978745, 0.05302413, 0.18908121, 0.11450357,…\n$ County     &lt;chr&gt; \"Anxiang\", \"Hanshou\", \"Jinshi\", \"Li\", \"Linli\", \"Shimen\", \"L…\n$ geometry   &lt;POLYGON [°]&gt; POLYGON ((112.0625 29.75523..., POLYGON ((112.2288 …\n\n\n\n\nthe following code use read_csv() from readr package to import\n\nhunan2012 &lt;- read_csv(\"../data/aspatial/Hunan_2012.csv\")\nglimpse(hunan2012)\n\nRows: 88\nColumns: 29\n$ County      &lt;chr&gt; \"Anhua\", \"Anren\", \"Anxiang\", \"Baojing\", \"Chaling\", \"Changn…\n$ City        &lt;chr&gt; \"Yiyang\", \"Chenzhou\", \"Changde\", \"Hunan West\", \"Zhuzhou\", …\n$ avg_wage    &lt;dbl&gt; 30544, 28058, 31935, 30843, 31251, 28518, 54540, 28597, 33…\n$ deposite    &lt;dbl&gt; 10967.0, 4598.9, 5517.2, 2250.0, 8241.4, 10860.0, 24332.0,…\n$ FAI         &lt;dbl&gt; 6831.7, 6386.1, 3541.0, 1005.4, 6508.4, 7920.0, 33624.0, 1…\n$ Gov_Rev     &lt;dbl&gt; 456.72, 220.57, 243.64, 192.59, 620.19, 769.86, 5350.00, 1…\n$ Gov_Exp     &lt;dbl&gt; 2703.0, 1454.7, 1779.5, 1379.1, 1947.0, 2631.6, 7885.5, 11…\n$ GDP         &lt;dbl&gt; 13225.0, 4941.2, 12482.0, 4087.9, 11585.0, 19886.0, 88009.…\n$ GDPPC       &lt;dbl&gt; 14567, 12761, 23667, 14563, 20078, 24418, 88656, 10132, 17…\n$ GIO         &lt;dbl&gt; 9276.90, 4189.20, 5108.90, 3623.50, 9157.70, 37392.00, 513…\n$ Loan        &lt;dbl&gt; 3954.90, 2555.30, 2806.90, 1253.70, 4287.40, 4242.80, 4053…\n$ NIPCR       &lt;dbl&gt; 3528.3, 3271.8, 7693.7, 4191.3, 3887.7, 9528.0, 17070.0, 3…\n$ Bed         &lt;dbl&gt; 2718, 970, 1931, 927, 1449, 3605, 3310, 582, 2170, 2179, 1…\n$ Emp         &lt;dbl&gt; 494.310, 290.820, 336.390, 195.170, 330.290, 548.610, 670.…\n$ EmpR        &lt;dbl&gt; 441.4, 255.4, 270.5, 145.6, 299.0, 415.1, 452.0, 127.6, 21…\n$ EmpRT       &lt;dbl&gt; 338.0, 99.4, 205.9, 116.4, 154.0, 273.7, 219.4, 94.4, 174.…\n$ Pri_Stu     &lt;dbl&gt; 54.175, 33.171, 19.584, 19.249, 33.906, 81.831, 59.151, 18…\n$ Sec_Stu     &lt;dbl&gt; 32.830, 17.505, 17.819, 11.831, 20.548, 44.485, 39.685, 7.…\n$ Household   &lt;dbl&gt; 290.4, 104.6, 148.1, 73.2, 148.7, 211.2, 300.3, 76.1, 139.…\n$ Household_R &lt;dbl&gt; 234.5, 121.9, 135.4, 69.9, 139.4, 211.7, 248.4, 59.6, 110.…\n$ NOIP        &lt;dbl&gt; 101, 34, 53, 18, 106, 115, 214, 17, 55, 70, 44, 84, 74, 17…\n$ Pop_R       &lt;dbl&gt; 670.3, 243.2, 346.0, 184.1, 301.6, 448.2, 475.1, 189.6, 31…\n$ RSCG        &lt;dbl&gt; 5760.60, 2386.40, 3957.90, 768.04, 4009.50, 5220.40, 22604…\n$ Pop_T       &lt;dbl&gt; 910.8, 388.7, 528.3, 281.3, 578.4, 816.3, 998.6, 256.7, 45…\n$ Agri        &lt;dbl&gt; 4942.253, 2357.764, 4524.410, 1118.561, 3793.550, 6430.782…\n$ Service     &lt;dbl&gt; 5414.5, 3814.1, 14100.0, 541.8, 5444.0, 13074.6, 17726.6, …\n$ Disp_Inc    &lt;dbl&gt; 12373, 16072, 16610, 13455, 20461, 20868, 183252, 12379, 1…\n$ RORP        &lt;dbl&gt; 0.7359464, 0.6256753, 0.6549309, 0.6544614, 0.5214385, 0.5…\n$ ROREmp      &lt;dbl&gt; 0.8929619, 0.8782065, 0.8041262, 0.7460163, 0.9052651, 0.7…\n\n\n\n\nThe following code use left_join() from dplyr package to merge the aspatial data to the geospatial data\n\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\nhead(hunan, n = 10)\n\nSimple feature collection with 10 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 26.28322 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n     NAME_2  ID_3    NAME_3   ENGTYPE_3    County GDPPC\n1   Changde 21098   Anxiang      County   Anxiang 23667\n2   Changde 21100   Hanshou      County   Hanshou 20981\n3   Changde 21101    Jinshi County City    Jinshi 34592\n4   Changde 21102        Li      County        Li 24473\n5   Changde 21103     Linli      County     Linli 25554\n6   Changde 21104    Shimen      County    Shimen 27137\n7  Changsha 21109   Liuyang County City   Liuyang 63118\n8  Changsha 21110 Ningxiang      County Ningxiang 62202\n9  Changsha 21111 Wangcheng      County Wangcheng 70666\n10 Chenzhou 21112     Anren      County     Anren 12761\n                         geometry\n1  POLYGON ((112.0625 29.75523...\n2  POLYGON ((112.2288 29.11684...\n3  POLYGON ((111.8927 29.6013,...\n4  POLYGON ((111.3731 29.94649...\n5  POLYGON ((111.6324 29.76288...\n6  POLYGON ((110.8825 30.11675...\n7  POLYGON ((113.9905 28.5682,...\n8  POLYGON ((112.7181 28.38299...\n9  POLYGON ((112.7914 28.52688...\n10 POLYGON ((113.1757 26.82734..."
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html#visualizing-regional-development-indicator",
    "href": "hands-on/hoe2-spatialweight.html#visualizing-regional-development-indicator",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "3 Visualizing Regional Development Indicator",
    "text": "3 Visualizing Regional Development Indicator\nthis section will explore distribution of Gross Domestic Product Per Capita (GDPPC) 2012 in Hunan by creating base map and build choropleth map. qtm() from tmap package is used to build the map.\n\n\nCode\n# Creating The Basemap\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size = 0.5) +\n  tm_layout(main.title = \"Basemap\", main.title.position = \"left\")  # Add title\n\n# Creating The Choropleth Map\ngdppc &lt;- qtm(hunan, \"GDPPC\") +\n  tm_layout(main.title = \"Choropleth Map\", main.title.position = \"left\",\n            legend.outside = TRUE, legend.outside.position = 'right')  # adjust the legend\n\n# show the map\ntmap_arrange(basemap, gdppc, asp=1, ncol=2, widths = c(0.4,0.6))"
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html#computing-contiguity-spatial-weights",
    "href": "hands-on/hoe2-spatialweight.html#computing-contiguity-spatial-weights",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "4 Computing Contiguity Spatial Weights",
    "text": "4 Computing Contiguity Spatial Weights\n\nContiguity Spatial Weights are used in spatial data analysis to understand how close or connected different geographic areas are to each other. Simply put, if two areas, like counties or neighborhoods, share a border, they’re considered “contiguous” or neighbors. This concept is important for understanding patterns like how a phenomenon in one area might affect neighboring areas. Two main criteria are used to define contiguity: ‘rook’ and ‘queen’. Rook contiguity means areas are neighbors if they share a common edge. Queen contiguity is a bit broader, including areas that share either a common edge or a corner. This is akin to the movements of rook and queen pieces in chess Summarized from: Anselin\n\n\n\n\nQueen vs Rook Contiguity\n\n\nSource: Research Gate\nThis section explore poly2nb() from spdep package to compute contiguity weight matrices. The function builds a neighbours list based on regions with contiguous boundaries. Using “queen” parameter that takes TRUE or FALSE as options, if it is set to TRUE, the function will return a list of first order neighbours using the Queen criteria.\n\nQueenRookVisualising Contiguity Weights\n\n\n\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe output summarizes the spatial relationships in Hunan using Queen’s contiguity method. There are 88 regions, and the analysis reveals a total of 448 connections among them. The percentage of nonzero weights, indicating connected regions, is approximately 5.79%. On average, each region has around 5.09 links with other regions. The distribution of links shows that most regions have 4 or 5 connections, with the least connected regions being 30 and 65, each having only 1 link. The most connected region is labeled as 85, with 11 links.\nto list all neighboring polygons of a unit, use wm_q as shown in the following code, where 1 represent the polygon Unit ID being shown, and the output shows the 5 negiboring polygon Unit ID\n\n\nCode\nwm_q[[1]]\n\n\n[1]  2  3  4 57 85\n\n\nto retrieve the name of the county, use the following code\n\n\nCode\nhunan$County[1]\n\n\n[1] \"Anxiang\"\n\n\nto retrieve county names of more than one polygons, use the following example that display the neigbor of Anxiang\n\n\nCode\nhunan$NAME_3[c(2,3,4,57,85)]\n\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nadditionally the GDDPC data of multiple countries can also be displayed using the following code\n\n\nCode\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nto display the complete weight matrix which represent the neigbors of each region, use the following code\n\n\nCode\nstr(wm_q)\n\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\n\n\nSimilar to the example of Queen method, Rook method can be executed by changing queen parameter to False\n\n\nCode\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nAs expected from the stricter condition of Rook compared to Queen, the regions will have less neighbor on average\n\n\nTo create a connectivity graph, we first need to represent polygons as points. In our case, we’re working with polygons, so we’ll use polygon centroids as points for our graph. The common approach is to calculate these centroids using the sf package. To achieve this, we employ the st_centroid function on the geometry column of our spatial object (in this case, hunan). Since we require the coordinates in a separate data frame, we utilize a mapping function. This function applies st_centroid to each element of the geometry column and returns a vector of the same length. We specifically use the map_dbl variation from the purrr package. For latitude and longitude values, we extract them using double bracket notation, [[1]] for longitude and [[2]] for latitude. Finally, we combine these coordinates into a single object using cbind(), and we verify the formatting by checking the first few observations using head().\n\n\nCode\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\ncoords &lt;- cbind(longitude, latitude)\n\nhead(coords)\n\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\nNext, the following code will be used to display and compare Queen and Rook contiguity neighbours maps\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html#computing-distance-based-neighbours",
    "href": "hands-on/hoe2-spatialweight.html#computing-distance-based-neighbours",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "5 Computing Distance Based Neighbours",
    "text": "5 Computing Distance Based Neighbours\nIn this part, we will explore how to figure out which areas are close to each other using distances, by utilizing dnearneigh() function from the spdep package.\nThis function looks at points on a map and finds their neighbors based on how far apart they are. Range of distances can be set using bounds argument, with a lower limit d1= and an upper limit d2=. If the locations are given in regular coordinates (like x and y on a typical map) and latitude and longitude argument set to true (longlat=TRUE), the function measures distances in kilometers. It does this as if by figuring out how far it is on the Earth’s surface, using something called the WGS84 reference ellipsoid.\n\nThe WGS84 reference ellipsoid is a mathematical model that approximates the shape of the Earth. It’s not a perfect sphere but more like a slightly squashed ball, wider at the equator than at the poles. When measuring distances using this model, it considers the Earth’s curvature. This method provides a more accurate way to measure real distances on the Earth’s surface, especially over long distances where the Earth’s curvature becomes significant. It’s like tracing a line along the surface of an orange, rather than cutting straight through it.\n\nThe following part will explore how to find the right distance cut-off, fixed distance calculation, and adaptive distance calculation.\n\n5.1 Determine the cut-off distance\nTo find the right distance for the analysis, execute the following steps:\n\nUse knearneigh() to get a list of indices representing the k nearest neighbors for each point.\nConvert this list into a neighbor list with knn2nb().\nFind the lengths of these neighbor relationships with nbdists(). Remove any complex structure with unlist().\n\n\n\nCode\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary shows that the maximum distance to the first nearest neighbor is 61.79 km. Use it as threshold to ensure each unit has at least one neighbor.\n\n\n5.2 Computing fixed distance weight matrix\nBased on the previous knowledge, create the distance weight matrix using the specified distance range (0 to 62 km).\n\n\nCode\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\nThe “Average number of links: 3.681818” means that, on average, each location is linked to approximately 3.68 other locations within the specified distance range.\n\nWe can inspect the structure of the weight matrix using str() or combining table() and card() of spdep.\n\n\nCode\nstr(wm_d62)\n\n\nList of 88\n $ : int [1:5] 3 4 5 57 64\n $ : int [1:4] 57 58 78 85\n $ : int [1:4] 1 4 5 57\n $ : int [1:3] 1 3 5\n $ : int [1:4] 1 3 4 85\n $ : int 69\n $ : int [1:2] 67 84\n $ : int [1:4] 9 46 47 78\n $ : int [1:4] 8 46 68 84\n $ : int [1:4] 16 22 70 72\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:2] 11 17\n $ : int 13\n $ : int [1:4] 10 17 22 83\n $ : int [1:3] 11 14 16\n $ : int [1:3] 20 22 63\n $ : int [1:5] 20 21 73 74 82\n $ : int [1:5] 18 19 21 22 82\n $ : int [1:6] 19 20 35 74 82 86\n $ : int [1:4] 10 16 18 20\n $ : int [1:3] 41 77 82\n $ : int [1:4] 25 28 31 54\n $ : int [1:4] 24 28 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:2] 26 29\n $ : int [1:6] 24 25 33 49 52 54\n $ : int [1:2] 27 37\n $ : int 33\n $ : int [1:2] 24 36\n $ : int 50\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 46 47 80 82\n $ : int [1:5] 31 34 45 56 80\n $ : int [1:2] 29 42\n $ : int [1:3] 44 77 79\n $ : int [1:4] 40 42 43 81\n $ : int [1:3] 39 45 79\n $ : int [1:5] 23 35 45 79 82\n $ : int [1:5] 26 37 39 43 81\n $ : int [1:3] 39 42 44\n $ : int [1:2] 38 43\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:5] 8 9 35 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:6] 32 48 51 52 54 55\n $ : int [1:4] 48 49 50 52\n $ : int [1:6] 28 48 49 50 51 54\n $ : int [1:2] 48 55\n $ : int [1:5] 24 28 49 50 52\n $ : int [1:4] 48 50 53 75\n $ : int 36\n $ : int [1:5] 1 2 3 58 64\n $ : int [1:5] 2 57 64 66 68\n $ : int [1:3] 60 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:5] 12 60 62 63 87\n $ : int [1:4] 61 63 77 87\n $ : int [1:5] 12 18 61 62 83\n $ : int [1:4] 1 57 58 76\n $ : int 76\n $ : int [1:5] 58 67 68 76 84\n $ : int [1:2] 7 66\n $ : int [1:4] 9 58 66 84\n $ : int [1:2] 6 75\n $ : int [1:3] 10 72 73\n $ : int [1:2] 73 74\n $ : int [1:3] 10 11 70\n $ : int [1:4] 19 70 71 74\n $ : int [1:5] 19 21 71 73 86\n $ : int [1:2] 55 69\n $ : int [1:3] 64 65 66\n $ : int [1:3] 23 38 62\n $ : int [1:2] 2 8\n $ : int [1:4] 38 40 41 45\n $ : int [1:5] 34 35 36 45 47\n $ : int [1:5] 25 26 33 39 42\n $ : int [1:6] 19 20 21 23 35 41\n $ : int [1:4] 12 13 16 63\n $ : int [1:4] 7 9 66 68\n $ : int [1:2] 2 5\n $ : int [1:4] 21 46 47 74\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language dnearneigh(x = coords, d1 = 0, d2 = 62, longlat = TRUE)\n - attr(*, \"dnn\")= num [1:2] 0 62\n - attr(*, \"bounds\")= chr [1:2] \"GE\" \"LE\"\n - attr(*, \"nbtype\")= chr \"distance\"\n - attr(*, \"sym\")= logi TRUE\n\n\nthe output shows for who are the neighbors of each county (shown in unit ID list per row)\n\n\nCode\ntable(hunan$County, card(wm_d62))\n\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nCode\nn_comp &lt;- n.comp.nb(wm_d62)\ntable(n_comp$comp.id)\n\n\n\n 1 \n88 \n\n\nthe table shows, for each county, how many neighbors it has.\n\nOverlapping Visualization\nThe red lines represent 1st nearest neighbors, while the black lines are links within the 62 km cut-off distance.\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\n\n\n\nSide by Side Visualization\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n5.3 Computing adaptive distance weight matrix\nUsing fixed distance, densely settled urban areas tend to have more neigbours compared to rural. Having many neighbours smoothes the neighbour relationship across more neighbours. Number of neighbors can be adapted by accepting asymmetric neighbours or imposing symmetry.\nThe following code chunk impose 6 neighbors in the argument, hence the average number of links is 6 as well.\n\n\nCode\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nVisualize the weight matrix.\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html#weights-based-on-idw",
    "href": "hands-on/hoe2-spatialweight.html#weights-based-on-idw",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "6 Weights based on IDW",
    "text": "6 Weights based on IDW\nAnother method to derive spatial weight matrix is based on Inversed Distance method (IDW).\nCompute distance of areas using nbdists() of spdep.\n\n\nCode\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\nids\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\nAssign equal weights (style=“W”) to neighboring polygons. It’s calculated by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values.\n\nOne downside of using this approach is that regions at the edges of the study area might rely on fewer neighboring regions. This could lead to either overestimating or underestimating the real spatial connections in the data.\nFor a stronger and more reliable choice, you can use “style=B.”\n\n\n\nCode\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\nBe careful when setting zero.policy=TRUE because it lets you have lists of regions that are not neighbors. This can be risky because you might not notice if some neighbors are missing in your data. On the other hand, using zero.policy=FALSE would result in an error if there are missing neighbors.\n\nCheck the weight of the first polygon’s eight neighbors with the following code chunk.\n\n\nCode\nrswm_q$weights[10]\n\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor gets a share of 0.125 from the total weight. This implies that when R calculates the average income of neighboring areas, it multiplies each neighbor’s income by 0.2 before adding them up.\nWe can apply a similar approach to create a distance weight matrix that is standardized by rows.\n\n\nCode\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\n\nCode\nrswm_ids$weights[1]\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\n\nCode\nsummary(unlist(rswm_ids$weights))\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html#application-of-spatial-weight-matrix",
    "href": "hands-on/hoe2-spatialweight.html#application-of-spatial-weight-matrix",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "7 Application of Spatial Weight Matrix",
    "text": "7 Application of Spatial Weight Matrix\nThis part will explore how to create four types of spatial lagged variables as shown in the panel.\n\nrow-standardized weights The following code computes the average neighbor GDPPC. These values are called spatially lagged values.Spatial lag as a sum of neighboring valuesSpatial window averageSpatial window sum\n\n\n\n\nCode\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalling the GDPPC values obtained earlier for these five counties\n\n\nCode\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\n\nSpatial Lag with Row-Standardized Weights method measures how much an observation at one location is influenced by observations at neighboring locations. The spatial lag is calculated as a weighted average, where the weights are standardized so that they add up to one for each location. This means that each location’s value is influenced equally by its neighbors, creating a balanced representation of neighboring influence. summarized from: Anselin\n\nWe can add the spatially lagged GDPPC values to the hunan sf data frame using the following code:\n\n\nCode\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\n\nThe table below shows the average neighboring income values for each region.\n\n\nCode\nhead(hunan)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, plot both GDPPC and spatially lagged GDPPC for comparison.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\nusing row-standardized weights, the distribution of lagged GDPPC on the right shows how neighboring countries becomes more similar. note that some region which was originally much richer than it’s neighbors, becomes poorer than its neighbors while it’s neighbor becomes richer. this indicates caution when using the row-standardized weights\n\n\n\nWe can calculate spatial lag as a sum of neighboring values using binary weights. This involves going back to the neighbors list, applying a function to assign binary weights, and explicitly assigning these weights in the nb2listw function.\nWe start by assigning a value of 1 to each neighbor using lapply:\n\n\nCode\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, compute the lag variable from our weights and GDPPC.\n\n\nCode\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\nexamine the result:\n\n\nCode\nlag_sum\n\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\n\n\nThis method involves summing up the values of neighboring observations to calculate the spatial lag. Unlike the row-standardized method, this doesn’t involve any kind of averaging or standardization, so the total influence is simply the sum of the influences from each neighbor. This approach is particularly useful when dealing with binary data (like 0 or 1 values). summarized from: Anselin\n\nappend the lag_sum GDPPC field to the hunan sf data frame:\n\n\nCode\nhunan &lt;- left_join(hunan, lag.res)\n\n\nplot both GDPPC and Spatial Lag Sum GDPPC for comparison.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\nSpatial window average uses row-standardized weights and includes the diagonal element. To achieve this in R, we need to add the diagonal element to the neighbors’ structure before assigning weights.\nAdd the diagonal element using include.self() from spdep\n\n\nCode\nwm_qs &lt;- include.self(wm_q)\n\n\nobtain weights with nb2listw()\n\n\nCode\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\ncreate the lag variable from our weight structure and GDPPC variable:\n\n\nCode\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nconvert the lag variable listw object into a data.frame:\n\n\nCode\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\nappend lag_window_avg GDPPC values to hunan:\n\n\nCode\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\n\ncompare the values of lag GDPPC and Spatial window average by using kable()\n\n\nCode\nhunan %&gt;%\n  select(\"County\", \n         \"lag GDPPC\", \n         \"lag_window_avg GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n32671.00\n26627.50\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n20810.00\n22690.17\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n25711.50\n25366.40\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n30672.33\n25825.75\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n33457.75\n30329.00\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n31689.20\n32682.83\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n20269.00\n25948.62\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n23901.60\n23987.67\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n25126.17\n25463.14\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n21903.43\n21904.38\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n22718.60\n23127.50\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n25918.80\n25949.83\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n20307.00\n20018.75\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n20023.80\n19524.17\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n16576.80\n18955.00\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n18667.00\n17800.40\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n14394.67\n15883.00\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n19848.80\n18831.33\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n15516.33\n14832.50\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518.00\n17965.00\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n17572.00\n17159.89\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n15200.12\n16199.44\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n18413.80\n18764.50\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n14419.33\n26878.75\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n24094.50\n23188.86\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n22019.83\n20788.14\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n12923.50\n12365.20\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n14756.00\n15985.00\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n13869.80\n13764.83\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n12296.67\n11907.43\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n15775.17\n17128.14\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n14382.86\n14593.62\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n11566.33\n11644.29\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n13199.50\n12706.00\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n23412.00\n21712.29\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n39541.00\n43548.25\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n36186.60\n35049.00\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n16559.60\n16226.83\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n20772.50\n19294.40\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n19471.20\n18156.00\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n19827.33\n19954.75\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n15466.80\n18145.17\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n12925.67\n12132.75\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n18577.17\n18419.29\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n14943.00\n14050.83\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n24913.00\n23619.75\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n25093.00\n24552.71\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n24428.80\n24733.67\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n17003.00\n16762.60\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n21143.75\n20932.60\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n20435.00\n19467.75\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n17131.33\n18334.00\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n24569.75\n22541.00\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n23835.50\n26028.00\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360.00\n29128.50\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n47383.40\n46569.00\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n55157.75\n47576.60\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n37058.00\n36545.50\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n21546.67\n20838.50\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n23348.67\n22531.00\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n42323.67\n42115.50\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n28938.60\n27619.00\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n25880.80\n27611.33\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n47345.67\n44523.29\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n18711.33\n18127.43\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n29087.29\n28746.38\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n20748.29\n20734.50\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n35933.71\n33880.62\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n15439.71\n14716.38\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n29787.50\n28516.22\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n18145.00\n18086.14\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n21617.00\n21244.50\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n29203.89\n29568.80\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n41363.67\n48119.71\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n22259.09\n22310.75\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n44939.56\n43151.60\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n16902.00\n17133.40\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n16930.00\n17009.33\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nuse qtm() to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison:\n\n\nCode\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\nThis concept extends the idea of spatial lag by including the observation itself in the average calculation. It’s like creating a window that includes the value at a specific location and its neighbors, and then computing the average of all these values. This method is useful when you want to take into account both the value at a specific point and the influence of its surroundings. summarized from: Anselin\n\n\n\nSpatial window sum is similar to window average but without using row-standardized weights.\nLet’s add the diagonal element to the neighbor list:\n\n\nCode\nwm_qs &lt;- include.self(wm_q)\n\n\nNext, we assign binary weights:\n\n\nCode\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_qs, glist = b_weights, style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nNow, we can compute the lag variable with lag.listw():\n\n\nCode\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we convert the lag variable listw object into a data.frame:\n\n\nCode\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\n\nNow, we append w_sum GDPPC values to hunan:\n\n\nCode\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\n\nuse kable() To compare the values of lag GDPPC and Spatial window average\n\n\nCode\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\nGuidong\n98013\n106510\nPOLYGON ((114.1799 26.20117…\n\n\nJiahe\n104050\n136141\nPOLYGON ((112.4425 25.74358…\n\n\nLinwu\n102846\n126832\nPOLYGON ((112.5914 25.55143…\n\n\nRucheng\n92017\n103303\nPOLYGON ((113.6759 25.87578…\n\n\nYizhang\n133831\n151645\nPOLYGON ((113.2621 25.68394…\n\n\nYongxing\n158446\n196097\nPOLYGON ((113.3169 26.41843…\n\n\nZixing\n141883\n207589\nPOLYGON ((113.7311 26.16259…\n\n\nChangning\n119508\n143926\nPOLYGON ((112.6144 26.60198…\n\n\nHengdong\n150757\n178242\nPOLYGON ((113.1056 27.21007…\n\n\nHengnan\n153324\n175235\nPOLYGON ((112.7599 26.98149…\n\n\nHengshan\n113593\n138765\nPOLYGON ((112.607 27.4689, …\n\n\nLeiyang\n129594\n155699\nPOLYGON ((112.9996 26.69276…\n\n\nQidong\n142149\n160150\nPOLYGON ((111.7818 27.0383,…\n\n\nChenxi\n100119\n117145\nPOLYGON ((110.2624 28.21778…\n\n\nZhongfang\n82884\n113730\nPOLYGON ((109.9431 27.72858…\n\n\nHuitong\n74668\n89002\nPOLYGON ((109.9419 27.10512…\n\n\nJingzhou\n43184\n63532\nPOLYGON ((109.8186 26.75842…\n\n\nMayang\n99244\n112988\nPOLYGON ((109.795 27.98008,…\n\n\nTongdao\n46549\n59330\nPOLYGON ((109.9294 26.46561…\n\n\nXinhuang\n20518\n35930\nPOLYGON ((109.227 27.43733,…\n\n\nXupu\n140576\n154439\nPOLYGON ((110.7189 28.30485…\n\n\nYuanling\n121601\n145795\nPOLYGON ((110.9652 28.99895…\n\n\nZhijiang\n92069\n112587\nPOLYGON ((109.8818 27.60661…\n\n\nLengshuijiang\n43258\n107515\nPOLYGON ((111.5307 27.81472…\n\n\nShuangfeng\n144567\n162322\nPOLYGON ((112.263 27.70421,…\n\n\nXinhua\n132119\n145517\nPOLYGON ((111.3345 28.19642…\n\n\nChengbu\n51694\n61826\nPOLYGON ((110.4455 26.69317…\n\n\nDongan\n59024\n79925\nPOLYGON ((111.4531 26.86812…\n\n\nDongkou\n69349\n82589\nPOLYGON ((110.6622 27.37305…\n\n\nLonghui\n73780\n83352\nPOLYGON ((110.985 27.65983,…\n\n\nShaodong\n94651\n119897\nPOLYGON ((111.9054 27.40254…\n\n\nSuining\n100680\n116749\nPOLYGON ((110.389 27.10006,…\n\n\nWugang\n69398\n81510\nPOLYGON ((110.9878 27.03345…\n\n\nXinning\n52798\n63530\nPOLYGON ((111.0736 26.84627…\n\n\nXinshao\n140472\n151986\nPOLYGON ((111.6013 27.58275…\n\n\nShaoshan\n118623\n174193\nPOLYGON ((112.5391 27.97742…\n\n\nXiangxiang\n180933\n210294\nPOLYGON ((112.4549 28.05783…\n\n\nBaojing\n82798\n97361\nPOLYGON ((109.7015 28.82844…\n\n\nFenghuang\n83090\n96472\nPOLYGON ((109.5239 28.19206…\n\n\nGuzhang\n97356\n108936\nPOLYGON ((109.8968 28.74034…\n\n\nHuayuan\n59482\n79819\nPOLYGON ((109.5647 28.61712…\n\n\nJishou\n77334\n108871\nPOLYGON ((109.8375 28.4696,…\n\n\nLongshan\n38777\n48531\nPOLYGON ((109.6337 29.62521…\n\n\nLuxi\n111463\n128935\nPOLYGON ((110.1067 28.41835…\n\n\nYongshun\n74715\n84305\nPOLYGON ((110.0003 29.29499…\n\n\nAnhua\n174391\n188958\nPOLYGON ((111.6034 28.63716…\n\n\nNan\n150558\n171869\nPOLYGON ((112.3232 29.46074…\n\n\nYuanjiang\n122144\n148402\nPOLYGON ((112.4391 29.1791,…\n\n\nJianghua\n68012\n83813\nPOLYGON ((111.6461 25.29661…\n\n\nLanshan\n84575\n104663\nPOLYGON ((112.2286 25.61123…\n\n\nNingyuan\n143045\n155742\nPOLYGON ((112.0715 26.09892…\n\n\nShuangpai\n51394\n73336\nPOLYGON ((111.8864 26.11957…\n\n\nXintian\n98279\n112705\nPOLYGON ((112.2578 26.0796,…\n\n\nHuarong\n47671\n78084\nPOLYGON ((112.9242 29.69134…\n\n\nLinxiang\n26360\n58257\nPOLYGON ((113.5502 29.67418…\n\n\nMiluo\n236917\n279414\nPOLYGON ((112.9902 29.02139…\n\n\nPingjiang\n220631\n237883\nPOLYGON ((113.8436 29.06152…\n\n\nXiangyin\n185290\n219273\nPOLYGON ((112.9173 28.98264…\n\n\nCili\n64640\n83354\nPOLYGON ((110.8822 29.69017…\n\n\nChaling\n70046\n90124\nPOLYGON ((113.7666 27.10573…\n\n\nLiling\n126971\n168462\nPOLYGON ((113.5673 27.94346…\n\n\nYanling\n144693\n165714\nPOLYGON ((113.9292 26.6154,…\n\n\nYou\n129404\n165668\nPOLYGON ((113.5879 27.41324…\n\n\nZhuzhou\n284074\n311663\nPOLYGON ((113.2493 28.02411…\n\n\nSangzhi\n112268\n126892\nPOLYGON ((110.556 29.40543,…\n\n\nYueyang\n203611\n229971\nPOLYGON ((113.343 29.61064,…\n\n\nQiyang\n145238\n165876\nPOLYGON ((111.5563 26.81318…\n\n\nTaojiang\n251536\n271045\nPOLYGON ((112.0508 28.67265…\n\n\nShaoyang\n108078\n117731\nPOLYGON ((111.5013 27.30207…\n\n\nLianyuan\n238300\n256646\nPOLYGON ((111.6789 28.02946…\n\n\nHongjiang\n108870\n126603\nPOLYGON ((110.1441 27.47513…\n\n\nHengyang\n108085\n127467\nPOLYGON ((112.7144 26.98613…\n\n\nGuiyang\n262835\n295688\nPOLYGON ((113.0811 26.04963…\n\n\nChangsha\n248182\n336838\nPOLYGON ((112.9421 28.03722…\n\n\nTaoyuan\n244850\n267729\nPOLYGON ((112.0612 29.32855…\n\n\nXiangtan\n404456\n431516\nPOLYGON ((113.0426 27.8942,…\n\n\nDao\n67608\n85667\nPOLYGON ((111.498 25.81679,…\n\n\nJiangyong\n33860\n51028\nPOLYGON ((111.3659 25.39472…\n\n\n\n\n\nFinally, we’ll use qtm() to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison:\n\n\nCode\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\nThis is similar to the spatial window average, but instead of averaging the values, it sums them up. This method calculates the total value by adding the value at a specific location to the sum of its neighboring values. It provides a more cumulative measure of spatial influence compared to the average. summarized from: Anselin"
  },
  {
    "objectID": "hands-on/hoe2-spatialweight.html#references",
    "href": "hands-on/hoe2-spatialweight.html#references",
    "title": "Hands-on Exercise 2: Spatial Weights and Application",
    "section": "8 References",
    "text": "8 References\n\nr4gdsa chapter 8\nAnselin - Weights Applications\nAnselin - Contiguity Weights\nGetis - Spatial Analysis Handbook"
  },
  {
    "objectID": "in-class/ice1.html",
    "href": "in-class/ice1.html",
    "title": "In-class Exercise 1: Preparing the Data Flow",
    "section": "",
    "text": "Preparing Data Flow Illustration"
  },
  {
    "objectID": "in-class/ice1.html#getting-started",
    "href": "in-class/ice1.html#getting-started",
    "title": "In-class Exercise 1: Preparing the Data Flow",
    "section": "1 Getting Started",
    "text": "1 Getting Started\nThe code chunk below load the following packages: - tmap: for thematic mapping - sf : for geospatial data handling - tidyverse for non-spatial data handling.\n\npacman::p_load(tmap, sf, tidyverse)\n# load the libraries, the pacman itself will only be loaded temporarily"
  },
  {
    "objectID": "in-class/ice1.html#preparing-the-data-flow",
    "href": "in-class/ice1.html#preparing-the-data-flow",
    "title": "In-class Exercise 1: Preparing the Data Flow",
    "section": "2 Preparing the Data Flow",
    "text": "2 Preparing the Data Flow\n\n2.1 Importing the Aspatial data\nImport the Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall by using read_csv() of readr package.\n\nodbus &lt;- read_csv(\"../data/aspatial/origin_destination_bus_202308.csv.gz\")\nhead(odbus)\n\n\nChange Character Data Type to Numerical Factor\nodbus08 is a tibble dataframe. However, ORIGIN_PT_CODE and DESTINATION_PT_CODE are in character format. These are transformed into factors (categorical data type) for further analysis.\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\n\n\nExtracting the data for analysis\nextract commuting flows by extracting Origin bus stop codes and number of trips for weekdays between 7 and 9 o’clock, into a new dataframe:\n\n\nCode\norigtrip_7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\nglimpse(origtrip_7_9)\n# the %&gt;% sign is for stepping the process in order \n\n\n\n\n\n2.2 Importing the geospatial data\nTwo geospatial data will be used in this exercise both data contain coordinate in geometry column:\n\nbusstop &lt;- st_read(dsn = \"../data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414) # the st_transform is for projection\nglimpse(busstop)\n\n\nmpsz &lt;- st_read(dsn = \"../data/geospatial\",\n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\nglimpse(mpsz)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Geospatial Analytics (ISSS624)",
    "section": "",
    "text": "Exploring Geospatial Analytics Illustration\n\n\nHello there! I’m Muhamad Ameer Noor, and this is my space dedicated to the exciting world of geospatial analytics. Join me on this journey as I delve into the fascinating realm of spatial data analysis and its applications.\n\n\nOn this webpage, I’ll be sharing insights, discoveries, and projects related to geospatial analytics that I learned from ISSS624 - Applied Geospatial Analytics Course under Prof Kam Tin Seong during my MITB Programme at Singapore Management University. Whether you’re a fellow enthusiast, a student, or just curious about the power of location-based data, you’re in the right place!\n\n\n\nI’ll document my experiences and challenges as I navigate through ISSS624 Geospatial Analytics. Expect a mix of tutorials and case studies that showcase the practical applications of geospatial analytics.\n\n\n\nCurious about how this webpage was built? Check out the Quarto websites documentation\nThanks for stopping by, and let’s explore the world through geospatial analytics!"
  },
  {
    "objectID": "index.html#what-to-expect",
    "href": "index.html#what-to-expect",
    "title": "Applied Geospatial Analytics (ISSS624)",
    "section": "",
    "text": "On this webpage, I’ll be sharing insights, discoveries, and projects related to geospatial analytics that I learned from ISSS624 - Applied Geospatial Analytics Course under Prof Kam Tin Seong during my MITB Programme at Singapore Management University. Whether you’re a fellow enthusiast, a student, or just curious about the power of location-based data, you’re in the right place!"
  },
  {
    "objectID": "index.html#learning-journey",
    "href": "index.html#learning-journey",
    "title": "Applied Geospatial Analytics (ISSS624)",
    "section": "",
    "text": "I’ll document my experiences and challenges as I navigate through ISSS624 Geospatial Analytics. Expect a mix of tutorials and case studies that showcase the practical applications of geospatial analytics."
  },
  {
    "objectID": "index.html#dive-deeper",
    "href": "index.html#dive-deeper",
    "title": "Applied Geospatial Analytics (ISSS624)",
    "section": "",
    "text": "Curious about how this webpage was built? Check out the Quarto websites documentation\nThanks for stopping by, and let’s explore the world through geospatial analytics!"
  }
]